<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2021-12-17T11:53:50-05:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  
  <entry>
    <title type="html"><![CDATA[Slitter: a slab allocator that trusts, but verifies]]></title>
    <link href="https://www.pvk.ca/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator/"/>
    <updated>2021-08-01T17:26:04-04:00</updated>
    <id>https://www.pvk.ca/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator</id>
    <content type="html"><![CDATA[<p><small>Originally posted on the <a href="https://engineering.backtrace.io/2021-08-04-slitter-a-slab-allocator-that-trusts-but-verifies/">Backtrace I/O tech blog</a>.</small></p>

<p><a href="https://github.com/backtrace-labs/slitter">Slitter</a> is Backtrace’s
deliberately middle-of-the-road
<a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">thread-caching</a>
<a href="https://people.eecs.berkeley.edu/~kubitron/courses/cs194-24-S13/hand-outs/bonwick_slab.pdf">slab allocator</a>,
with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/include/slitter.h#L7-L44">explicit allocation class tags</a>
(rather than derived from the object’s size class).
It’s mostly written in Rust, and we use it in our C backend server.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a>’s design is about as
standard as it gets: we hope to dedicate the project’s complexity
budget to always-on “observability” and safety features.  We don’t
wish to detect all or even most memory management errors, but we
should statistically catch a small fraction (enough to help pinpoint
production issues) of such bugs, and <em>always</em> constrain their scope to
the mismanaged allocation class.<sup id="fnref:blast-radius" role="doc-noteref"><a href="#fn:blast-radius" class="footnote" rel="footnote">1</a></sup></p>

<p>We decided to code up Slitter last April, when we noticed
that we would immediately benefit from backing allocation with
temporary file mappings:<sup id="fnref:when-does-it-flush" role="doc-noteref"><a href="#fn:when-does-it-flush" class="footnote" rel="footnote">2</a></sup>
the bulk of our data is mapped from
persistent data files, but we also regenerate some cold metadata
during startup, and accesses to that metadata have amazing locality,
both temporal and spatial (assuming bump allocation).  We don’t want the OS
to swap out all the heap–that way lie <a href="https://blog.acolyer.org/2017/06/15/gray-failure-the-achilles-heel-of-cloud-scale-systems/">grey failures</a>–so
we opt specific allocation classes into it.</p>

<p>By itself, this isn’t a reason to write a slab allocator: we could
easily have configured <a href="http://jemalloc.net/jemalloc.3.html#arena.i.extent_hooks">specialised arenas in jemalloc</a>,
for example.  However, we also had eyes on longer term improvements to
observability and debugging or mitigation of memory management errors in
production, and those could only be unlocked by migrating to an
interface with explicit tags for each allocation class (type).</p>

<p>Classic mallocs like <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
and <a href="https://github.com/google/tcmalloc">tcmalloc</a> are fundamentally
unable to match that level of integration: we can’t tell <code>malloc(3)</code>
what we’re trying to allocate (e.g., a <code>struct request</code> in the HTTP
module), only its size.  It’s still possible to wrap malloc in a richer
interface, and, e.g., track heap consumption by tag.  Unfortunately,
the result is slower than a native solution, and, without help from
the underlying allocator, it’s easy to incorrectly match tags between
<code>malloc</code> and <code>free</code> calls.  In my experience, this frequently leads to
useless allocation statistics, usually around the very faulty code
paths one is attempting to debug.</p>

<p>Even once we have built detailed statistics on top of a regular
malloc, it’s hard to convince the underlying allocator to only recycle
allocations within an object class: not only do mallocs eagerly
recycle allocations of similar sizes regardless of their type, but
they will also release unused runs of address space, or repurpose them
for totally different size classes.  That’s what mallocs are supposed
to do…  it just happens to also make debugging a lot harder when
things inevitably go wrong.<sup id="fnref:ub" role="doc-noteref"><a href="#fn:ub" class="footnote" rel="footnote">3</a></sup></p>

<p>Slab allocators work with semantically richer allocation tags: an
allocation tag describes its objects’ size, but can also specify how
to initialise, recycle, or deinitialise them.  The problem is that
slab allocators tend to focus exclusively on speed.</p>

<p><a href="https://github.com/omniti-labs/portableumem">Forks of libumem</a>
may be the exception, thanks to the Solaris culture of pervasive
hooking.  However, <code>umem</code>’s design reflects the sensibilities of the
00s, when it was written: threads share a few caches, and the
allocator tries to reuse address space.  In contrast, Slitter assumes memory
is plentiful enough for thread-local caches and type-stable
allocations.<sup id="fnref:not-as-configurable" role="doc-noteref"><a href="#fn:not-as-configurable" class="footnote" rel="footnote">4</a></sup></p>

<h2 id="our-experience-so-far">Our experience so far</h2>

<p>We have been running <a href="https://crates.io/crates/slitter">Slitter</a> in
production for over two months, and rely on it to:</p>

<ul>
  <li>detect when an allocation is freed with the wrong allocation class
tag (i.e., detect type confusion on free).</li>
  <li>avoid any in-band metadata: there are guard pages between
allocations and allocator metadata, and no intrusive freelist for
use-after-frees to stomp over.</li>
  <li>guarantee <a href="https://www.usenix.org/legacy/publications/library/proceedings/osdi96/full_papers/greenwald/node2.html">type stable allocations</a>:
once an address has been used to fulfill a request for a certain
allocation class, it will only be used for that class.  Slitter
doesn’t overlay intrusive lists on top of freed allocations, so the
data always reflects what the application last stored there.  This
means that double-frees and use-after-frees only affect the faulty
allocation class.  An application could even rely on
read-after-free being benign to simplify non-blocking algorithms.<sup id="fnref:but-we-dont-depend-on-it-too-much" role="doc-noteref"><a href="#fn:but-we-dont-depend-on-it-too-much" class="footnote" rel="footnote">5</a></sup></li>
  <li>let each allocation class specify how its backing memory should
be mapped in (e.g., plain 4 KB pages or file-backed swappable pages).</li>
</ul>

<p>Thanks to extensive contracts and a mix of hardcoded and random tests,
we encountered only two issues during the initial rollout, both in the
small amount of lock-free C code that is hard to test.<sup id="fnref:legacy-gcc" role="doc-noteref"><a href="#fn:legacy-gcc" class="footnote" rel="footnote">6</a></sup></p>

<p>Type stability exerts a heavy influence all over Slitter’s design, and
has obvious downsides.  For example, a short-lived application that
progresses through a pipeline of stages, where each stage allocates
different types, would definitely waste memory if it were to replace a
regular malloc with a type-stable allocator like Slitter.  We believe
the isolation benefits are more than worth the trouble, at least for
long-lived servers that quickly enter a steady state.</p>

<p>In the future, we hope to also:</p>

<ul>
  <li>detect when an interior pointer is freed.</li>
  <li>detect simple<sup id="fnref:jump" role="doc-noteref"><a href="#fn:jump" class="footnote" rel="footnote">7</a></sup> buffer overflows that cross allocation classes, by inserting guard pages.</li>
  <li>always detect frees of addresses Slitter does not manage.</li>
  <li>detect most back-to-back double-frees.</li>
  <li>detect a random fraction of buffer overflows, with a sampling <a href="https://en.wikipedia.org/wiki/Electric_Fence">eFence</a>.</li>
</ul>

<p>In addition to these safety features, we plan to rely on the allocator
to improve observability into the calling program, and wish to:</p>

<ul>
  <li>track the number of objects allocated and recycled in each
allocation class.</li>
  <li>sample the call stack when the heap grows.</li>
  <li>track allocation and release call stacks for a small fraction of objects.</li>
</ul>

<p>Here’s how it currently works, and why we wrote it in Rust, with dash
of C.</p>

<h2 id="the-high-level-design-of-slitter">The high level design of Slitter</h2>

<p>At a <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/doc/design.md">high level</a>,
Slitter</p>

<ol>
  <li>reserves shared 1 GB <code>Chunk</code>s of memory via the <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/src/mapper.rs"><code>Mapper</code> trait</a></li>
  <li>carves out smaller type-specific <code>Span</code>s from each chunk with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs"><code>Mill</code> objects</a></li>
  <li>bump-allocates objects from <code>Span</code>s with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/press.rs"><code>Press</code> objects</a>, into <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/magazine.rs">allocation <code>Magazines</code></a></li>
  <li>pushes and pops objects into/from <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/c/cache.c">thread-local magazines</a></li>
  <li>caches populated magazines in global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/class.rs#L62-L67">type-specific lock-free stacks</a></li>
  <li>manages empty magazines with a global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/rack.rs">mostly lock-free <code>Rack</code></a></li>
</ol>

<p>Many general purpose memory allocators implement strategies similarly
inspired by <a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">Bonwick’s slab allocator</a>,
and time-tested mallocs may well provide better performance
and lower fragmentation than Slitter.<sup id="fnref:type-stable" role="doc-noteref"><a href="#fn:type-stable" class="footnote" rel="footnote">8</a></sup>
The primary motivation for designing Slitter is that having explicit
allocation classes in the API makes it easier for the allocator to
improve the debuggability and resilience of the calling program.<sup id="fnref:also-good-for-perf" role="doc-noteref"><a href="#fn:also-good-for-perf" class="footnote" rel="footnote">9</a></sup>
For example, most allocators can tell you the size of your program’s
heap, but that data is much more useful when broken down by struct
type or program module.</p>

<p>Most allocators try to minimise accesses to the metadata associated with
allocations.  In fact, that’s often seen as a strength of the slab
interface: the allocator can just rely on the caller to pass the
correct allocation class tag, instead of hitting metadata to figure
out there the freed address should go.</p>

<p>We went in the opposite direction with Slitter.  We still rely on the
allocation class tag for speed, but also actively look for mismatches
before returning from deallocation calls. Nothing depends on
values computed by the mismatch detection logic, and the resulting
branch is trivially predictable (the tag always matches), so we can
hope that wide out-of-order CPUs will hide most of the checking
code, if it’s simple enough.</p>

<p>This concern (access to metadata in few instructions) combined with
our goal of avoiding in-band metadata lead to a
<a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs#L6-L19">simple layout for each chunk’s data and metadata</a>.</p>

<pre><code>.-------.------.-------|---------------.-------.
| guard | meta | guard | data ... data | guard |
'-------'------'-------|---------------'-------'
  2 MB    2 MB   2 MB  |      1 GB        2 MB
                       v
               Aligned to 1 GB
</code></pre>

<p>A chunk’s data is always a 1 GB address range, aligned to 1 GB: the
underlying mapper doesn’t have to immediately back that with memory,
but it certainly can, e.g., in order to use gigantic pages.  The chunk
is preceded and followed by 2 MB guard pages.  The metadata for the
chunk’s data lives in a 2 MB range, just before the preceding guard
page (i.e., 4 MB to 2 MB before the beginning of the aligned 1 GB
range).  Finally, the 2 MB metadata range is itself preceded by a 2MB
guard page.</p>

<p>Each chunk is statically divided in 65536 spans of 16 KB each.  We can
thus map a span to its slot in the metadata block with a shifts,
masks, and some address arithmetic.  <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs">Mill</a>s
don’t have to hand out individual 16 KB spans at a time, they simply
have to work in multiples of 16 KB, and never split a span in two.</p>

<h2 id="why-we-wrote-slitter-in-rust-and-c">Why we wrote Slitter in Rust and C</h2>

<p>We call Slitter from C, but wrote it in Rust, despite the
more painful build<sup id="fnref:uber-crate" role="doc-noteref"><a href="#fn:uber-crate" class="footnote" rel="footnote">10</a></sup> process: that pain isn’t going
anywhere, since we expect our backend to be in a mix of C, C++, and
Rust for a long time.  We also sprinkled in some C when the
alternative would have been to pull in a crate just to make a couple
syscalls, or to enable unstable Rust features: we’re not
“rewrite-it-in-Rust” absolutists, and merely wish to use Rust for its
strengths (control over data layout, support for domain-specific
invariants, large ecosystem for less performance-sensitive logic, ability to
lie to the compiler where necessary, …), while avoiding its
weaknesses (interacting with Linux interfaces defined by C headers, or
fine-tuning code generation).</p>

<p>The majority of allocations only interact with the thread-local
magazines.  That’s why we <a href="https://github.com/backtrace-labs/slitter/blob/main/c/cache.c">wrote that code in C</a>:
stable Rust doesn’t (yet) let us access <a href="https://doc.rust-lang.org/std/intrinsics/fn.likely.html">likely/unlikely annotations</a>,
nor <a href="https://www.akkadia.org/drepper/tls.pdf#page=35">fast “initial-exec”</a> <a href="https://github.com/rust-lang/rust/issues/29594">thread-local storage</a>.
Of course, allocation and deallocation are the main entry points into
a memory allocation library, so this creates a bit of friction with
Rust’s linking process.<sup id="fnref:bad-linker" role="doc-noteref"><a href="#fn:bad-linker" class="footnote" rel="footnote">11</a></sup></p>

<p>We also had to implement our <a href="https://github.com/backtrace-labs/slitter/blob/main/c/stack.c">lock-free multi-popper Treiber stack</a>
in C: x86-64 doesn’t have anything like LL/SC, so we instead pair
the top-of-stack pointer with a generation counter… and 
<a href="https://github.com/rust-lang/rust/issues/32976#issuecomment-641360955">Rust hasn’t stabilised 128-bit atomics</a> yet.</p>

<p>We chose to use atomics in C instead of a simple lock in Rust because
the lock-free stack (and the atomic bump pointer, which Rust handles
fine) are important for our use case: when we rehydrate cold metadata
at startup, we do so from multiple I/O-bound threads, and we have
observed hiccups due to lock contention in malloc.  At some point,
lock acquisitions are rare enough that contention isn’t an issue;
that’s why we’re comfortable with locks when refilling bump allocation
regions.</p>

<h2 id="come-waste-performance-on-safety">Come waste performance on safety!</h2>

<p>A recurring theme in the design of <a href="https://github.com/backtrace-labs/slitter">Slitter</a>
is that we find ways to make the core (de)allocation logic slightly
faster, and immediately spend that efficiency on safety, debuggability
or, eventually, observability.  For a lot of code, performance is a
constraint to satisfy, not a goal to maximise; once we’re close to
good enough, it makes sense to trade performance
away.<sup id="fnref:even-works-for-perf" role="doc-noteref"><a href="#fn:even-works-for-perf" class="footnote" rel="footnote">12</a></sup> I also believe that there are
<a href="https://research.google/pubs/pub50370/">lower hanging fruits in memory placement</a>
than shaving a few nanoseconds from the allocation path.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a> also focuses on
instrumentation and debugging features that are always active, even in
production, instead of leaving that to development tools, or to logic
that must be explicitly enabled.  In a SaaS world, development and
debugging is never done.  Opt-in tools are definitely useful, but
always-on features are much more likely to help developers catch
the rarely occurring bugs on which they tend to spend an inordinate
amount of investigation effort (and if a debugging feature can be
safely enabled in production at a large scale, why not leave it
enabled forever?).</p>

<p>If that sounds like an interesting philosophy for a slab allocator,
<a href="https://github.com/backtrace-labs/slitter">come hack on Slitter</a>!
Admittedly, the value of Slitter isn’t as clear for pure Rust hackers
as it is for those of us who blend C and Rust, but per-class allocation
statistics and placement decisions should be useful, even in safe
Rust, especially for larger programs with long runtimes.</p>

<p>Our <a href="https://github.com/backtrace-labs/slitter">MIT-licensed code is on github</a>,
there are <a href="https://github.com/backtrace-labs/slitter/issues">plenty of small improvements to work on</a>,
and, while we still have to re-review the documentation, it has decent
test coverage, and we try to write straightforward code.</p>

<p><small>This post was much improved by feedback from my beta readers, Barkley, David,
Eloise, Mark, Per, Phil, Ruchir, and Samy.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:blast-radius" role="doc-endnote">
      <p>In my experience, their unlimited blast radius is what makes memory management bugs so frustrating to track down.  The design goals of generic memory allocators (e.g., recycling memory quickly) and some implementation strategies (e.g., <a href="http://phrack.org/issues/57/9.html#article">in-band metadata</a>) make it easy for bugs in one module to show up as broken invariants in a completely unrelated one that happened to share allocation addresses with the former.  <a href="http://phrack.org/issues/57/8.html#article">Adversarial thinkers</a> will even exploit the absence of isolation to <a href="https://www.openwall.com/articles/JPEG-COM-Marker-Vulnerability">amplify small programming errors into arbitrary code execution</a>.  Of course, one should simply not write bugs, but when they do happen, it’s nice to know that the broken code most likely hit itself and its neighbours in the callgraph, and not unrelated code that also uses the same memory allocator (something Windows got right with private heaps). <a href="#fnref:blast-radius" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:when-does-it-flush" role="doc-endnote">
      <p>Linux does not have anything like the <a href="https://www.freebsd.org/cgi/man.cgi?query=mmap&amp;sektion=2&amp;format=html#:~:text=a%20core%20file.-,MAP_NOSYNC,-Causes%20data%20dirtied">BSD’s <code>MAP_NOSYNC</code> mmap flag</a>.  This has historically created problems for <a href="https://lkml.org/lkml/2013/9/7/135">heavy mmap users like LMDB</a>.  Empirically, Linux’s flushing behaviour is much more reasonable these days, especially when dirty pages are a small fraction of physical RAM, as it is for us: in a well configured installation of our backend server, most of the RAM goes to clean file mappings, so only the <a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt"><code>dirty_expire_centisec</code> timer</a> triggers write-outs, and we haven’t been growing the file-backed heap fast enough for the time-based flusher to thrash too much. <a href="#fnref:when-does-it-flush" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ub" role="doc-endnote">
      <p>There are obvious parallels with undefined behaviour in C and C++… <a href="#fnref:ub" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-as-configurable" role="doc-endnote">
      <p>umem also takes a performance hit in order to let object classes define callbacks for object initialisation, recycling, and destruction.  It makes sense to let the allocator do some pre-allocation work: if you’re going to incur a cache miss for the first write to an allocation, it’s preferable to do so before you immediately want that newly allocated object (yes, profiles will show more cycles in the allocators, but you’re just shifting work around, hopefully farther from the critical path). Slitter only supports the bare minimum: objects are either always zero-initialised, or initially zero-filled and later left untouched.  That covers the most common cases, without incurring too many branch mispredictions. <a href="#fnref:not-as-configurable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-we-dont-depend-on-it-too-much" role="doc-endnote">
      <p>One could be tempted to really rely on it not just for isolation and resilience, but during normal operations.  That sounds like a bad idea (we certainly haven’t taken that leap), at least until <a href="https://github.com/backtrace-labs/slitter/issues/11">Slitter works with Valgrind/ASan/LSan</a>: it’s easier to debug easily reproducible issues when one can just slot in calls to regular malloc/calloc/free with a dedicated heap debugger. <a href="#fnref:but-we-dont-depend-on-it-too-much" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:legacy-gcc" role="doc-endnote">
      <p>It would be easy to blame the complexity of lock-free code, but the initial version, with C11 atomics, was correct.  Unfortunately, gcc backs C11 atomic <code>uint128_t</code>s with locks, so we had to switch to the legacy interface, and that’s when the errors crept in. <a href="#fnref:legacy-gcc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:jump" role="doc-endnote">
      <p>There isn’t much the allocator can do if an application writes to a wild address megabytes away from the base object.  Thankfully, buffer overflows tend to proceed linearly from the actual end of the undersized object. <a href="#fnref:jump" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:type-stable" role="doc-endnote">
      <p>In fact, Slitter actively worsens external fragmentation to guarantee type-stable allocations. We think it’s reasonable to sacrifice heap footprint in order to control the blast radius of use-after-frees and double-frees. <a href="#fnref:type-stable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:also-good-for-perf" role="doc-endnote">
      <p>That’s why we’re interested in allocation class tags, but they can also help application and malloc performance.  Some malloc developers are looking into tags for placement (should the allocation be backed by memory local to the NUMA node, with huge pages, …?) or lifetime (is the allocation immortal, short-lived, or tied to a request?) hints. <a href="#fnref:also-good-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:uber-crate" role="doc-endnote">
      <p>We re-export our dependencies from an uber-crate, and let our outer <a href="https://mesonbuild.com/">meson</a> build invoke <code>cargo</code> to generate a static library for that facade uber-crate. <a href="#fnref:uber-crate" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bad-linker" role="doc-endnote">
      <p><a href="https://github.com/rust-lang/rfcs/issues/2771">Rust automatically hides foreign symbols when linking <code>cdylib</code>s</a>.  We worked around that with static linking, but statically linked rust libraries are mutually incompatible, hence the uber-crate. <a href="#fnref:bad-linker" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:even-works-for-perf" role="doc-endnote">
      <p>And not just for safety or productivity features!  I find it often makes sense to give up on small performance wins (e.g., aggressive autovectorisation or link-time optimisation) when they would make future performance investigations harder.  The latter are higher risk, and only potential benefits, but their upside (order of magnitude improvements) dwarfs guaranteed small wins that freeze the code in time. <a href="#fnref:even-works-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Entomological solutions]]></title>
    <link href="https://www.pvk.ca/Blog/2021/06/07/entomological-solutions/"/>
    <updated>2021-06-07T00:36:03-04:00</updated>
    <id>https://www.pvk.ca/Blog/2021/06/07/entomological-solutions</id>
    <content type="html"><![CDATA[<p>Non-blocking algorithms have a reputation for complexity.  However, if
you ask people who work on systems where strong progress guarantees
are mandatory (e.g., hard real-time systems), they’ll often disagree.</p>

<p>I believe the difference is rooted in the way systems which <em>must</em>
bound their pauses will sacrifice nice-to-haves to more cleanly
satisfy that hard requirement.</p>

<p>Researchers and library writers instead tend to aim for maximal
guarantees or functionality while assuming (sacrificing) as little as
possible.  Someone who’s sprinkling lock-free algorithms in a large
codebase will similarly want to rely on maximally general algorithms,
in order to avoid increasing the scope of their work: if
tail latency and lock-freedom were high enough priorities to justify
wide-ranging changes, the program would probably have been designed
that way from the start.</p>

<p>It makes sense to explore general solutions, and academia is
definitely a good place to do so.  It was fruitful for mathematicians
to ask questions about complex numbers, then move on to fields, rings,
groups, etc., like sadistic kids probing what a bug can still do as
they tear its legs off one by one.  Quicksort and mergesort are
probably the strongest exemplars of that sort of research in computer
science: we don’t even ask what data is made of before assuming a
comparison sort is probably a good idea.</p>

<p>It is however more typical to trade something in return for
generality.  When there’s no impact on performance or resource
usage, code complexity usually takes a hit.</p>

<p>When solving a core problem like lock-freedom in a parallel
realtime system, we instead ask how much more we can assume, what else
we can give up, in order to obtain simpler, more robust solutions.
We don’t want generality, we’re not crippling bugs; we want
specificity, we’re dosing eggs with <a href="https://en.wikipedia.org/wiki/Antennapedia">Hox to get more legs</a>.</p>

<p>The first time someone used to academic non-blocking algorithms hears
about the resulting maximally specialised solutions, they’ll sometimes
complain about “cheating.” Of course, it’s never cheating when a
requirement actually is met; the surprise merely shows that the rules
typically used to evaluate academic solutions are but approximations
of reality, and can be broken…  and practitioners faced with
specific problems are ideally placed to determine what rules
they can flout.</p>

<h2 id="hoarding-is-caring">Hoarding is caring?</h2>

<p>My favourite example of such cheating is type-stable memory.  The
literature on Safe memory reclamation (SMR) conflates<sup id="fnref:or-does-it" role="doc-noteref"><a href="#fn:or-does-it" class="footnote" rel="footnote">1</a></sup> two
problems that are addressed by SMR algorithms: reclamation races, and
the ABA problem.</p>

<p>A reclamation race is what happens when a thread dereferences a
pointer to the heap, but the pointee has already been deallocated;
even when the racy accesses are loads, they can result in a
segmentation fault (and a crash).</p>

<p>The ABA problem is what happens when a descriptor (e.g., a pointer) is
reused to refer to something else, but some code is unaware of the
swap. For example, a thread could load a global pointer to a logically
read-only object, read data off that pointer, sleep for a while, and
observe that the global pointer has the same value. That does not mean
nothing has changed: while the thread was sleeping, the pointee could
have been freed, and then recycled to satisfy a fresh allocation.</p>

<p>Classic SMR algorithms like <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation</a>
and <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">hazard pointers</a>
solve both problems at once; in fact, addressing
reclamation races is their core contribution (it’s certainly the
challenging part), and ABA protection is simply a nice corollary.</p>

<p>However, programs <em>can choose</em> not to crash on benign use-after-free:
reading from freed objects only triggers crashes when memory is mapped
and unmapped dynamically, and that’s usually not an option for hard
realtime systems.  On smaller embedded targets, there’s a fixed
physical memory budget; either the program fits, or the program is
broken.  Larger shared-memory parallel programs often can’t afford the IPIs
and other hiccups associated with releasing memory to the operating
system.  Either way, half the problem solved by SMR doesn’t even exist
for them.</p>

<p>The other half, ABA, is still an issue…  but that subproblem is
easier to solve.  For example, we can tag data with sequence counters.<sup id="fnref:reuse-dont-recycle" role="doc-noteref"><a href="#fn:reuse-dont-recycle" class="footnote" rel="footnote">2</a></sup></p>

<h2 id="lock-free-stacks-and-aba">Lock-free stacks and ABA</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Treiber_stack">lock-free multiple producers / single consumer linked stack</a> might be
the simplest lock-free data structure.<sup id="fnref:but-maybe-not-scalable" role="doc-noteref"><a href="#fn:but-maybe-not-scalable" class="footnote" rel="footnote">3</a></sup></p>

<p>Pushing a new record to such a stack is easy:<sup id="fnref:not-in-c" role="doc-noteref"><a href="#fn:not-in-c" class="footnote" rel="footnote">4</a></sup> load the current top
of stack pointer, publish that in our <a href="https://en.wikipedia.org/wiki/CAR_and_CDR">new record’s “next” (CDR) field</a>,
and attempt to replace the top of stack with a pointer to our new
record with a compare-and-swap (CAS).</p>

<p>How do we consume from such a stack?</p>

<p>The simplest way is to use a fetch-and-set (atomic exchange) to
simultaneously clear the stack (set the top-of-stack pointer to the
“empty stack” sentinel, e.g., <code>NULL</code>) and read the previous
top-of-stack.  Any number of consumers can concurrently execute such a
batch pop, although only one will grab everything.</p>

<p>Alternatively, if there’s only one consumer at a time, it can pop with
a compare-and-swap.  The consumer must load the current top-of-stack
pointer.  If the stack is empty, there’s nothing to pop.  Otherwise,
it can read the top record’s “next” field, and attempt to CAS out the
top-of-stack pointer from the one it just read to the “next” record.</p>

<p>The tricky step here is the one where the consumer reads the “next”
field in the current top-of-stack record: that step would be subject
to reclamation races, except that there’s only one consumer, so
we know no one else concurrently popped that record and freed it.</p>

<p>What can we do to support multiple consumers? Can we
simply make sure that stack records are always safe to read, e.g., by
freeing them to an object pool? Unfortunately, while
use-after-free is benign for producers, it is not for
consumers.</p>

<p>The key problem is that a consumer can observe that the top of stack
points to record A, and that record A’s “next” field points to B, and
then get stuck or sleep for a while.  During that time, another thread
pops A and B, frees both, pushes C, and then pushes A’, a new record
that happens to have the same address as A.  Finally, the initial
consumer will compare the top-of-stack pointers with A (which also
matches A’), and swap that for B, resurrecting a record that has
already been consumed and freed.</p>

<p>Full-blown SMR would fix all that.  However, if we can instead assume
read-after-free do not crash (e.g., we use a type-stable allocator or
an explicit object pool for records), we simply have to reliably
detect when a record has returned to the top of the stack.<sup id="fnref:ll-sc" role="doc-noteref"><a href="#fn:ll-sc" class="footnote" rel="footnote">5</a></sup></p>

<p>We can do that by tagging the top-of-stack pointer with a sequence
counter, and update both with a double-wide compare-and-swap: instead
of CASing the top-of-stack pointer, we want to CAS that pointer and
its monotonically increasing counter.  Every successful CAS of the
pointer will also increment the counter by one, so the sequence counter
will differ when a record is popped and pushed back on the stack.</p>

<p>There is still a risk of ABA: the counter can wrap around.  That’s not
a practical concern with 64-bit counters, and there are reasonable
arguments that narrower counters are safe because no consumer will
stay “stuck” for minutes or hours.<sup id="fnref:bitpacking" role="doc-noteref"><a href="#fn:bitpacking" class="footnote" rel="footnote">6</a></sup></p>

<h2 id="single-compare-multiple-swaps">Single-compare multiple-swaps</h2>

<p>Sometimes, the application can naturally guarantee that CASed fields
are ABA-free.</p>

<p>For example, a hierarchical bump allocator may carve out global
type-specific arenas from a shared chunk of address space, and satisfy
allocations for each object type from the type’s current arena.
Within an arena, allocations are reserved with atomic increments.
Similarly, we carve out each arena from the shared chunk of address
space with a (larger) atomic increment.
Neither bump pointer ever decreases: once a region of address space
has been converted to an arena, it stays that way, and once an object
has been allocated from an arena, it also remains allocated (although
it might enter a freelist).  Arenas are also never recycled: once
exhausted, they stay exhausted.</p>

<p>When an allocation type has exhausted its current arena (the arena’s
bump pointer is exhausted), we want to atomically grab a new arena
from the shared chunk of address space, and replace the type’s arena
pointer with the newly created arena.</p>

<p>A lock-free algorithm for such a transaction looks like it would have
to build on top of multi-word compare-and-swap (MCAS), a hard operation that can
be <a href="https://www.cl.cam.ac.uk/research/srg/netos/papers/2002-casn.pdf">implemented in a wait-free manner, but with complex algorithms</a>.</p>

<p>However, we know that the compare-and-swapped state evolves
monotonically: once an arena has been carved out from the shared
chunk, it will never be returned as a fresh arena again.  In other
words, there is no ABA, and a compare-and-swap on an arena pointer
will never succeed spuriously.</p>

<p>Monotonicity also means that we can acquire a consistent snapshot of both
the type’s arena pointer and the chunk’s allocation state by reading
everything twice.  Values are never repeated, so any write
that executes concurrently with our snapshot loop will be detected: the
first and second reads of the updated data will differ.</p>

<p>We also know that the only way a type’s arena pointer can be replaced
is by allocating a new one from the shared chunk.  If we took a
consistent snapshot of the type’s arena pointer and of the shared
chunk’s allocation state, and the allocation state hasn’t changed
since, the arena pointer must also be unchanged (there’s a
hierarchy).</p>

<p>We can combine all these properties to atomically replace a type’s
arena pointer with a new one obtained from the shared chunk, using a
much simpler core operation, a single-compare multiple-swap (SCMS). We
want to execute a series of CASes (one to allocate an arena
in the chunk, a few to initialise the arena, and one to publish
the new arena), but we can also assume that once the first
updated location matches the CAS’s expected value, all other ones will
as well.  In short, only the first CAS may fail.</p>

<p>That’s the key simplifier compared to full-blown multi-word
compare-and-swap algorithms: they have to incrementally acquire update
locations, any of which might turn the operation into a failure.</p>

<p>We can instead encode all the CASes in a transaction descriptor, CAS
that descriptor in the first update location, and know that the
multi-swaps will all succeed iff that CAS is successful.</p>

<p>If the first CAS is successful, we also know that it’s safe to execute
the remaining CASes, and finally replace the descriptor with its
final value with one last CAS.  We don’t even have to publish the
descriptor to all updated locations, because concurrent allocations
will notice the current arena has been exhausted, and try to get a new
one from the shared chunk… at which point they will notice the
transaction descriptor.</p>

<p>All the CASes after the first one are safe to execute arbitrarily
often thanks to monotonicity.  We already know that any descriptor that has
been published with the initial CAS will not fail, which means the only
potential issue is spuriously successful CASes… but our mutable
fields never repeat a value, so that can’t happen.</p>

<p>The application’s guarantee of ABA-safety ends up really simplifying
this single-compare multiple-swap algorithm (SCMS), compared to a
multi-word compare-and-swap (MCAS).  In a typical MCAS implementation, helpers
must abort when they detect that they’re helping a MCAS operation that
has already failed or already been completed.  Our single-compare
assumption (once the first CAS succeeds, the operation succeeds) takes
care of the first case: helpers never see failed operations.  Lack
of ABA means helpers don’t have to worry about their CASes succeeding
after the SCMS operation has already been completed: they will always fail.</p>

<p>Finally, we don’t even need any form of SMR on the transaction
descriptor: <a href="https://arxiv.org/pdf/1708.01797.pdf#page=4">a sequence counter in the descriptor and a copy of that counter in a tag next to pointers to that descriptor</a>
suffice to disambiguate incarnations of the same physical descriptor.</p>

<p>Specialising to the allocator’s monotonic state let us use
single-compare multiple-swap, a simplification of full multi-word
compare-and-swap, and further specialising that primitive for
monotonic state let us get away with nearly half as many CASes (k + 1
for k locations) as the state of the art for MCAS (2k + 1 for k locations).</p>

<h2 id="a-plea-for-integration">A plea for integration</h2>

<p>There is a common thread between never unmapping allocated addresses,
sequence tags, type-stable memory, and the allocator’s single-compare
multiple-swap: monotonicity.</p>

<p>The lock-free stack shows how easy it is to conjure up artificial
monotonicity.  However, when we integrate algorithms more tightly
with the program and assume the program’s state is naturally
monotonic, we’ll often unlock simpler and more efficient solutions.  I
also find there’s something of a virtuous cycle: it’s easier for a
module to guarantee monotonicity to its components when it itself only
has to handles monotonic state, like a sort of end-to-end monotonicity
principle.</p>

<p>Unfortunately, it’s not clear how much latent monotonicity there is
in real programs.  I suppose that makes it hard to publish algorithms
that assumes its presence.  I think it nevertheless makes sense to
explore such stronger assumptions, in order to help practitioners estimate
what we could gain in exchange for small sacrifices.</p>

<p>Asymmetric synchronisation is widely used these days, but I imagine it
was once unclear how much practical interest there might be in that
niche; better understood benefits lead to increased adoption.  I hope
the same can happen for algorithms that assume monotonic state.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:or-does-it" role="doc-endnote">
      <p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.5131&amp;rep=rep1&amp;type=pdf">Maged Michael’s original Safe Memory Reclamation paper</a> doesn’t: allowing arbitrary memory management is the paper’s main claim.  I think there’s a bit of a first mover’s advantage, and researchers are incentivised to play within the sandbox defined by Michael.  For example, <a href="https://arxiv.org/abs/1708.01797">Arbel-Raviv and Brown in “Reuse, don’t Recycle”</a> practically hide the implementation of their proposal on page 17 (Section 5), perhaps because a straightforward sequence counter scheme is too simple for publication nowadays. <a href="#fnref:or-does-it" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:reuse-dont-recycle" role="doc-endnote">
      <p>See <a href="https://arxiv.org/abs/1708.01797">Reuse, don’t Recycle</a> for a flexible take. <a href="#fnref:reuse-dont-recycle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-maybe-not-scalable" role="doc-endnote">
      <p>A stack fundamentally focuses contention towards the top-of-stack pointer, so lock-free definitely doesn’t imply scalable. It’s still a good building block. <a href="#fnref:but-maybe-not-scalable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-in-c" role="doc-endnote">
      <p>In assembly language, anyway. Language memory models make this surprisingly hard. For example, any ABA in the push sequence is benign (we still have the correct bit pattern in the “next” field), but C and C++’s pointer provenance rules say that accessing a live object through a pointer to a freed object that happens to alias the new object is undefined behaviour. <a href="#fnref:not-in-c" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ll-sc" role="doc-endnote">
      <p>Load Linked / Store Conditional solves <em>this specific problem</em>, but that doesn’t mean LL/SC as found on real computers is necessarily a better primitive than compare-and-swap. <a href="#fnref:ll-sc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bitpacking" role="doc-endnote">
      <p>Which is nice, because it means we can pack data and sequence counters in 64-bit words, and use the more widely available single-word compare-and-swap. <a href="#fnref:bitpacking" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Baseline implementations should be predictable]]></title>
    <link href="https://www.pvk.ca/Blog/2021/05/14/baseline-implementations-should-be-predictable/"/>
    <updated>2021-05-14T01:53:05-04:00</updated>
    <id>https://www.pvk.ca/Blog/2021/05/14/baseline-implementations-should-be-predictable</id>
    <content type="html"><![CDATA[<p>I wrote <a href="https://crates.io/crates/reciprocal">Reciprocal</a> because I
couldn’t find a nice implementation of div-by-mul in Rust without
data-dependent behaviour. Why do I care?</p>

<p>Like <a href="https://ridiculousfish.com/blog/posts/benchmarking-libdivide-m1-avx512.html">ridiculous fish mentions in his review of integer divisions on M1 and Xeon</a>,
certain divisors (those that lose a lot of precision when rounding up
to a fraction of the form \(n / 2^k\)) need a different, slower,
code path in classic implementations. Powers of two are also typically
different, but at least divert to a faster sequence, a variable right
shift.</p>

<p>Reciprocal instead <a href="https://github.com/pkhuong/reciprocal/blob/c4f6eeeb7108a778c6e8c1f8a5ac7c6df13e2943/src/lib.rs#L116">uses a unified code path</a>
to implement two expressions, 
\(f_{m,s}(x) = \left\lfloor \frac{m x}{2^s} \right\rfloor\) and
\(g_{m^\prime,s^\prime}(x) = \left\lfloor\frac{m^\prime \cdot \min(x + 1, \mathtt{u64::MAX})}{2^{s^\prime}}\right\rfloor\),
that are identical except for the saturating increment of \(x\) in
\(g_{m^\prime,s^\prime}(x)\).</p>

<p>The first expression, \(f_{m,s}(x)\) corresponds to the usual
div-by-mul approximation (implemented in gcc, LLVM, libdivide, etc.)
where the reciprocal \(1/d\) is approximated in fixed point by rounding
\(m\) <em>up</em>, with the upward error compensated by the truncating
multiplication at runtime.  See, for example, Granlund and
Montgomery’s <a href="https://gmplib.org/~tege/divcnst-pldi94.pdf">Division by invariant integers using multiplication</a>.</p>

<p>The second, \(g_{m^\prime,s^\prime}(x)\), is the multiply-and-add
scheme of described by Robison in <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.512.2627&amp;rep=rep1&amp;type=pdf">N-Bit Unsigned Division Via N-Bit Multiply-Add</a>.</p>

<p>In that approximation, the reciprocal multiplier \(m^\prime\) is
rounded <em>down</em> when converting \(1/d^\prime\) to fixed point.  At 
runtime, we then bump the product up (by the largest value
\(\frac{n}{2^{s^\prime}} &lt; 1/d^\prime\), i.e., \(\frac{m^\prime}{2^{s^\prime}}\)) before dropping the low bits.</p>

<p>With a bit of algebra, we see that \(m^\prime x + m^\prime = m^\prime (x + 1)\)…
and we can use a saturating increment to avoid a 64x65 multiplication
as long as we don’t trigger this second expression for divisors
\(d^\prime\) for which
\(\left\lfloor \frac{\mathtt{u64::MAX}}{d^\prime}\right\rfloor \neq \left\lfloor \frac{\mathtt{u64::MAX} - 1}{d^\prime}\right\rfloor\).</p>

<p>We have a pair of dual approximations, one that rounds the reciprocal
up to a fixed point value, and another that rounds down; it makes
sense to round to nearest, which nets us one extra bit of precision in
the worst case, compared to always applying one or the other.</p>

<p>Luckily,<sup id="fnref:or-is-it" role="doc-noteref"><a href="#fn:or-is-it" class="footnote" rel="footnote">1</a></sup>
<a href="https://github.com/pkhuong/reciprocal/blob/c4f6eeeb7108a778c6e8c1f8a5ac7c6df13e2943/src/lib.rs#L322">all of <code>u64::MAX</code>’s factors (except 1 and <code>u64::MAX</code>) work with the “round up” approximation</a>
that doesn’t increment, so the saturating increment is always safe
when we actually want to use the second “round-down” approximation
(unless \(d^\prime \in \{1, \mathtt{u64::MAX}\}\)).</p>

<p>This duality is the reason why Reciprocal can get away with
64-bit multipliers.</p>

<p>Even better, \(f_{m,s}\) and \(g_{m^\prime,s^\prime}\)
differ only in the absence or presence of a saturating increment.
Rather than branching, <a href="https://github.com/pkhuong/reciprocal/blob/c4f6eeeb7108a778c6e8c1f8a5ac7c6df13e2943/src/lib.rs#L116">Reciprocal executes a data-driven increment</a>
by 0 or 1,
for \(f_{m,s}(x)\) or \(g_{m^\prime,s^\prime}(x)\) respectively.
The upshot: predictable improvements over hardware division, even when
dividing by different constants.</p>

<p>Summary of the results below: when measuring the throughput of
independent divisions on my i7 7Y75 @ 1.3 GHz, Reciprocal consistently
needs 1.3 ns per division, while hardware division can only achieve
~9.6 ns / division (Reciprocal needs 14% as much / 86% less time).
This looks comparable to the
<a href="https://ridiculousfish.com/blog/posts/benchmarking-libdivide-m1-avx512.html#:~:text=intel%20xeon%203.0%20ghz%20(8275cl)">results reported by fish for libdivide when dividing by 7</a>.
Fish’s <a href="https://github.com/ridiculousfish/libdivide">libdivide</a> no
doubt does better on nicer divisors, especially powers of two, but
it’s good to know that a simple implementation comes close.</p>

<p>We’ll also see that, in Rust land, the
<a href="https://crates.io/crates/fastdivide">fast_divide crate</a>
is dominated by <a href="https://github.com/ejmahler/strength_reduce">strength_reduce</a>,
and that strength_reduce is only faster than <a href="https://github.com/pkhuong/reciprocal/">Reciprocal</a>
when dividing by powers of two (although, looking at the disassembly,
it probably comes close for single-result latency).</p>

<p>First, results for division with the same precomputed inverse.  The
timings are from
<a href="https://github.com/bheisler/criterion.rs">criterion.rs</a>, 
for <a href="https://github.com/pkhuong/reciprocal/blob/c4f6eeeb7108a778c6e8c1f8a5ac7c6df13e2943/benches/div_throughput.rs#L29">\(10^4\) divisions in a tight loop</a>.</p>

<ul>
  <li>“Hardware” is a regular HW DIV,</li>
  <li>“compiled” lets LLVM generate specialised code,</li>
  <li>“reciprocal” is <a href="https://github.com/pkhuong/reciprocal/blob/d591c59044b3a4f662112aae73c3adae9f168ea6/src/lib.rs#L11">PartialReciprocal</a>,<sup id="fnref:why-partial" role="doc-noteref"><a href="#fn:why-partial" class="footnote" rel="footnote">2</a></sup></li>
  <li>“strength_reduce” is the <a href="https://github.com/ejmahler/strength_reduce">strength_reduce crate’s u64 division</a>,</li>
  <li>and “fast_divide” is <a href="https://crates.io/crates/fastdivide">the fast_divide crate’s u64 division</a>.</li>
</ul>

<p>The last two options are the crates I considered before writing
Reciprocal.  The strength_reduce crate switches between a special
case for powers of two (implemented as a bitscan and a shift), and a
general slow path that handles everything with a 128-bit fixed
point multiplier.  fast_divide is inspired by libdivide and 
implements the same three paths: a fast path for powers of two (shift
right), a slow path for reciprocal multipliers that need one more bit
than the word size (e.g, division by 7), and a regular round-up
div-by-mul sequence.</p>

<p>Let’s look at <a href="https://github.com/pkhuong/reciprocal/blob/c4f6eeeb7108a778c6e8c1f8a5ac7c6df13e2943/benches/div_throughput.rs#L29">the three cases in that order</a>.</p>

<p>\(10^4\) divisions by 2 (i.e., a mere shift right by 1)</p>

<pre><code>hardware_u64_div_2      time:   [92.297 us 95.762 us 100.32 us]
compiled_u64_div_by_2   time:   [2.3214 us 2.3408 us 2.3604 us]
reciprocal_u64_div_by_2 time:   [12.667 us 12.954 us 13.261 us]
strength_reduce_u64_div_by_2
                        time:   [2.8679 us 2.9190 us 2.9955 us]
fast_divide_u64_div_by_2
                        time:   [2.7467 us 2.7752 us 2.8025 us]
</code></pre>

<p>This is the <em>comparative</em> worst case for Reciprocal: while Reciprocal
always uses the same code path (1.3 ns/division), the compiler shows
we can do much better with a right shift. Both branchy implementations
include a special case for powers of two, and thus come close to the
compiler, thanks a predictable branch into a right shift.</p>

<p>\(10^4\) divisions by 7 (a “hard” division)</p>

<pre><code>hardware_u64_div_7      time:   [95.244 us 96.096 us 97.072 us]
compiled_u64_div_by_7   time:   [10.564 us 10.666 us 10.778 us]
reciprocal_u64_div_by_7 time:   [12.718 us 12.846 us 12.976 us]
strength_reduce_u64_div_by_7
                        time:   [17.366 us 17.582 us 17.827 us]
fast_divide_u64_div_by_7
                        time:   [25.795 us 26.045 us 26.345 us]
</code></pre>

<p>Division by 7 is hard for compilers that do not implement the “rounded down”
approximation described in Robison’s
<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.512.2627&amp;rep=rep1&amp;type=pdf">N-Bit Unsigned Division Via N-Bit Multiply-Add</a>.
This is the comparative <em>best</em> case for Reciprocal, since it always
uses the same code (1.3 ns/division), but most other implementations
switch to a slow path (strength_reduce enters a general case that
is arguably more complex, but more transparent to LLVM). Even
divisions directly compiled with LLVM are ~20% faster than Reciprocal:
LLVM does not implement Robison’s round-down scheme, so it
<a href="https://godbolt.org/z/Wvn3hhahd">hardcodes a more complex sequence</a>
than Reciprocal’s.</p>

<p>\(10^4\) divisions by 11 (a regular division)</p>

<pre><code>hardware_u64_div_11     time:   [95.199 us 95.733 us 96.213 us]
compiled_u64_div_by_11  time:   [7.0886 us 7.1565 us 7.2309 us]
reciprocal_u64_div_by_11
                        time:   [12.841 us 13.171 us 13.556 us]
strength_reduce_u64_div_by_11
                        time:   [17.026 us 17.318 us 17.692 us]
fast_divide_u64_div_by_11
                        time:   [21.731 us 21.918 us 22.138 us]
</code></pre>

<p>This is a typical result. Again, Reciprocal can be trusted to work at
1.3 ns/division.  Regular round-up div-by-mul works fine when dividing
by 11, so code compiled by LLVM only needs a multiplication and a shift,
nearly twice as fast as Reciprocal’s generic sequence.  The fast_divide
crate does do better here than when dividing by 7, since it avoids the
slowest path, but Reciprocal is still faster; simplicity pays.</p>

<p>The three microbenchmarks above reward special-casing, since they always
divide by the same constant in a loop, and thus always hit the same
code path without ever incurring a mispredicted branch.</p>

<p>What happens to independent divisions <a href="https://github.com/pkhuong/reciprocal/blob/main/benches/div_throughput_variable.rs">by unpredictable precomputed divisors</a>,
for divisions by 2, 3, 7, or 11 (respectively easy, regular, hard, 
and regular divisors)?</p>

<pre><code>hardware_u64_div        time:   [91.592 us 93.211 us 95.125 us]
reciprocal_u64_div      time:   [17.436 us 17.620 us 17.828 us]
strength_reduce_u64_div time:   [40.477 us 41.581 us 42.891 us]
fast_divide_u64_div     time:   [69.069 us 69.562 us 70.100 us]
</code></pre>

<p>The hardware doesn’t care, and Reciprocal is only a bit slower (1.8
ns/division instead of 1.3 ns/division) presumably because the relevant
<code>PartialReciprocal</code> struct must now be loaded in the loop body.</p>

<p>The other two branchy implementations seemingly take a hit
proportional to the number of special cases. The <code>strength_reduce</code> hot
path only branches once, to detect divisors that are powers of two;
its runtime goes from 0.29 - 1.8 ns/division to 4.2 ns/division (at
least 2.4 ns slower/division).  The <code>fast_divide</code> hot path, like libdivide’s,
switches between <em>three</em> cases, and goes from 0.28 - 2.2
ns/division to 7.0 ns/division (at least 4.8 ns slower/division).</p>

<p>And that’s why I prefer to start with predictable baseline
implementations: unpredictable code with special cases can easily
perform well on benchmarks, but, early on during development, it’s
hard to tell how the benchmarks may differ from real workloads, and
whether the special cases “overfit” on these differences.</p>

<p>With special cases for classes of divisors, most runtime div-by-mul
implementations make you guess whether you’ll tend to divide by powers
of two, by “regular” divisors, or by “hard” ones in order to estimate
how they will perform.  Worse, they also force you to take into
account how often you’ll switch between the different classes.
Reciprocal does not have that problem: its hot path is the same
regardless of the constant divisor, so it has the same predictable
performance for all divisors,<sup id="fnref:partial" role="doc-noteref"><a href="#fn:partial" class="footnote" rel="footnote">3</a></sup>
and there’s only one code path, so we don’t have to worry about class
switches.</p>

<p>Depending on the workload, it may make sense to divert to faster code
paths, but it’s usually best to start without special cases when it’s
practical to do so…  and I think
<a href="https://crates.io/crates/reciprocal">Reciprocal</a> shows that, for
integer division by constants, it is.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:or-is-it" role="doc-endnote">
      <p>Is it luck?  Sounds like a fun number theory puzzle. <a href="#fnref:or-is-it" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:why-partial" role="doc-endnote">
      <p>The struct is “partial” because it can’t represent divisions by 1 or <code>u64::MAX</code>. <a href="#fnref:why-partial" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:partial" role="doc-endnote">
      <p>…all divisors except 1 and <code>u64::MAX</code>, which must instead <a href="https://github.com/pkhuong/reciprocal/blob/b2f1fa0093a3fbfb2758c434aba700bf852b4b30/src/lib.rs#L196">use the more general <code>Reciprocal</code> struct</a>. <a href="#fnref:partial" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stuff your logs!]]></title>
    <link href="https://www.pvk.ca/Blog/2021/01/11/stuff-your-logs/"/>
    <updated>2021-01-11T15:11:23-05:00</updated>
    <id>https://www.pvk.ca/Blog/2021/01/11/stuff-your-logs</id>
    <content type="html"><![CDATA[<p><small>Originally cross-posted on the <a href="https://engineering.backtrace.io/2021-01-11-stuff-your-logs/">Backtrace.io engineering blog</a>.</small></p>

<p>Nine months ago, we embarked on a format migration for the persistent
(on-disk) representation of variable-length strings like symbolicated
call stacks in the <a href="https://www.backtrace.io">Backtrace</a> server.  We
chose a variant of
<a href="http://www.stuartcheshire.org/papers/COBSforToN.pdf">consistent overhead byte stuffing (COBS)</a>,
a <a href="https://en.wikipedia.org/wiki/Self-synchronizing_code">self-synchronising code</a>,
for the metadata (variable-length as well).
This choice let us improve our software’s resilience to data
corruption in local files, and then parallelise data hydration, which
improved startup times by a factor of 10… without any hard
migration from the old to the current on-disk data format.</p>

<p>In this post, I will explain why I believe that the representation of
first resort for binary logs (write-ahead, recovery, replay, or
anything else that may be consumed by a program) should be
self-synchronising, backed by this migration and by prior experience
with COBS-style encoding.  I will also describe the
<a href="https://github.com/backtrace-labs/stuffed-record-stream">specific algorithm (available under the MIT license)</a>
we implemented for our server software.</p>

<p>This encoding offers low space overhead for framing, fast encoding and
faster decoding, resilience to data corruption, and a restricted form
of random access.  Maybe it makes sense to use it for your own data!</p>

<h2 id="what-is-self-synchronisation-and-why-is-it-important">What is self-synchronisation, and why is it important?</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Self-synchronizing_code">code is self-synchronising</a>
when it’s always possible to unambiguously detect where a valid code
word (record) starts in a stream of symbols (bytes).  That’s a
stronger property than prefix codes like Huffman codes, which only
detect when valid code words end.  For example, the UTF-8
encoding is self-synchronising, because initial bytes and continuation
bytes differ in their high bits.  That’s why it’s possible to decode
multi-byte code points when tailing a UTF-8 stream.</p>

<p>The UTF-8 code was designed for small integers (Unicode code points),
and can double the size of binary data.  Other encodings are more
appropriate for arbitrary bytes; for example,
<a href="http://www.stuartcheshire.org/papers/COBSforToN.pdf">consistent overhead byte stuffing (COBS)</a>,
a self-synchronising code for byte streams, offers a worst-case
space overhead of one byte plus a 0.4% space blow-up.</p>

<p>Self-synchronisation is important for binary logs because it lets us
efficiently (with respect to both run time and space overhead) frame
records in a simple and robust manner… and we want simplicity and
robustness because logs are most useful when something has gone wrong.</p>

<p>Of course, the storage layer should detect and correct errors, but
things will sometimes fall through, especially for on-premises
software, where no one fully controls deployments.  When that happens,
graceful partial failure is preferable to, e.g., losing all the
information in a file because one of its pages went to the great bit
bucket in the sky.</p>

<p>One easy solution is to spread the data out over multiple files or
blobs.  However, there’s a trade-off between keeping data
fragmentation and file metadata overhead in check, and minimising the
blast radius of minor corruption.  Our server must be able to run
on isolated nodes, so we can’t rely on design options available to
replicated systems… plus bugs tend to be correlated across replicas,
so there is something to be said for defense in depth, even with
distributed storage.</p>

<p>When each record is converted with a
<a href="https://en.wikipedia.org/wiki/Self-synchronizing_code">self-synchronising code</a>
like
<a href="https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing">COBS</a>
before persisting to disk, we can decode all records that weren’t
directly impacted by corruption, exactly like decoding a stream of
mostly valid UTF-8 bytes.  Any form of corruption will only make
us lose the records whose bytes were corrupted, and, at most, the two records
that immediately precede or follow the corrupt byte range.  This
guarantee covers overwritten data (e.g., when a network switch flips a
bit, or a read syscall silently errors out with a zero-filled page),
as well as bytes removed or garbage inserted in the middle of log
files.</p>

<p>The coding doesn’t store redundant information: replication or erasure
coding is the storage layer’s responsibility.  It instead guarantees
to <em>always</em> minimise the impact of corruption, and only lose records
that were adjacent to or directly hit by corruption.</p>

<p>A COBS encoding for log records achieves that by unambiguously
separating records with a reserved byte (e.g., 0), and re-encoding
each record to avoid that separator byte.  A reader can thus assume
that potential records start and end at a log file’s first and last
bytes, and otherwise look for separator bytes to determine where to
cut all potential records.  These records may be invalid: a
separator byte could be introduced or removed by corruption, and the
contents of a correctly framed record may be corrupt.  When
that happens, readers can simply scan for the next separator byte and
try to validate that new potential record.  The decoder’s state resets
after each separator byte, so any corruption is “forgotten” as soon as
the decoder finds valid a separator byte.</p>

<p>On the write side, the encoding logic is simple (a couple dozen lines
of C code), and uses a predictable amount of space, as expected from
an algorithm suitable for microcontrollers.</p>

<p>Actually writing encoded data is also easy:
on POSIX filesystems, we can make sure each record is delimited (e.g.,
prefixed with the delimiter byte), and issue a
<a href="https://pubs.opengroup.org/onlinepubs/007904875/functions/write.html">regular <code>O_APPEND</code> write(2)</a>.
Vectored writes can even insert delimiters without copying in
userspace.  Realistically, our code is probably less stable than
operating system and the hardware it runs on, so we make sure our
writes make it to the kernel as soon as possible, and let <code>fsync</code>s
happen on a timer.</p>

<p>When a write errors out, we can blindly (maybe once or twice) try
again: the encoding is independent of the output file’s state.  When a
write is cut short, we can still issue the same<sup id="fnref:suffix" role="doc-noteref"><a href="#fn:suffix" class="footnote" rel="footnote">1</a></sup> write call,
without trying to “fix” the short write: the encoding and the
read-side logic already protect against that kind of corruption.</p>

<p>What if multiple threads or processes write to the same log file?
When we <a href="https://pubs.opengroup.org/onlinepubs/007904875/functions/open.html">open with <code>O_APPEND</code></a>,
the operating system can handle the rest.  This doesn’t make
contention disappear, but at least we’re not adding a bottleneck in
userspace on top of what is necessary to append to the same file.
Buffering is also trivial: the encoding is independent of the state of
the destination file, so we can always concatenate buffered records
and write the result with a single syscall.</p>

<p>This simplicity also plays well  with
<a href="https://kernel.dk/io_uring.pdf">high-throughput I/O primitives like <code>io_uring</code></a>, and with
<a href="https://docs.microsoft.com/en-us/rest/api/storageservices/append-block">blob stores that support appends</a>:
independent workers can concurrently queue up blind append requests and
retry on failure.
There’s no need for application-level mutual exclusion or rollback.</p>

<h2 id="fun-tricks-with-robust-readers">Fun tricks with robust readers</h2>

<p>Our log encoding will recover from bad bytes, as long as readers can
detect and reject invalid records as a whole; the processing logic
should also handle duplicated valid records.  These are table stakes
for a reliable log consumer.</p>

<p>In our variable-length metadata use case, each record describes a
symbolicated call stack, and we recreate in-memory data structures by
replaying an append-only log of metadata records, one for each
unique call stack.  The hydration phase handles invalid records by
ignoring (not recreating) any call stack with corrupt metadata,
but only those call stacks.  That’s definitely an improvement over the
previous situation, where corruption in a size header would prevent us
from decoding the remainder of the file, and thus make us forget about
<em>all</em> call stacks stored at file offsets after the corruption.</p>

<p>Of course, losing data should be avoided, so we are careful to
<code>fsync</code> regularly and recommend reasonable storage configurations.
However, one can only make data loss unlikely, not impossible (if only
due to fat fingering), especially when cost is a factor. With the COBS
encoding, we can recover gracefully and automatically from any
unfortunate data corruption event.</p>

<p>We can also turn this robustness into new capabilities.</p>

<p>It’s often useful to process the tail of a log at a regular cadence.
For example, I once maintained a system that regularly tailed hourly
logs to update approximate views. One could support that use case with
length footers. COBS framing lets us instead scan
for a valid record from an arbitrary byte location, and read the rest
of the data normally.</p>

<p>When logs grow large enough, we want to process them in parallel.  The
standard solution is to shard log streams, which unfortunately couples
the parallelisation and storage strategies, and adds complexity to the
write side.</p>

<p>COBS framing lets us parallelise readers independently of the writer.
The downside is that the read-side code and I/O patterns are now more
complex, but, all other things being equal, that’s a trade-off I’ll
gladly accept, especially given that our servers run on independent
machines and store their data in files, where reads are fine-grained
and latency relatively low.</p>

<p>A parallel COBS reader partitions a data file arbitrarily (e.g., in
fixed size chunks) for independent workers.  A worker will scan for
the first valid record starting inside its assigned chunk, and handle
every record that <em>starts</em> in its chunk. Filtering on the start byte
means that a worker may read past the logical end of its chunk, when
it fully decodes the last record that starts in the chunk: that’s how
we unambiguously assign a worker to every record, including records
that straddle chunk boundaries.</p>

<p>Random access even lets us implement a form of binary or interpolation
search on raw unindexed logs, when we know the records are (k-)sorted
on the search key!  This lets us, e.g., access the metadata for a few
call stacks without parsing the whole log.</p>

<p>Eventually, we might also want to truncate our logs.</p>

<p>Contemporary filesystems like XFS (and even Ext4) support large sparse
files.  For example, sparse files can reach \(2^{63} - 1\) bytes on
XFS with a minimal metadata-only footprint: the on-disk data for such
sparse files is only allocated when we issue actual writes.  Nowadays,
we can <a href="https://lwn.net/Articles/415889/">sparsify files after the fact</a>,
and convert ranges of non-zero data into zero-filled “holes” in order
to release storage without messing with file offsets
(or even atomically
<a href="https://lwn.net/Articles/589260/">collapse old data away</a>).</p>

<p>Filesystems can only execute these operations at coarse granularity,
but that’s not an issue for our readers: they must merely remember to
skip sparse holes, and the decoding loop will naturally handle any
garbage partial record left behind.</p>

<h2 id="the-original-consistent-overhead-byte-stuffing-scheme">The original consistent overhead byte stuffing scheme</h2>

<p><a href="http://www.stuartcheshire.org/papers/COBSforToN.pdf">Cheshire and Baker’s original byte stuffing scheme</a>
targets small machines and slow transports (amateur radio and
phone lines).  That’s why it bounds the amount of buffering needed to
254 bytes for writers and 9 bits of state for readers, and attempts to
minimise space overhead, beyond its worst-case bound of 0.4%.</p>

<p>The algorithm is also reasonable. The encoder buffers data until it
encounters a reserved 0 byte (a delimiter byte), or there are 254
bytes of buffered data.  Whenever the encoder stops buffering, it
outputs a block whose contents are described by its first byte.  If
the writer stopped buffering because it found a reserved byte, it
emits one byte with <code>buffer_size + 1</code> before writing and clearing the
buffer.  Otherwise, it outputs 255 (one more than the buffer size),
followed by the buffer’s contents.</p>

<p>On the decoder side, we know that the first byte of each block
describes its size and decoded value (255 means 254 bytes of literal
data, any other value is one more than the number of literal bytes to
copy, followed by a reserved 0 byte).  We denote the end of a record
with an implicit delimiter: when we run out of data to decode, we
should have just decoded an extra delimiter byte that’s not really
part of the data.</p>

<p>With framing, an encoded record surrounded by delimiters thus looks
like the following</p>

<pre><code>|0   |blen|(blen - 1) literal data bytes....|blen|literal data bytes ...|0    |
</code></pre>

<p>The delimiting “0” bytes are optional at the beginning and end of a
file, and each <code>blen</code> size prefix is one byte with value in
\([1, 255]\).  A value \(\mathtt{blen} \in [1, 254]\) represents
a block \(\mathtt{blen} - 1\) literal bytes, followed by an implicit
 0 byte.  If we instead have \(\mathtt{blen} = 255\), we
have a block of \(254\) bytes, without any implicit byte.  Readers
only need to remember how many bytes remain until the end of the
current block (eight bits for a counter), and whether they should insert
an implicit 0 byte before decoding the next block (one binary flag).</p>

<h2 id="backtraces-word-stuffing-variant"><a href="https://www.backtrace.io">Backtrace</a>’s word stuffing variant</h2>

<p>We have different goals for the software we write at
<a href="https://www.backtrace.io">Backtrace</a>.  For our logging use case, we
pass around fully constructed records, and we want to issue a single
write syscall per record, with periodic <code>fsync</code>.<sup id="fnref:why-not-fewer" role="doc-noteref"><a href="#fn:why-not-fewer" class="footnote" rel="footnote">2</a></sup>
Buffering is baked in, so there’s no point in making sure we can work
with a small write buffer.  We also don’t care as much about the space
overhead (the worst-case bound is already pretty good) as much as we
do about encoding and decoding speed.</p>

<p>These different design goals lead us to an updated hybrid word/byte
stuffing scheme:</p>

<ol>
  <li>it uses a two-byte “reserved sequence,” carefully chosen to appear
infrequently in our data</li>
  <li>the size limit for the first block is slightly smaller (252 bytes instead
of 254)</li>
  <li>… but the limit for every subsequent block is much larger,
65008 bytes, for an asymptotic space overhead of 0.0031%.</li>
</ol>

<p>This hybrid scheme improves encoding and decoding speed compared to
COBS, and even marginally improves the asymptotic space overhead.  At
the low end, the worst-case overhead is only slightly worse than that
of traditional COBS: we need three additional bytes, including the
framing separator, for records of 252 bytes or fewer, and five bytes
for records of 253-64260 bytes.</p>

<p>In the past, I’ve seen
<a href="https://issues.apache.org/jira/browse/AVRO-27">“word” stuffing schemes</a>
aim to reduce the run-time overhead of COBS codecs by scaling up
the COBS loops to work on two or four bytes at a time.  However,
a byte search is trivial to vectorise, and there is no guarantee that
frameshift corruption will be aligned to word boundaries (for example,
POSIX allows short writes of an arbitrary number of bytes).</p>

<h3 id="much-ado-about-two-bytes">Much ado about two bytes</h3>

<p>Our hybrid word-stuffing looks for a reserved two-byte delimiter
sequence at arbitrary byte offsets.  We must still conceptually
process bytes one at a time, but delimiting with a pair of bytes
instead of with a single byte makes it easier to craft a delimiter
that’s unlikely to appear in our data.</p>

<p>Cheshire and Baker do the opposite, and use a frequent byte (0) to
eliminate the space overhead in the common case.  We care a lot more
about encoding and decoding speed, so an unlikely delimiter makes more
sense for us.  We picked <code>0xfe 0xfd</code> because that sequence doesn’t
appear in small integers (unsigned, two’s complement, varint, single
or double float) regardless of endianness, nor in valid UTF-8 strings.</p>

<p>Any positive integer with <code>0xfe 0xfd</code> (<code>254 253</code>) in its byte must be
around \(2^{16}\) or more.  If the integer is instead negative in
little-endian two’s complement, <code>0xfe 0xfd</code> equals -514 as a
little-endian <code>int16_t</code>, and -259 in big endian (not as great, but not
nothing).  Of course, the sequence could appear in two adjacent
<code>uint8_t</code>s, but otherwise, for <code>0xfe</code> or <code>0xfd</code> can only appear in
most significant byte of large 32- or 64-bit integers (unlike <code>0xff</code>,
which could be sign extension for, e.g., -1).</p>

<p>Any <a href="https://en.wikipedia.org/wiki/LEB128">(U)LEB varint</a> that
includes <code>0xfe 0xfd</code> must span at least 3 bytes (i.e., 15 bits),
since both these bytes have the most significant bit set to 1.
Even a negative SLEB has to be at least as negative as
\(- 2^{14} = -16384\).</p>

<p>For floating point types, we can observe that <code>0xfe 0xfd</code> in the
significand would represent an awful fraction in little or big
endian, so can only happen for the IEEE-754 representation of large
integers (approximately \(\pm 2^{15}\)).  If we instead assume
that <code>0xfd</code> or <code>0xfe</code> appear in the sign and exponent fields, we find
either very positive or very negative exponents (the exponent is
biased, instead of complemented).  A semi-exhaustive search confirms
that the smallest integer-valued single float that includes the
sequence is 32511.0 in little endian and 130554.0 in big endian;
among integer-valued double floats, we find 122852.0 and 126928.0
respectively.</p>

<p>Finally, the sequence isn’t valid UTF-8 because both <code>0xfe</code> and <code>0xfd</code>
have their top bit set (indicating a multi-byte code point), but neither
looks like a continuation byte: the two most significant bits are
<code>0b11</code> in both cases, while UTF-8 continuations must have <code>0b10</code>.</p>

<h3 id="encoding-data-to-avoid-the-reserved-sequence">Encoding data to avoid the reserved sequence</h3>

<p>Consistent overhead byte stuffing rewrites reserved 0 bytes away by
counting the number of bytes from the beginning of a record until the
next 0, and storing that count in a block size header followed by the
non-reserved bytes, then resetting the counter, and doing the same
thing for the remaining of the record. A complete record is stored as
a sequence of encoded blocks, none of which include the reserved
byte 0.  Each block header spans exactly one byte, and must never
itself be 0, so the byte count is capped at 254, and incremented by
one (e.g., a header value of 1 represents a count of 0); when the
count in the header is equal to the maximum, the decoder knows that
the encoder stopped short without finding a 0.</p>

<p>With our two-byte reserved sequence, we can encode the size of each
block in radix 253 (<code>0xfd</code>); given a two-byte header for each block, sizes
can go up to \(253^2 - 1 = 64008\).  That’s a reasonable granularity
for <code>memcpy</code>.  This radix conversion replaces the off-by-one weirdness
in COBS: that part of the original algorithm merely encodes values
from \([0, 254]\) into one byte while avoiding the reserved byte 0.</p>

<p>A two-byte size prefix is a bit ridiculous for small records (ours
tend to be on the order of 30-50 bytes). We thus encode the first
block specially, with a single byte in \([0, 252]\) for the size
prefix.  Since the reserved sequence <code>0xfe 0xfd</code> is unlikely to appear in
our data, the encoding for short record often boils down to adding a
<code>uint8_t</code> length prefix.</p>

<p>A framed encoded record now looks like</p>
<pre><code>|0xfe|0xfd|blen|blen literal bytes...|blen_1|blen_2|literal bytes...|0xfe|0xfd|
</code></pre>

<p>The first <code>blen</code> is in \([0, 252]\) and tells us how many literal
bytes follow in the initial block.  If the initial \(\mathtt{blen} =
252\), the literal bytes are immediately followed by the next block’s
decoded contents.  Otherwise, we must first append an implicit <code>0xfe
0xfd</code> sequence… which may be the artificial reserved sequence that
mark the end of every record.</p>

<p>Every subsequent block comes with a two-byte size prefix, in little-endian
radix-253.  In other words, <code>|blen_1|blen_2|</code> represents the
block size \(\mathtt{blen}\sb{1} + 253 \cdot \mathtt{blen}\sb{2}\), where
\(\mathtt{blen}_{{1, 2}} \in [0, 252]\).  Again, if the block
size is the maximum encodable size, \(253^2 - 1 = 64008\), we
have literal data followed by the next block; otherwise, we must
append a <code>0xfe 0xfd</code> sequence to the output before
moving on to the next block.</p>

<p>The encoding algorithm is only a bit more complex than for the
original COBS scheme.</p>

<p>Assume the data to encode is suffixed with an artificial two-byte
reserved sequence <code>0xfe 0xfd</code>.</p>

<p>For the first block, look for the reserved sequence in the first 252
bytes.  If we find it, emit its position (must be less than 251) in
one byte, then all the data bytes up to but not including the reserved
sequence, and enter regular encoding after the reserved sequence.  If
the sequence isn’t in the first block, emit <code>252</code>, followed
by 252 bytes of data, and enter regular encoding after those bytes.</p>

<p>For regular (all but the first) blocks, look for the reserved sequence in
the next 64008 bytes.  If we find it, emit the sequence’s byte offset
(must be less than 64008) in little-endian radix 253, followed by the
data up to but not including the reserved sequence, and skip that sequence
before encoding the rest of the data.  If we don’t find the reserved
sequence, emit 64008 in radix 253 (<code>0xfc 0xfc</code>), copy the next 64008
bytes of data, and encode the rest of the data without skipping anything.</p>

<p>Remember that we conceptually padded the data with a reserved sequence at
the end.  This means we’ll always observe that we fully consumed the
input data at a block boundary.  When we encode the block that stops
at the artificial reserved sequence, we stop (and frame with a reserved
sequence to delimit a record boundary).</p>

<p>You can find our <a href="https://github.com/backtrace-labs/stuffed-record-stream/blob/main/src/word_stuff.c">implementation in the stuffed-record-stream repository</a>.</p>

<p>When writing short records, we already noted that the encoding step is
often equivalent to adding a one-byte size prefix.  In fact, we can
encode and decode all records of size up to \(252 + 64008 = 64260\)
bytes in place, and only ever have to slide the initial 252-byte
block: whenever a block is shorter than the maximum length (252 bytes
for the first block, 64008 for subsequent ones), that’s because we
found a reserved sequence in the decoded data.  When that happens, we
can replace the reserved sequence with a size header when encoding,
and undo the substitution when decoding.</p>

<p>Our code does not implement these optimisations because encoding and
decoding stuffed bytes aren’t bottlenecks for our use case, but it’s
good to know that we’re nowhere near the performance ceiling.</p>

<h2 id="a-resilient-record-stream-on-top-of-word-stuffing">A resilient record stream on top of word stuffing</h2>

<p>The stuffing scheme only provides resilient framing.  That’s
essential, but not enough for an abstract stream or sequence of
records.  At the very least, we need checksums in order to detect
invalid records that happen to be correctly encoded (e.g., when a
block’s literal data is overwritten).</p>

<p>Our pre-stuffed records start with the little-endian header</p>

<pre><code>struct record_header {
        uint32_t crc;
        uint32_t generation;
};
</code></pre>

<p>where <code>crc</code> is the <code>crc32c</code> of whole record, including the
header,<sup id="fnref:initialize-your-crc" role="doc-noteref"><a href="#fn:initialize-your-crc" class="footnote" rel="footnote">3</a></sup> and <code>generation</code> is a yet-unused
arbitrary 32-bit payload that we added for forward compatibility.
There is no size field: the framing already handles that.</p>

<p>The remaining bytes in a record are an arbitrary payload.  We use
<a href="https://developers.google.com/protocol-buffers/docs/reference/proto2-spec">protobuf messages</a>
to help with schema evolution (and keep messages small and flat for
decoding performance), but there’s no special relationship between the
stream of word-stuffed records and the payload’s format.</p>

<p><a href="https://github.com/backtrace-labs/stuffed-record-stream/blob/main/include/record_stream.h">Our implementation</a>
let writers output to buffered <code>FILE</code> streams, or directly to file descriptors.</p>

<p>Buffered streams offer higher write throughput, but are only safe
when the caller handles synchronisation and flushing; we use them
as part of a commit protocol that
<a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/fsync.html">fsync</a>s
and publishes files with
<a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/rename.html">atomic <code>rename</code> syscalls</a>.</p>

<p>During normal operations, we instead write to file descriptors opened
with <code>O_APPEND</code> and a background fsync worker: in practice, the
hardware and operating system are more stable than our software, so
it’s more important that encoded records immediately make it to the
kernel than all the way to persistent storage.  We also avoid batching
write syscalls because we would often have to wait several minutes if
not hours to buffer more than two or three records.</p>

<p>For readers, we can either read from a buffer, or <code>mmap</code> in a file,
and read from the resulting buffer.  While we expose a linear iterator
interface, we can also override the start and stop byte offset of an
iterator; we use that capability to replay logs in parallel.  Finally,
when readers advance an iterator, they can choose to receive a raw data
buffer, or have it decoded with a protobuf message descriptor.</p>

<h2 id="whats-next">What’s next?</h2>

<p>We have happily been using this log format for more than nine months
to store a log of metadata records that we replay every time the
<a href="https://www.backtrace.io">Backtrace</a> server restarts.</p>

<p>Decoupling writes from the parallel read strategy let us improve our
startup time incrementally, without any hard migration.  Serialising
with flexible schemas
(<a href="https://developers.google.com/protocol-buffers/docs/reference/proto2-spec">protocol buffers</a>)
also made it easier to start small and slowly add optional metadata,
and only enforce a hard switch-over when we chose to delete backward
compatibility code.</p>

<p>This piecemeal approach let us transition from a length-prefixed data
format to one where all important metadata lives in a resilient record
stream, without any breaking change.  We slowly added more metadata to
records and eventually parallelised loading from the metadata record
stream, all while preserving backward and forward compatibility.  Six
months after the initial roll out, we flipped the switch and made the
new, more robust, format mandatory; the old length-prefixed files
still exist, but are now bags of arbitrary checksummed data bytes,
with metadata in record streams.</p>

<p>In the past nine months, we’ve gained a respectable amount of pleasant
operational experience with the format. Moreover, while performance is
good enough for us (the parallel loading phase is currently
dominated by disk I/O and parsing in <code>protobuf-c</code>), we also know
there’s plenty of headroom: our records are short enough that they can
usually be decoded without any write, and always in place.</p>

<p>We’re now starting laying the groundwork to distribute our single-node
embedded database and making it interact more fluently with other data
stores.  The first step will be generating a
<a href="https://materialize.com/change-data-capture-part-1/">change data capture stream</a>,
and re-using the word-stuffed record format was an obvious choice.</p>

<p>Word stuffing is simple, efficient, and robust.  If you can’t just
defer to a real database (maybe you’re trying to write one yourself)
for your log records, give it a shot!  Feel free to
<a href="https://github.com/backtrace-labs/stuffed-record-stream">play with our code</a>
if you don’t want to roll your own.</p>

<p><small> Thank you, Ruchir and Alex, for helping me clarify and restructure an earlier version. </small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:suffix" role="doc-endnote">
      <p>If you append with the delimiter, it probably makes sense to special-case short writes and also prepend with the delimiter after failures, in order to make sure readers will observe a delimiter before the new record. <a href="#fnref:suffix" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:why-not-fewer" role="doc-endnote">
      <p>High-throughput writers should batch records.  We do syscall-per-record because the write load for the current use case is so sporadic that any batching logic would usually end up writing individual records.  For now, batching would introduce complexity and bug potential for a minimal impact on write throughput. <a href="#fnref:why-not-fewer" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:initialize-your-crc" role="doc-endnote">
      <p>We overwrite the <code>crc</code> field with <code>UINT32_MAX</code> before computing a checksum for the header and its trailing data.  It’s important to avoid zero prefixes because the result of crc-ing a 0 byte into a 0 state is… 0. <a href="#fnref:initialize-your-crc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[1.5x the PH bits for one more CLMUL]]></title>
    <link href="https://www.pvk.ca/Blog/2020/10/31/nearly-double-the-ph-bits-with-one-more-clmul/"/>
    <updated>2020-10-31T18:30:19-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/10/31/nearly-double-the-ph-bits-with-one-more-clmul</id>
    <content type="html"><![CDATA[<p><small>Turns out the part where I simply asserted a property was slightly off 😉. I had to go back to an older proof with weaker bounds, but that’s not catastrophic: we still collide way more rarely than \(2^{-70}\).</small></p>

<p>The core of <a href="https://github.com/backtrace-labs/umash">UMASH</a> is a
hybrid <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.9929&amp;rep=rep1&amp;type=pdf#page=3">PH</a>/<a href="https://eprint.iacr.org/2004/319.pdf#page=4">(E)NH</a>
block compression function.
That function is fast
(it needs one multiplication for each 16-byte “chunk” in a block),
but relatively weak:
despite a 128-bit output, the worst-case probability of collision is
\(2^{-64}\).</p>

<p>For a <a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprinting</a>
application, we want collision probability less than \(\approx 2^{-70},\)
so that’s already too weak,
before we even consider merging a variable-length string of compressed
block values.</p>

<p>The <a href="https://pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting/">initial UMASH proposal</a>
compresses each block with two independent compression functions.
Krovetz showed that we could do so while reusing most of the key material
(random parameters), with a <a href="http://krovetz.net/csus/papers/thesis.pdf#page=50">Toeplitz extension</a>,
and I simply recycled the proof for UMASH’s hybrid compressor.</p>

<p>That’s good for the memory footprint of the random parameters, but
doesn’t help performance: we still have to do double the work to get
double the hash bits.</p>

<p>Earlier this month, <a href="https://github.com/jbapple">Jim Apple</a> pointed me at
a <a href="https://link.springer.com/chapter/10.1007/978-3-662-46706-0_25">promising alternative that doubles the hash bit with only one more multiplication</a>.
The construction adds finite field operations that aren’t particularly
efficient in software, on top of the additional 64x64 -&gt; 128
(carryless) multiplication, so isn’t a slam dunk win over a
straightforward Toeplitz extension.
However, Jim felt like we could “spend” some of the bits we don’t need
for fingerprinting (\(2^{-128}\) collision probability is overkill
when we only need \(2^{-70}\)) in order to make do with faster operations.</p>

<p>Turns out he was right! We can use carryless multiplications by sparse
constants (concretely, xor-shift and one more shift) without any
reducing polynomial, <em>on independent 64-bit halves</em>… and still
collide with probability at most <s>2^{-126}</s> \(2^{-98}\).</p>

<p>The proof is fairly simple, but relies on a bit of notation for clarity.
Let’s start by re-stating UMASH’s hybrid PH/ENH block compressor in
that notation.</p>

<h2 id="how-does-umash-currently-work">How does UMASH currently work?</h2>

<p>The current block compressor in UMASH splits a 256-byte block \(m\)
in 16 chunks \(m_i,\, i\in [0, 15]\) of 128 bits each, and processes
all but the last chunk with a PH loop,</p>

<p>\[ \bigoplus_{i=0}^{14} \mathtt{PH}(k_i, m_i), \]</p>

<p>where</p>

<p>\[ \mathtt{PH}(k_i, m_i) = ((k_i \bmod 2^{64}) \oplus (m_i \bmod 2^{64})) \odot (\lfloor k_i / 2^{64} \rfloor \oplus \lfloor m_i / 2^{64} \rfloor) \]</p>

<p>and each \(k_i\) is a randomly generated 128-bit parameter.</p>

<p>The compression loop in UMASH handles the last chunk, along with a
size tag (to protect against extension attacks), with
<a href="https://eprint.iacr.org/2004/319.pdf#page=4">ENH</a>:</p>

<p>\[ \mathtt{ENH}(k, x, y) = ((k + x) \bmod 2^{64}) \cdot (\lfloor k / 2^{64}\rfloor + \lfloor x / 2^{64} \rfloor \bmod 2^{64}) + y \mod 2^{128}. \]</p>

<p>The core operation in ENH is a full (64x64 -&gt; 128) integer
multiplication, which has lower latency than PH’s carryless
multiplication on x86-64.  That’s why UMASH switches to ENH for the
last chunk.  We use ENH for only one chunk because combining multiple
NH values calls for 128-bit additions, and that’s slower than PH’s
xors.  Once we have mixed the last chunk and the size tag with ENH,
the result is simply xored in with the previous chunks’ PH values:</p>

<p>\[ \left(\bigoplus_{i=0}^{14} \mathtt{PH}(k_i, m_i)\right)  \oplus \mathtt{ENH}(m_{15}, k_{15}, \mathit{tag}). \]</p>

<p>This function is annoying to analyse directly, because we end up
having to manipulate different proofs of almost-universality.  Let’s
abstract things a bit, and reduce the ENH/PH to the bare minimum we need
to find our collision bounds.</p>

<p>Let’s split our message blocks in \(n\) (\(n = 16\) for
UMASH) “chunks”, and apply an independently sampled mixing function
to each chunk.  Let’s say we have two messages \(m\) and
\(m^\prime\) with chunks \(m_i\) and \(m^\prime_i\), for \(i\in
[0, n)\), and let \(h_i\) be the result of mixing chunk \(m_i,\)
and \(h^\prime_i\) that of mixing \(m^\prime_i.\)</p>

<p>We’ll assume that the first chunk is mixed with a
\(2^{-w}\)-almost-universal (\(2^{-64}\) for UMASH) hash function:
if \(m_0 \neq m^\prime_0,\) \(\mathrm{P}[h_0 = h^\prime_0] \leq 2^{-w},\)
(where the probability is taken over the set of randomly chosen
parameters for the mixer).
Otherwise, \(m_0 = m^\prime_0 \Rightarrow h_i = h^\prime_i\).</p>

<p>This first chunks stands for the ENH iteration in UMASH.</p>

<p>Every remaining chunk will instead be mixed with a
\(2^{-w}\)-XOR-almost-universal hash function:
if \(m_i \neq m^\prime_i\) (\(0 &lt; i &lt; n\)),
\(\mathrm{P}[h_i \oplus h^\prime_i = y] \leq 2^{-w}\)
for any \(y,\)
where the probability is taken over the randomly generated
parameter for the mixer.</p>

<p>This stronger condition represents the PH iterations in UMASH.</p>

<p>We hash a full block by xoring all the mixed chunks together:</p>

<p>\[ H = \bigoplus_{i = 0}^{n - 1} h_i, \]</p>

<p>and</p>

<p>\[ H^\prime = \bigoplus_{i = 0}^{n - 1} h^\prime_i. \]</p>

<p>We want to bound the probability that \(H = H^\prime \Leftrightarrow
H \oplus H^\prime = 0,\) assuming that the messages differ
(i.e., there is at least one index \(i\) such that
\(m_i \neq m^\prime_i\)).</p>

<p>If the two messages only differ in \(m_0 \neq n^\prime_0\) (and thus
\(m_i = m^\prime_i,\,\forall i \in [1, n)\)),</p>

<p>\[ \bigoplus_{i = 1}^{n - 1} h_i = \bigoplus_{i = 1}^{n - 1} h^\prime_i, \]</p>

<p>and thus \(H = H^\prime \Leftrightarrow h_0 = h^\prime_0\).</p>

<p>By hypothesis, the 0th chunks are mixed with a
\(2^{-w}\)-almost-universal hash, so this happens with probability
at most \(2^{-w}\).</p>

<p>Otherwise, assume that \(m_j \neq m^\prime_j\), for some \(j \in
[1, n)\).
We will rearrange the expression</p>

<p>\[ H \oplus H^\prime = h_j \oplus h^\prime_j \oplus \left(\bigoplus_{i\in [0, n) \setminus \{ j \}} h_i \oplus h^\prime_i\right). \]</p>

<p>Let’s conservatively replace that unwieldly sum with an adversarially
chosen value \(y\):</p>

<p>\[ H \oplus H^\prime = h_j \oplus h^\prime_j \oplus y, \]</p>

<p>and thus \(H = H^\prime\) iff \(h_j \oplus h^\prime_j = y.\)
By hypothesis, the \(j\)th chunk (every chunk but the 0th),
is mixed with a \(2^{-w}\)-almost-XOR-universal hash, and
this thus happens with probability at most \(2^{-w}\).</p>

<p>In both cases, we find a collision probability at most \(2^{-w}\)
with a simple analysis, despite combining mixing functions from
different families over different rings.</p>

<h2 id="wringing-more-bits-out-of-the-same-mixers">Wringing more bits out of the same mixers</h2>

<p>We combined strong mixers (each is \(2^{-w}\)-almost-universal),
and only got a \(2^{-w}\)-almost-universal output.
It seems like we should be able to do better when two or
more chunks differ.</p>

<p>As <a href="https://link.springer.com/chapter/10.1007/978-3-662-46706-0_25">Nandi</a>
points outs, we can apply erasure codes to derive additional
chunks from the original messages’ contents.  We only need
one more chunk, so we can simply xor together all the original
chunks:</p>

<p>\[m_n = \bigoplus_{i=0}^{n - 1} m_i,\]</p>

<p>and similarly for \(m^\prime_n\).
If \(m\) and \(m^\prime\) differ in only one chunk, \(m_n \neq
m^\prime_n\).  It’s definitely possible for \(m_n = m^\prime_n\)
when \(m \neq m^\prime\), but only if two or more chunks differ.</p>

<p>We will again mix \(m_n\) and \(m^\prime_n\) with a fresh
\(2^{-w}\)-almost-XOR-universal hash function to yield \(h_n\) and
\(h^\prime_n\).</p>

<p>We want to xor the result \(h_n\) and \(h^\prime_n\) with the second
(still undefined) hash values \(H_2\) and \(H^\prime_2\); if
\(m_n \neq m^\prime_n\), the final xored values are equal with
probability at most \(2^{-w}\), regardless of \(H_2\) and
\(H^\prime_2\ldots\) and, crucially, independently of \(H \neq
H^\prime\).</p>

<p>When the two messages \(m\) and \(m^\prime\) only differ in a
single (initial) chunk, mixing a <a href="https://en.wikipedia.org/wiki/Longitudinal_redundancy_check">LRC checksum</a>
gives us an independent hash function, which
squares the collision probability to \(2^{-2w}\).</p>

<p>Now to the interesting bit: we must define a second hash function
that combines \(h_0,h_1,\ldots, h_{n - 1}\) and \(h^\prime_0, h^\prime_1, \ldots, h^\prime_{n - 1}\)
such that the resulting hash values \(H_2\) and \(H^\prime_2\) collide
independently enough of \(H\) and \(H^\prime\).
That’s a tall order, but we do have one additional assumption to work
with: we only care about collisions in this second hash function if the
additional checksum chunks are equal, which means that the two messages
differ in two or more chunks (or they’re identical).</p>

<p>For each index \(0 &lt; i &lt; n\), we’ll fix a <em>public</em> linear (with xor
as the addition) function \(\overline{xs}_i(x)\).  This family of
function must have two properties:</p>

<ol>
  <li>\(f(x) = x \oplus \overline{xs}_i(x)\) is invertible for all \(0 &lt; i &lt; n\).</li>
  <li>For any two distinct \(0 &lt; i &lt; j &lt; n\), xoring the two functions
together into \(g(x) = \overline{xs}_i(x) \oplus
\overline{xs}_j(x)\) yields a function with a small (low rank)
null space.  In other words, while \(g\) may not be invertible,
it must be “pretty close.”</li>
</ol>

<p>For regularity, we will also define \(\overline{xs}_0(x) = x\).</p>

<p>Concretely, let \(\overline{xs}_1(x) = x \mathtt{«} 1\), where the
bitshift is computed for the two 64-bit halves independently, and
\(\overline{xs}_i(x) = (x  \mathtt{«} 1) \oplus (x \mathtt{«} i)\)
for \(i &gt; 1\), again with all the bitshifts computed independently
over the two 64-bit halves.</p>

<p>To see that these satisfy our requirements, we can represent the
functions as carryless multiplication by distinct “even” constants
(the least significant bit is 0) on each 64-bit half:</p>

<ol>
  <li>Once we xor in \(x\), we get a multiplication by an odd constant,
and that’s invertible.</li>
  <li>Combining \(\overline{xs}_i\) and \(\overline{xs}_j\) with xor
yields either \(x \mathtt{«} j\) or \((x \mathtt{«} i) \oplus (x \mathtt{«} j)\).  In the worst case, we lose \(j\) bits in each 64-bit half,
and there are thus \(2^{2j}\) values in \(g^{-1}(0)\).</li>
</ol>

<p>To recapitulate, we defined the first hash function as</p>

<p>\[ H = \bigoplus_{i = 0}^{n - 1} h_i, \]</p>

<p>the (xor) sum of the mixed value \(h_i\) for each chunk \(m_i\) in
the message block \(m\), and similarly for \(H^\prime\) and
\(h^\prime_i\).</p>

<p>We’ll let the second hash function be</p>

<p>\[ H_2 \oplus h_n = \left(\bigoplus_{i = 0}^{n - 1} \overline{xs}_i(h_i)\right) \oplus h_n, \]</p>

<p>and</p>

<p>\[ H^\prime_2 \oplus h^\prime_n = \left(\bigoplus_{i = 0}^{n - 1} \overline{xs}_i(h^\prime_i)\right) \oplus h^\prime_n. \]</p>

<p>We can finally get down to business and find some collision bounds.
We’ve already shown that both \(H = H^\prime\) <em>and</em> \(H_2 \oplus h_n =
H^\prime_2 \oplus h^\prime_n\) collide simultaneously with probability at most \(2^{-2w}\)
when the checksum chunks differ, i.e., when \(m_n \neq m^\prime_n\).</p>

<p>Let’s now focus on the case when \(m \neq m^\prime\), but \(m_n =
m^\prime_n\).
In that case, we know that at least two chunks \(0 \leq i &lt; j &lt; n\)
differ: \(m_i \neq m^\prime_i\) and \(m_j \neq m^\prime_j\).</p>

<p>If only two chunks \(i\) and \(j\) differ, and one of them is the
\(i = 0\)th chunk, we want to bound the probability that</p>

<p>\[ h_0 \oplus h_j = h^\prime_0 \oplus h^\prime_j \]</p>

<p>and</p>

<p>\[ h_0 \oplus \overline{xs}_j(h_j) = h^\prime_0 \oplus \overline{xs}_j(h^\prime_j), \]</p>

<p>both at the same time.</p>

<p>Letting \(\Delta_i = h_i \oplus h^\prime_i\), we can reformulate the
two conditions as</p>

<p>\[ \Delta_0 = \Delta_j \]
and
\[ \Delta_0 = \overline{xs}_j(\Delta_j). \]</p>

<p>Taking the xor of the two conditions yields</p>

<p>\[ \Delta_j \oplus \overline{xs}_j(\Delta_j) = 0, \]</p>

<p>which is only satisfied for \(\Delta_j = 0\), since \(f(x) = x
\oplus \overline{xs}_j(x)\) is an invertible linear function.
This also forces \(\Delta_0 = 0\).</p>

<p>By hypothesis, \(\mathrm{P}[\Delta_j = 0] \leq 2^{-w}\), and
\(\mathrm{P}[\Delta_0 = 0] \leq 2^{-w}\) as well.  These two
probabilities are independent, so we get a probability that both
hash collide less than or equal to \(2^{-2w}\) (\(2^{-128}\)).</p>

<p>In the other case, we have messages that differ in at least two chunks
\(0 &lt; i &lt; j &lt; n\): \(m_i \neq m^\prime_i\) and \(m_j \neq
m^\prime_j\).</p>

<p>We can simplify the collision conditions to</p>

<p>\[ h_i \oplus h_j = h^\prime_i \oplus h^\prime_j \oplus y \]</p>

<p>and</p>

<p>\[ \overline{xs}_i(h_i) \oplus \overline{xs}_j(h_j) = \overline{xs}_i(h^\prime_i) \oplus \overline{xs}_j(h^\prime_j) \oplus z, \]</p>

<p>for \(y\) and \(z\) generated arbitrarily (adversarially), but
without knowledge of the parameters that generated \(h_i, h_j, h^\prime_i,
h^\prime_j\).</p>

<p>Again, let \(\Delta_i = h_i \oplus h^\prime_i\) and \(\Delta_j = h_j \oplus h^\prime_j\), and reformulate the conditions into</p>

<p>\[ \Delta_i \oplus \Delta_j = y \]
and
\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_j(\Delta_j) = z. \]</p>

<p>Let’s apply the linear function \(\overline{xs}_i\) to the first
condition</p>

<p>\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_i(\Delta_j) = \overline{xs}_i(y); \]</p>

<p>since \(\overline{xs}_i\) isn’t invertible, the result
isn’t equivalent, but is a weaker (necessary, not sufficient)
version of the initial condiion.</p>

<p>After xoring that with the second condition</p>

<p>\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_j(\Delta_j) = z, \]</p>

<p>we find</p>

<p>\[ \overline{xs}_i(\Delta_j) \oplus \overline{xs}_j(\Delta_j) = \overline{xs}_i(y) \oplus z. \]</p>

<p>By hypothesis, the null space of
\(g(x) = \overline{xs}_i(x) \oplus \overline{xs}_j(x)\)
is “small.”
For our concrete definition of \(\overline{xs}\), there
are \(2^{2j}\) values in that null space, which means that \(\Delta_j\)
can only satisfy the combined xored condition by taking one of at most
\(2^{2j}\) values; otherwise, the two hashes definitely can’t both
collide.</p>

<p>Since \(j &lt; n\), this happens with probability at most \(2^{2(n -
1) - w} \leq 2^{-34}\) for UMASH with \(w = 64\) and \(n =
16\).</p>

<p>Finally, for any given \(\Delta_j\), there is at most one
\(\Delta_i\) that satisfies</p>

<p>\[ \Delta_i \oplus \Delta_j = y,\]</p>

<p>and so <em>both</em> hashes collide with probability at most \(2^{-98}\),
for \(w = 64\) and \(n = 16\).</p>

<p>Astute readers will notice that we could let
\(\overline{xs}_i(x) = x \mathtt{«} i\),
and find the same combined collision probability.
However, this results in a much weaker secondary hash, since a chunk
could lose up to \(2n - 2\) bits (\(n - 1\) in each 64-bit half)
of hash information to a plain shift.
The shifted xor-shifts might be a bit slower to compute, but
guarantees that we only lose at most 2 bits<sup id="fnref:one-bit-with-ph" role="doc-noteref"><a href="#fn:one-bit-with-ph" class="footnote" rel="footnote">1</a></sup> of
information per chunk.  This feels like an interface that’s harder to
misuse.</p>

<p>If one were to change the \(\overline{xs}_i\) family of functions, I
think it would make more sense to look at a more diverse form of
(still sparse) multipliers, which would likely let us preserve a
couple more bits of independence.  Jim has constructed such a family
of multipliers, in arithmetic modulo \(2^{64}\); I’m sure we could
find something similar in carryless multiplication.  The hard part is
implementing these multipliers: in order to exploit the multipliers’
sparsity, we’d probably have to fully unroll the block hashing loop,
and that’s not something I like to force on implementations.</p>

<h2 id="what-does-this-look-like-in-code">What does this look like in code?</h2>

<p>The base <a href="https://github.com/backtrace-labs/umash/blob/8fd6287617f41e236bfb679e8d29a8b32f82c0e9/umash.c#L336">UMASH block compressor</a>
mixes all but the last of the message block’s 16-byte chunks with
PH: xor the chunk with the corresponding bytes in the parameter
array, computes a carryless multiplication of the xored chunks’ half
with the other half.  The last chunk goes through a variant of
<a href="https://github.com/backtrace-labs/umash/blob/8fd6287617f41e236bfb679e8d29a8b32f82c0e9/umash.c#L358">ENH</a>
with an invertible finaliser (safe because we only rely on
\(\varepsilon\)-almost-universality),
and everything is xored in the accumulator.</p>

<p>The collision proofs above preserved the same structure for the first hash.</p>

<p>The second hash reuses so much work from the first that it mostly
makes sense to consider a combined loop that computes both (regular
UMASH and this new xor-shifted variant) block compression functions at
the same time.</p>

<p>The first change for this combined loop is that we need to xor
together all 16-bytes chunk in the message, and mix the resulting
checksum with a fresh PH function.  That’s equivalent to xoring
everything in a new accumulator (or two accumulators when working with
256-bit vectors) initialised with the PH parameters, and <code>CLMUL</code>ing
together the accumulator’s two 64-bit halves at the end.</p>

<p>We also have to apply the \(\overline{xs}_i\) quasi-xor-shift
functions to each \(h_i\).  The trick is to accumulate the shifted
values in two variables: one is the regular UMASH accumulator without
\(h_0\) (i.e., \(h_1 \oplus h_2 \ldots\)), and the other shifts
the current accumulator before xoring in a new value, i.e.,
\(\mathtt{acc}^\prime = (\mathtt{acc} \mathtt{«} 1) \oplus h_i\),
where the left shift on parallel 64-bit halves simply adds <code>acc</code>
to itself.</p>

<p>This additional shifted accumulator includes another special case to
skip \(\overline{xs}_1(x) = x \mathtt{«} 1\); that’s not a
big deal for the code, since we already have to special case the last
iteration for the ENH mixer.</p>

<p>Armed with \(\mathtt{UMASH} = \bigoplus_{i=1}^{n - 1} h_i\) and
\(\mathtt{acc} = \bigoplus_{i=2}^{n - 1} h_i \mathtt{«} (i - 1),\)
we have
\[\bigoplus_{i=1}^{n - 1} \overline{xs}_i(h_i) = (\mathtt{UMASH} \oplus \mathtt{acc}) \mathtt{«} 1.\]</p>

<p>We just have to xor in the <code>PH</code>-mixed checksum \(h_n\), and finally
\(h_0\) (which naturally goes in GPRs, so can be computed while we
extract values out of vector registers).</p>

<p>We added two vector xors and one addition for each chunk in a block,
and, at the end, one <code>CLMUL</code> plus a couple more xors and adds again.</p>

<p>This should most definitely be faster than computing two UMASH at the
same time, which incurred two vector xors and a <code>CLMUL</code> (or full
integer multiplication) for each chunk: even when <code>CLMUL</code> can pipeline
one instruction per cycle, vector additions can dispatch to more
execution units, so the combined throughput is still higher.</p>

<h2 id="one-last-thing-what-if-we-have-blocks-of-different-length">One last thing: what if we have blocks of different length?</h2>

<p>It’s easy to show that UMASH is relatively safe when one block is
shorter than the other, and we simply xor together fewer mixed chunks.
Without loss of generality, we can assume the longer block has \(n\)
chunks; that block’s final ENH is independent of the shorter block’s
UMASH, and any specific value occurs with probability at most
\(2^{-63}\) (the probability of a multiplication by zero).</p>

<p>A similar argument seems more complex to defend for the shifted UMASH.</p>

<p>Luckily, we can tweak the <a href="https://en.wikipedia.org/wiki/Longitudinal_redundancy_check">LRC checksum</a>
we use to generate an additional chunk in the block: rather than xoring
together the raw message chunks, we’ll xor them <em>after</em> xoring them
with the PH key, i.e.,</p>

<p>\[m_n = \bigoplus_{i=0}^{n - 1} m_i \oplus k_i, \]</p>

<p>where \(k_i\) are the PH parameters for each chunk.</p>

<p>When checksumming blocks of the same size, this is a no-op with respect
to collision probabilities.  Implementations might however benefit
from the ability to use a fused <code>xor</code> with load from memory<sup id="fnref:latency" role="doc-noteref"><a href="#fn:latency" class="footnote" rel="footnote">2</a></sup>
to compute \(m_i \oplus k_i\), and feed that both into the checksum
and into <code>CLMUL</code> for PH.</p>

<p>Unless we’re extremely unlucky (\(m_{n - 1} = k_{n - 1}\), with
probability \(2^{-2w}\)), the long block’s LRC will differ from the
shorter block’s.  As long as we always xor in the same PH parameters
when mixing the artificial LRC, the secondary hashes collide with
probability at most \(2^{-64}\).</p>

<p>With a small tweak to the checksum function, we can easily guarantee
that blocks with a different number of chunks collide with probability
less than \(2^{-126}\).<sup id="fnref:tag" role="doc-noteref"><a href="#fn:tag" class="footnote" rel="footnote">3</a></sup></p>

<p><small>Thank you Joonas for helping me rubber duck the presentation, and Jim for pointing me in the right direction, and for the fruitful discussion!</small></p>

<p><hr style="width: 50%" /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:one-bit-with-ph" role="doc-endnote">
      <p>It’s even better for UMASH, since we obtained these shifted chunks by mixing with PH.  The result of PH is the carryless product of two 64-bit values, so the most significant bit is always 0.  The shifted-xorshift doesn’t erase any information in the high 64-bit half! <a href="#fnref:one-bit-with-ph" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:latency" role="doc-endnote">
      <p>This might also come with a small latency hit, which is unfortunate since PH-ing \(m_n\) is likely to be on the critical path… but one cycle doesn’t seem that bad. <a href="#fnref:latency" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tag" role="doc-endnote">
      <p>The algorithm to expand any input message to a sequence of full 16-byte chunks is fixed.  That’s why we incorporate a size tag in ENH; that makes it impossible for two messages of different lengths to collide when they are otherwise identical after expansion. <a href="#fnref:tag" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
