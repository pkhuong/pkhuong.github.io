<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2020-02-26T19:28:56-05:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Too much locality... for stores to forward]]></title>
    <link href="https://www.pvk.ca/Blog/2020/02/01/too-much-locality-for-store-forwarding/"/>
    <updated>2020-02-01T17:29:21-05:00</updated>
    <id>https://www.pvk.ca/Blog/2020/02/01/too-much-locality-for-store-forwarding</id>
    <content type="html"><![CDATA[<p><small>Apologies for the failed <a href="https://www.youtube.com/watch?v=K3DRkVjuqmc">Cake reference</a>.<br />
2020-02-02: Refer to Travis Downs’s investigation into this pathological case for forwarding.</small></p>

<p>I’ve been responsible for <a href="https://backtrace.io/">Backtrace.io</a>’s crash analytic database<sup id="fnref:started-work"><a href="#fn:started-work" class="footnote">1</a></sup> for a couple months now.
I have focused my recent efforts on improving query times for in-memory grouped aggregations, i.e.,
the archetypal MapReduce use-case where we generate key-value pairs, and <a href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)">fold</a> over the values
for each key in some <a href="https://en.wikipedia.org/wiki/Semigroup">(semi)</a><a href="https://en.wikipedia.org/wiki/Group_(mathematics)">group</a>.
We have a cute cache-efficient data structure for this type of workload;
the inner loop simply inserts in a small hash table with <a href="/Blog/more_numerical_experiments_in_hashing.html">Robin Hood linear probing</a>,
in order to guarantee entries in the table are ordered by hash value.  This
ordering lets us easily dump the entries in sorted order, and <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/1993/6309.html">block</a> the merge loop for an arbitrary number of sorted arrays
into a unified, larger, ordered hash table (which we can, again, dump to a sorted array).<sup id="fnref:more-later"><a href="#fn:more-later" class="footnote">2</a></sup></p>

<h1 id="observation">Observation</h1>

<p>As I updated more operators to use this data structure, I noticed that we were spending a lot of time in its inner loop.
In fact, <a href="http://www.brendangregg.com/linuxperf.html">perf</a> showed that the query server as a whole was spending 4% of its CPU time on one instruction in that loop:</p>

<pre><code> 2.17 |       movdqu     (%rbx),%xmm0
39.63 |       lea        0x1(%r8),%r14  # that's 40% of the annotated function
      |       mov        0x20(%rbx),%rax
 0.15 |       movaps     %xmm0,0xa0(%rsp)
</code></pre>

<p>The first thing to note is that instruction-level profiling tends to put the blame on the instruction <em>following</em> the one that triggered a sampling interrupt.
It’s not the <code>lea</code> (which computes <code>r14 &lt;- r8 + 1</code>) that’s slow, but the <code>movdqu</code> just before.
So, what is that <code>movdqu</code> loading into <code>xmm0</code>?  Maybe it’s just a normal cache miss, something inherent to the workload.</p>

<p>I turned on source locations <a href="http://man7.org/linux/man-pages/man1/perf-report.1.html">(hit <code>s</code> in <code>perf report</code>)</a>, and observed that this instruction was simply copying to the stack an argument that was passed by address.
The source clearly showed that the argument should be hot in cache: the inner loop was essentially</p>

<pre><code>A1. Generate a new key-value pair
B1. Mangle that kv pair just a bit to turn it into a hash element
C1. Insert the new hash element
A2.
B2.
C2.
</code></pre>

<p>and the <code>movdqu</code> happens in step C, to copy the element that step B just constructed.<sup id="fnref:dont-copy"><a href="#fn:dont-copy" class="footnote">3</a></sup></p>

<p>At this point, an important question suggests itself: does it matter?
We could simply increase the size of the base case and speed up the rest of the bottom-up recursion… eventually, the latency for the random accesses in the initial hash table will dominate the inner loop.</p>

<p>When I look into the performance of these deep inner loop, my goal isn’t only to do the same thing better.
The big wins, in my experience, come from the additional design freedom that we get from being able to find new uses for the same code.
Improved latency, throughput, or memory footprint really shine when the increased optionality from multiple such improvements compounds and lets us consider a much larger design space for the project as a whole.
That’s why I wanted to make sure this hash table insertion loop worked on as wide a set of parameter as possible: because that will give future me the ability to combine versatile tools.</p>

<h1 id="hypothesis">Hypothesis</h1>

<p>Back to the original question. Why do we spend so many cycles loading data we just wrote to cache?</p>

<p>The answer is in the question and in the title of this post: too little time elapses between the instructions that write data to the cache and the ones that read the same data.<sup id="fnref:but-forwarding"><a href="#fn:but-forwarding" class="footnote">4</a></sup>
A modern out-of-order machine (e.g., most amd64 chips) can execute multiple instructions at the same time, and will start executing instructions as soon as their operands are ready, even when earlier instructions in program order are still waiting for theirs.
Machine code is essentially a messy way to encode a <a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">dataflow graph</a>, 
which means our job as micro-optimisers is, at a high level, to avoid long dependency chains and make the dataflow graph as wide as possible.
When that’s too hard, we should distribute as much scheduling slack as possible between nodes in a chain, in order to absorb the knock-on effects of cache misses and other latency spikes.
If we fail, the chip will often find itself with no instruction ready to execute; stalling the pipeline like that is like slowing down by a factor of 10.</p>

<p>The initial inner loop simply executes steps A, B, and C in order, where step C depends on the result of step B, and step B on that of step A.
In theory, a chip with a wide enough instruction reordering window could pipeline multiple loop iterations.
In practice, real hardware can only <a href="http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/">plan on the order of 100-200 instructions ahead</a>, and that mechanism depends on branches being predicted correctly.
We have to explicitly insert slack in our dataflow schedule, and we must distribute it well enough for instruction reordering to see the gaps.</p>

<p>This specific instance is a particularly bad case for contemporary machines:
step B populates the entry with regular (64-bit) register writes,
while step C copies the same bytes with vector reads and writes.
<a href="https://gist.github.com/travisdowns/bc9af3a0aaca5399824cf182f85b9e9c">Travis Downs looked into this forwarding scenario</a> and found that no other read-after-write setup behaves this badly, on Intel or AMD.
That’s probably why the <code>movdqu</code> vector load instruction was such an issue.
If the compiler had emitted the copy with GPR reads and writes,
that <em>might</em> have been enough for the hardware to hide the latency.
However, as <a href="https://twitter.com/trav_downs/status/1223766684932222976">Travis points out on Twitter</a>, it’s hard for a compiler to get that right across compilation units.
In any case, our most reliable (more so than passing this large struct by value and hoping the compiler will avoid mismatched instructions) and powerful tool to fix this at the source level is to schedule operations manually.</p>

<p>The dataflow graph for each loop iteration is currently a pure chain:</p>

<pre><code>         A1
         |
         v
         B1
         |
         v
         C1
                A2
                |
                v
                B2
                |
                v
                C2
</code></pre>

<p>How does one add slack to these chains? With bounded queues!</p>

<h1 id="experiment">Experiment</h1>

<p>My first fix was to add a one-element buffer between steps B and C.  The inner loop became</p>

<pre><code>A1. Generate a new key-value pair
C0. Insert the hash element from the previous iteration
B1. Mangle the kv pair and stash that in the buffer
A2.
C1.
B2
etc.
</code></pre>

<p>which yields a dataflow graph like</p>

<pre><code>        |     A1
        v     |
        C0    |
              |
              v
              B1
              |
              |     A2
              v     |
              C1    |
                    |
                    v
                    B2
                    |
</code></pre>

<p>We’ve introduced slack between steps A and B (there’s now step C from the previous iteration between them), and between steps B and C (we shifted step A from the next iteration between them).
There isn’t such a long delay between the definition of a value and its use that the data is likely to be evicted from L1.
However, there is more than enough work available between them to keep the pipeline busy with useful work while C waits for B’s result, or B for A’s.
That was a nice single-digit improvement in query latency for my internal benchmark, just by permuting a loop.</p>

<p>If a one-element buffer helps, we should clearly experiment with the buffer size, and that’s where I found a more impactful speed-up.
Once we have an array of elements to insert in a hash table, we can focus on a bulk insert of maybe 8 or 10 elements: instead of trying to improve the latency for
individual writes, we can focus on the throughput for multiple inserts at once.
That’s good because <a href="http://www.stuartcheshire.org/rants/Latency.html">throughput is an easier problem than latency</a>.
In the current case, passing the whole buffer to the hash table code made it easier to <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/1993/6309.html">pipeline the insert loop in software</a>:
we can compute hashes ahead of time, and accelerate random accesses to the hash table with <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_prefetch&amp;expand=4391">software prefetching</a>.
The profile for the new inner loop is flatter, and the hottest part is as follows</p>

<pre><code>      |       mov        0x8(%rsp),%rdx
 9.91 |       lea        (%r12,%r12,4),%rax
 0.64 |       prefetcht0 (%rdx,%rax,8)
17.04 |       cmp        %rcx,0x28(%rsp)
</code></pre>

<p>Again, the blame for a “slow” instruction hits the following instruction, so it’s not <code>lea</code> (multiplying by 5) or <code>cmp</code> that are slow; it’s the load from the stack and the prefetch.
The good news is that these instructions do not have any dependent.  It’s all prefetching, and that’s only used for its side effects.
Moreover, they come from a block of code that was pipelined in software and executes one full iteration ahead of where its side effects might be useful.
It doesn’t really matter if these instructions are slow: they’re still far from being on the critical path!  This last restructuring yielded a 20% speed-up on a few slow queries.</p>

<p>I described two tools that I use regularly when optimising code for contemporary hardware.
Finding ways to scatter around scheduling slack is always useful, both in software and in real life planning.<sup id="fnref:unless-people"><a href="#fn:unless-people" class="footnote">5</a></sup>
One simple way to do so is to add bounded buffers, and to flush buffers as soon as they fill up (or refill when they become empty), instead of waiting until the next write to the buffer.
However, I think the more powerful transformation is using buffering to expose bulk operations, which tends to open up more opportunities than just doing the same thing in a loop.
In the case above, we found a 20% speed-up; for someone who visit their <a href="https://help.backtrace.io/en/articles/2765535-triage">Backtrace dashboard</a> a couple times a day, that can add up to an hour or two at the end of the year.</p>

<p>TL;DR: When a function is hot enough to look into, it’s worth asking why it’s called so often, in order to focus on higher level bulk operations.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes">
  <ol>
    <li id="fn:started-work">
      <p>And by that, I mean I started working there a couple months ago (: <a href="#fnref:started-work" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:more-later">
      <p>I think that’s a meaty idea, and am planning a longer post on that data structure and where it fits in the hash/sort join continuum. <a href="#fnref:more-later" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:dont-copy">
      <p>Would I have avoided this issue if I had directly passed by value? The resulting code might have been friendlier to store-to-load forwarding than loading a whole 128 bit SSE register, but see the next footnote. <a href="#fnref:dont-copy" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:but-forwarding">
      <p>Store-to-load forwarding can help improve the performance of this pattern, when we use forwarding patterns that the hardware supports. However, this mechanism can only decrease the penalty of serial dependencies, e.g., by shaving away some or all of the time it takes to store a result to cache and load it back; even when results can feed directly into dependencies, we still have to wait for inputs to be computed. This is fundamentally a scheduling issue. <a href="#fnref:but-forwarding" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:unless-people">
      <p>Unless you’re writing schedule optimising software and people will look at the result. A final hill climbing pass to make things look artificially tight often makes for an easier sale in that situation. <a href="#fnref:unless-people" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lazy Linear Knapsack]]></title>
    <link href="https://www.pvk.ca/Blog/2020/01/20/lazy-linear-knapsack/"/>
    <updated>2020-01-20T23:10:19-05:00</updated>
    <id>https://www.pvk.ca/Blog/2020/01/20/lazy-linear-knapsack</id>
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Continuous_knapsack_problem">continuous knapsack problem</a> may be the simplest non-trivial linear
programming problem:</p>

<p>\[\max_{x \in [0, 1]^n} p’x\]
subject to
\[w’x \leq b.\]</p>

<p>It has a linear objective, one constraint, and each decision variable
is bounded to ensure the optimum exists.  Note the key difference from
the <a href="https://en.wikipedia.org/wiki/Knapsack_problem#0/1_knapsack_problem">binary knapsack problem</a>: decision variables are allowed to take any
value between 0 and 1.  In other words, we can, e.g., stick half of
a profitable but large item in the knapsack. That’s why this knapsack
problem can be solved in linear time.</p>

<h2 id="dual-to-primal-is-reasonable">Dual to primal is reasonable</h2>

<p>Duality also lets us determine the shape of all optimal solutions to
this problem.  For each item \(i\) with weight \(w_i\) and profit
\(p_i\), let its profit ratio be \(r_i = p_i / w_i,\)
and let \(\lambda^\star\) be the optimal dual (Lagrange or linear)
multiplier associated with the capacity constraint \(w’x \leq b.\)
If \(\lambda^\star = 0,\) we simply take all items with a positive
profit ratio (\(r_i &gt; 0\)) and a non-negative weight \(w_i \geq 0.\)
Otherwise, every item with a profit ratio \(r_i &gt; \lambda^\star\)
will be at its weight upper bound (1 if \(w_i \geq 0\), 0
otherwise), and items with \(r_i &lt; \lambda^\star\) will instead be
at their lower bound (0 of \(w_i \leq 0\), and 1 otherwise).</p>

<p>Critical items, items with \(r_i = \lambda^\star,\) will take any value
that results in \(w’x = b.\) Given \(\lambda^\star,\) we can
derive the sum of weights for non-critical items; divide the
remaining capacity for critical items by the total weight of critical
items, and let that be the value for every critical item (with the
appropriate sign for the weight).</p>

<p>For example, if we have capacity \(b = 10,\) and the sum of weights
for non-critical items in the knsapsack is \(8,\) we’re left with another
two units of capacity to distribute however we want among
critical items (they all have the same profit ratio \(r_i =
\lambda^\star,\) so it doesn’t matter where that capacity goes).  Say
critical items with a positive weight have a collective weight of 4;
we could then assign a value of \(2 / 4 = 0.5\) to the corresponding
decision variable (and 0 for critical items with a non-positive
weight).</p>

<p>We could instead have \(b = 10,\) and the sum of weights for
non-critical items in the knapsack \(12\): we must find two units of
capacity among critical items (they all cost \(r_i = \lambda^\star\)
per unit, so it doesn’t matter which).  If critical items with a
negative weight have a collective weight of \(-3,\) we could assign
a value of \(-2 / -3 = 0.6\overline{6}\) to the corresponding decision
variables, and 0 for critical items with a non-negative weight.</p>

<p>The last case highlights something important about the knapsack: in
general, we can’t assume that the weights <em>or profits</em> are positive.
We could have an item with a non-positive weight and non-negative
profit (that’s always worth taking), an item with positive weight and
negative profit (never interesting), or weights and profits of the
same sign.  The last case is the only one that calls for actual
decision making.  Classically, items with negative weight and profit
are rewritten away, by assuming they’re taken in the knapsack, and
replacing them with a decision variable for the complementary decision
of removing that item from the knapsack (i.e., removing the additional
capacity in order to improve the profit).  I’ll try to treat them
directly as much as possible, because that reduction can be a
significant fraction of solve times in practice.</p>

<p>The characterisation of optimal solutions above makes it easy to
directly handle elements with a negative weight: just find the optimal
multiplier, compute the contribution of non-critical elements (with
decision variables at a bound) to the left-hand side of the capacity
constraint, separately sums the negative and positive weights for
critical elements, then do a final pass to distribute the remaining
capacity to critical elements (and 0-weight / 0-value elements if one
wishes).</p>

<h2 id="solving-the-dual-looks-like-selection">Solving the dual looks like selection</h2>

<p>Finding the optimal multiplier \(\lambda^\star\) is similar to a
selection problem: the value is either 0 (the capacity constraint is
redundant), or one of the profit ratios \(r_i,\) and, given a
multiplier value \(\lambda,\) we can determine if it’s too high or
too low in linear time.  If the non-critical elements yield a left-hand
side such that critical elements
can’t add enough capacity (i.e., no solution with the optimal form can
be feasible), \(\lambda\) is too low.  If the maximum weight of
potentially optimal solutions is too low, \(\lambda\) is too high.</p>

<p>We can thus sort the items by profit ratio \(r_i\), compute the
total weight corresponding to each ratio with a prefix sum (with a
pre-pass to sum all negative weights), and perform a linear (or
binary) search to find the critical profit ratio.
Moreover, the status of non-critical items is monotonic as
\(\lambda\) grows: if an item with positive weight is taken at
\(\lambda_0\), it is also taken for every \(\lambda \leq
\lambda_0\), and a negative-weight item that’s taken at
\(\lambda_0\) is also taken for every \(\lambda \geq \lambda_0.\)
This means we can adapt selection algorithms like <a href="https://en.wikipedia.org/wiki/Quickselect">Quickselect</a> to solve
the continuous knapsack problem in linear time.</p>

<p>I’m looking at large instances, so I would like to run these
algorithms in parallel or even distributed on multiple machines, and
ideally use GPUs or SIMD extensions.  Unfortunately, selection doesn’t
parallelise very well: we can run a distributed quickselect where
every processor partitions the data in its local RAM, but that still
requires a logarithmic number of iterations.</p>

<h2 id="selection-looks-like-quantile-estimation-does-the-dual">Selection looks like quantile estimation; does the dual?</h2>

<p><a href="https://cs.stackexchange.com/questions/27685/can-someone-explain-lazyselect">Lazy Select</a> offers a completely different angle for the selection
problem.  Selecting the \(k\)th smallest element from a list of
\(n\) elements is the same as finding the \(k / n\)th quantile<sup id="fnref:abuse-of-language"><a href="#fn:abuse-of-language" class="footnote">1</a></sup> in
that list of \(n\) elements.  We can use <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">concentration bounds</a><sup id="fnref:binomial"><a href="#fn:binomial" class="footnote">2</a></sup> to estimate quantiles from a sample of, e.g.,
\(m = n^{3/4}\) elements: the population quantile value is very
probably between the \(qm - \frac{\log m}{\sqrt{m}}\)th and \(qm +
\frac{\log m}{\sqrt{m}}\)th values of the sample.  Moreover, this
range very probably includes at most \(\mathcal{O}(n^{3/4})\)
elements<sup id="fnref:same-bounds"><a href="#fn:same-bounds" class="footnote">3</a></sup>, so a second pass suffices to buffer all the
elements around the quantile, and find the exact quantile.  Even with
a much smaller sample size \(m = \sqrt{n},\) we would only need four
passes.</p>

<p>Unfortunately, we can’t directly use that correspondence between
selection and quantile estimation for the continuous knapsack.</p>

<p>I tried to apply a similar idea by sampling the knapsack elements
equiprobably, and extrapolating from a solution to the sample.  For
every \(\lambda,\) we can derive a selection function 
\(f_\lambda (i) = I[r_i \geq \lambda]w_i\) 
(invert the condition if the weight is negative),
and scale up \(\sum_i f(i)\) from the sample to the population).
As long as we sample independently of \(f\), we can reuse the
same sample for all \(f_\lambda.\)
The difficulty here is that, while the error for Lazy Select
scales as a function of \(n,\) the equivalent bounds with
variable weights are a function of \(n(|\max_i w_i| + |\min_i w_i|)^2.\)
That doesn’t seem necessarily practical; scaling with \(\sum_i |w_i|\)
would be more reasonable.</p>

<p>Good news: we can hit that, thanks to linearity.</p>

<p>Let’s assume weights are all integers.  Any item with weight \(w_i\)
is equivalent to \(w_i\) subitems with unit weight (or \(-w_i\)
elements with negative unit weight), and the same profit ratio \(r_i\),
i.e., profit \(p_i / |w_i|\).  The range of <em>subitem</em> weights is now a constant.</p>

<p>We could sample uniformly from the subitems with a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> for each
subitem, but that’s clearly linear time in the sum of weights, rather
than the number of elements.  If we wish to sample roughly \(m\) elements
from a total weight \(W = \sum_i |w_i|,\) we can instead determine how
many subitems (units of weight) to skip before sampling with a
<a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a> of
success probability \(m / W.\) This shows us how to lift the
integrality constraint on weights: sample from an <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential</a>
with the same parameter \(m / W!\)</p>

<p>That helps, but we could still end up spending much more than constant
time on very heavy elements.  The trick is to deterministically
special-case these elements: stash any element with large weight
\(w_i \geq W / m\) to the side, exactly once.  By <a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov’s inequality</a>,<sup id="fnref:pigeonhole"><a href="#fn:pigeonhole" class="footnote">4</a></sup>
we know there aren’t too many heavy elements: at most \(m.\)</p>

<h2 id="lets-test-this-out">Let’s test this out</h2>

<p>The heart of the estimation problem can be formalised as follows:
given a list of elements \(i \in [n]\) with weight \(w_i \geq 0\),
generate a sample of \(m \leq n\) elements ahead of time. After the
sample has been generated, we want to accept an arbitrary predicate
\(p \in \{0,1\}^n\) and estimate \(\sum_{i\in [n]} p(i) w_i.\)</p>

<p>We just had a sketch of an algorithm for this problem.  Let’s see
what it looks like in Python.  The initial sample logic has to
determine the total weight, and sample items with probability
proportional to their weight.  Items heavier than the cutoff are not
considered in the sample and instead saved to an auxiliary list.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>sample.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Samples from a list of (index, weight), with weight &gt;= 0.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Items with weight &gt;= cutoff are taken with probability one.</span>
</span><span class="line"><span class="sd">    Others are sampled with rate `rate` / unit of weight.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">next_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
</span><span class="line">        <span class="n">index</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">item</span>
</span><span class="line">        <span class="k">if</span> <span class="n">weight</span> <span class="o">&gt;=</span> <span class="n">cutoff</span><span class="p">:</span>
</span><span class="line">            <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">next_sample</span> <span class="o">-=</span> <span class="n">weight</span>
</span><span class="line">            <span class="k">while</span> <span class="n">next_sample</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</span><span class="line">                <span class="n">next_sample</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We can assemble the resulting sample (and list of “large” elements) to
compute a lower bound on the weight of items that satisfy any
predicate that’s independent of the sampling decisions.  The value for
large elements is trivial: we have a list of all large elements.
We can subtract the weight of all large elements from the total item
weight, and determine how much we have to extrapolate up.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>extrapolate.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hoeffding</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Determines how much we can expect a sample of n i.i.d. values</span>
</span><span class="line"><span class="sd">    sampled from a Bernouli to differ, given an error rate of alpha.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Given a sample X of n i.i.d. values from a Bernoulli distribution,</span>
</span><span class="line"><span class="sd">    let delta be \bar{X} - E[\bar{X}], the one-sided difference</span>
</span><span class="line"><span class="sd">    between the sample average value and the expected sample average.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Hoeffding&#39;s upper bound (see below) is conservative when the</span>
</span><span class="line"><span class="sd">    empirical probability is close to 0 or 1 (trivially, it can yield</span>
</span><span class="line"><span class="sd">    confidence bounds that are outside [0, 1]!), but simple, and in</span>
</span><span class="line"><span class="sd">    general not much worse than tighter confidence interval.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    P(delta &gt;= eps) &lt;= exp(-2 eps^2 n) = alpha</span>
</span><span class="line"><span class="sd">      -&gt; -2 eps^2 n = ln alpha</span>
</span><span class="line"><span class="sd">     &lt;-&gt;        eps = sqrt[-(ln alpha) / 2n ]</span>
</span><span class="line">
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">eval_weight</span><span class="p">(</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Given a population&#39;s total weight, a memoryless sample (by weight)</span>
</span><span class="line"><span class="sd">    from the population&#39;s items, and large items that were</span>
</span><span class="line"><span class="sd">    deterministically picked, evaluates a lower bound for the sum of</span>
</span><span class="line"><span class="sd">    weights for items in the population that satisfy predicate.</span>
</span><span class="line"><span class="sd">    </span>
</span><span class="line"><span class="sd">    The lower bound is taken with error rate &lt;= alpha.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">large_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="c1"># The remainder was up for sampling, unit of weight at a time.</span>
</span><span class="line">    <span class="n">sampled_weight</span> <span class="o">=</span> <span class="n">total_weight</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">sampled_weight</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">sample</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">large_sum</span>
</span><span class="line">    <span class="c1"># Estimate the Binomial success rate with a Beta</span>
</span><span class="line">    <span class="n">successes</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">)</span>
</span><span class="line">    <span class="n">failures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">successes</span>
</span><span class="line">    <span class="c1"># We want a lower bound, and the uniform prior can result in a</span>
</span><span class="line">    <span class="c1"># (valid) bound that&#39;s higher than the empirical rate, so take the</span>
</span><span class="line">    <span class="c1"># min of the two.</span>
</span><span class="line">    <span class="n">empirical_rate</span> <span class="o">=</span> <span class="n">successes</span> <span class="o">/</span> <span class="n">sampled_weight</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">hoeffding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">large_sum</span> <span class="o">+</span> <span class="n">sampled_weight</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">empirical_rate</span> <span class="o">-</span> <span class="n">delta</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And finally, here’s how we can sample from an arbitrary list of items,
compure a lower bound on the weight of items that satisfy a predicate,
and compare that with the real lower bound.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>lower_bound.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">predicate</span><span class="p">):</span>
</span><span class="line">    <span class="n">total</span> <span class="o">=</span> <span class="n">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># We expect a sample size of roughly rate * len(items), and</span>
</span><span class="line">    <span class="c1"># at most rate * len(items) large items.</span>
</span><span class="line">    <span class="n">sample</span><span class="p">,</span> <span class="n">large</span> <span class="o">=</span> <span class="n">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">total</span><span class="p">)</span>
</span><span class="line">    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">eval_weight</span><span class="p">(</span><span class="n">total</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># Check if the lower bound is valid.</span>
</span><span class="line">    <span class="n">actual</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">lower_bound</span> <span class="o">&lt;=</span> <span class="n">actual</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">actual</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>How do we test that? Far too often, I see tests for randomised
algorithms where the success rate is computed over randomly generated
inputs.  That’s too weak!  For example, this approach could lead us to accept
that the identity function is a randomised sort function, with success
probability \(\frac{1}{n!}.\)</p>

<p>The property we’re looking for is that, for any input, the success
rate (with the expectation over the pseudorandom sampling decisions)
is as high as requested.</p>

<p>For a given input (list of items and predicate), we can use the <a href="http://pvk.ca/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method/">Confidence sequence method (CSM)</a>
to confirm that the lower bound is valid at least
\(1 - \alpha\) of the time.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>csm_test.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">items</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)]</span>
</span><span class="line">    <span class="n">chosen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span><span class="p">)</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="k">yield</span> <span class="n">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">chosen</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Test case is a list of pairs of weight and predicate value</span>
</span><span class="line"><span class="sd">       rate is the sample rate</span>
</span><span class="line"><span class="sd">       alpha is the confidence parameter for the lower bound.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">wanted</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>  <span class="c1"># The Hoeffding bound is conservative, so</span>
</span><span class="line">                        <span class="c1"># this should let csm_driver stop quickly.</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">csm</span><span class="o">.</span><span class="n">csm_driver</span><span class="p">(</span><span class="n">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">rate</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">alpha</span><span class="p">),</span>
</span><span class="line">                            <span class="n">wanted</span><span class="p">,</span>
</span><span class="line">                            <span class="mf">1e-6</span><span class="p">,</span>  <span class="c1"># Wrong conclusion with p &lt; 1e-6.</span>
</span><span class="line">                            <span class="nb">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>
</span><span class="line">                            <span class="p">)</span>
</span><span class="line">    <span class="n">stop</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">result</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">actual</span> <span class="o">&gt;=</span> <span class="n">wanted</span><span class="p">,</span> <span class="s2">&quot;Result: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With a false positive rate of at most one in a million,<sup id="fnref:lotta-errors"><a href="#fn:lotta-errors" class="footnote">5</a></sup> we can
run automated tests against <code>check_bounds</code>.  I’ll use
<a href="https://hypothesis.works/">Hypothesis</a> to generate list of pairs of weight and predicate value:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">given</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">Verbosity</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line">
</span><span class="line"><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Bimodal inputs tend to be harder, so we can add a specialised test
generator.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bimodal_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">one_of</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bimodal_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, we use <a href="https://hypothesis.readthedocs.io/en/latest/data.html">Hypothesis</a> to generate inputs, and
the <a href="https://github.com/pkhuong/csm">Confidence sequence method (available in C, Common Lisp, and Python)</a> to check that the lower bound is
valid with probability at least \(1 - \alpha\).  The CSM tests
for this statistical property with power 1 and adjustable error rate
(in our case, one in a million): we only provide a generator
for success values, and the driver adaptively determines when it makes
sense to make a call and stop generating more data, while accounting
for multiple hypothesis testing.</p>

<p>TL;DR: the estimation algorithm for individual sampling passes works,
and the combination of <a href="https://hypothesis.works/">Hypothesis</a> and <a href="https://github.com/pkhuong/csm">Confidence Sequence Method</a>
lets us painlessly test for a statistical property.</p>

<p>We can iteratively use this sampling procedure to derive lower and
(symmetrically) upper bounds for the optimal Lagrange multiplier
\(\lambda^\star,\) and Hoeffding’s inequality lets us control the
probability that the lower and upper bounds are valid.  Typically,
we’d use a tolerance of \(\sqrt{\log(n) / n},\) for an error
rate of \(1 / n^2.\) I prefer to simply use something like \(7 /
\sqrt{n}:\) the error rate is then less than \(10^{-42},\)
orders of manitude smaller than the probability of hardware failure in any given
nanosecond.<sup id="fnref:memory-error"><a href="#fn:memory-error" class="footnote">6</a></sup>
We can still check for failure of our Las Vegas algorithm,
but if something went wrong, it’s much more likely that we detected
a hardware failure than anything else.  It’s like running <a href="https://en.wikipedia.org/wiki/Super_PI">SuperPi</a>
to stress test a computer, except the work is useful. 😉</p>

<h2 id="repeat-as-necessary-to-solve-a-knapsack">Repeat as necessary to solve a knapsack</h2>

<p>How many sampling passes do we need? Our bounds are in terms of the
sum of item weight: if we let our sample size be in
\(\Theta(\sqrt{n}),\) the sum of weights \(\sum_i |w_i|\) for
unfathomed items (that may or may not be chosen depending on the exact
optimal multiplier \(\lambda^\star\) in the current range) will very
probably shrink by a factor of \(\Omega(n^{1/4}).\) The initial sum can, in
the worst case, be exponentially larger than the bitlength of the
input, so even a division by \(n^{1/4}\) isn’t necessarily that
great.</p>

<p>I intend to apply this Lazy Linear Knapsack algorithm on subproblems in
a more interesting solver, and I know that the sum of weights is
bounded by the size of the initial problem, so that’s good enough for
me!  After a constant (\(\approx 4\)) number of passes, the
difference in item weight between the lower and upper bound on
\(\lambda^\star\) should also be at most 1.  One or two additional
passes will get me near optimality (e.g., within \(10^{-4}\)),
and the lower bound on \(\lambda^\star\) should thus yield
a super-optimal solution that’s infeasible by at most \(10^{-4},\)
which is, for my intended usage (again), good enough.</p>

<p>Given an optimal enough \(\lambda^\star,\) we can construct an
explicit solution in one pass, plus a simple fixup for critical items.
This Lazy Knapsack seems pretty reasonable for parallel or GPU
computing: each sampling pass only needs to read the items (i.e., no
partitioning-like shuffling) before writing a fraction of the data to
a sample buffer, and we only need a constant number of passes (around
6 or 7) in the worst case.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes">
  <ol>
    <li id="fn:abuse-of-language">
      <p>It’s more like a fractional percentile, but you know what I mean: the value such that the distribution function at that point equals \(k / n\). <a href="#fnref:abuse-of-language" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:binomial">
      <p>Binomial bounds offer even stronger confidence intervals when the estimate is close to 0 or 1 (where Hoeffding’s bound would yield a confidence interval that juts outside \([0, 1]\)), but don’t impact worst-case performance. <a href="#fnref:binomial" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:same-bounds">
      <p>Thanks to Hoeffding’s inequality, again. <a href="#fnref:same-bounds" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pigeonhole">
      <p>That’s a troll. I think any self-respecting computer person would rather see it as a sort of pigeonhole argument. <a href="#fnref:pigeonhole" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:lotta-errors">
      <p>We’re juggling a handful of error rates here. We’re checking whether the success rate for the Lazy Knapsack sampling subroutine is at least as high as \(1 - \alpha,\) as requested in the test parameters, and we’re doing so with another randomised procedure that will give an incorrect conclusion at most once every one million invocation. <a href="#fnref:lotta-errors" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:memory-error">
      <p><a href="http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf">This classic Google study</a> found 8% of DIMMs hit at least one error per year; that’s more than one single-bit error every \(10^9\) DIMM-second, and they’re mostly hard errors.  <a href="https://users.ece.cmu.edu/~omutlu/pub/memory-errors-at-facebook_dsn15.pdf">More recently, Facebook</a> reported that uncorrectable errors affect 0.03% of servers each month; that’s more than one uncorrectable error every \(10^{10}\) server-second.  If we performed one statistical test every nanosecond, the probability of memory failure alone would still dominate statistical errors by \(10^{20}!\) <a href="#fnref:memory-error" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A multiset of observations with constant-time sample mean and variance]]></title>
    <link href="https://www.pvk.ca/Blog/2019/11/30/a-multiset-of-observations-with-constant-time-sample-mean-and-variance/"/>
    <updated>2019-11-30T23:51:41-05:00</updated>
    <id>https://www.pvk.ca/Blog/2019/11/30/a-multiset-of-observations-with-constant-time-sample-mean-and-variance</id>
    <content type="html"><![CDATA[<p><small><em>Fixed notation issues in the “Faster multiset updates” section. Thank you Joonas.</em></small></p>

<p>Let’s say you have a <a href="https://en.wikipedia.org/wiki/Multiset">multiset (bag)</a> of “reals” (floats or rationals),
where each value is a sampled observations.
It’s easy to augment any implementation of the multiset ADT
to also return the sample mean of the values in the multiset in constant time:
track the sum of values in the multiset, as they are individually added and removed.
This requires one accumulator
and a counter for the number of observations in the multiset (i.e., constant space),
and adds a constant time overhead to each update.</p>

<p>It’s not as simple when you also need the sample variance of the multiset \(X\), i.e.,</p>

<p>\[\frac{1}{n - 1} \sum\sb{x \in X} (x - \hat{x})\sp{2},\]</p>

<p>where \(n = |X|\) is the sample size and
\(\hat{x}\) is the sample mean \(\sum\sb{x\in X} x/n,\)
ideally with constant query time, 
and constant and update time overhead.</p>

<p>One could try to apply the textbook equality</p>

<p>\[s\sp{2} = \frac{1}{n(n-1)}\left[n\sum\sb{x\in X} x\sp{2} - \left(\sum\sb{x\in X} x\right)\sp{2}\right].\]</p>

<p>However, as <a href="https://books.google.com/books?id=Zu-HAwAAQBAJ&amp;printsec=frontcover&amp;dq=the+art+of+computer+programming+volume+2&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwja5aGCzpPmAhWjY98KHYCGBksQuwUwAXoECAQQBw#v=onepage&amp;q=welford%20technometrics&amp;f=false">Knuth notes in TAoCP volume 2</a>,
this expression loses a lot of precision to round-off in floating point:
in extreme cases, the difference might be negative
(and we know the variance is never negative).
More commonly, we’ll lose precision
when the sampled values are clustered around a large mean.
For example, the sample standard deviation of <code>1e8</code> and <code>1e8 - 1</code>
is <code>1</code>, same as for <code>0</code> and <code>1</code>.
However, the expression above would evaluate that to <code>0.0</code>, even in double precision:
while <code>1e8</code> is comfortably within range for double floats,
its square <code>1e16</code> is outside the range where all integers are represented exactly.</p>

<p>Knuth refers to a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.302.7503&amp;rep=rep1&amp;type=pdf">better behaved recurrence by Welford</a>, where
a running sample mean is subtracted from each new observation
before squaring.
<a href="https://www.johndcook.com/blog/standard_deviation/">John Cook has a <code>C++</code> implementation</a>
of the recurrence that adds observations to a sample variance in constant time.
In Python, this streaming algorithm looks like this.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>streaming_variance.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">StreamingVariance</span><span class="p">:</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># centered 2nd moment (~variance of the sum of observations)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="k">def</span> <span class="nf">get_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>That’s all we need for insert-only multisets,
but does not handle removals;
if only we had removals,
we could always implement updates (replacement)
as a removal and an insertion.</p>

<p>Luckily, <code>StreamingVariance.observe</code> looks invertible.
It’s shouldn’t be hard to recover the previous sample mean, given <code>v</code>,
and, given the current and previous sample means,
we can re-evaluate <code>(v - old_mean) * (v - self.mean)</code> and
subtract it from <code>self.var_sum</code>.</p>

<p>Let \(\hat{x}\sp{\prime}\) be the sample mean after <code>observe(v)</code>.
We can derive the previous sample mean \(\hat{x}\) from \(v\):</p>

<p>\[(n - 1)\hat{x} = n\hat{x}\sp{\prime} - v \Leftrightarrow \hat{x} = \hat{x}\sp{\prime} + \frac{\hat{x}\sp{\prime} - v}{n-1}.\]</p>

<p>This invertibility means that we can undo calls to <code>observe</code> in
LIFO order.  We can’t handle arbitrary multiset updates, only a
stack of observation.  That’s still better than nothing.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStack</span><span class="p">:</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># variance of the sum</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">next_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">old_mean</span> <span class="o">+</span> <span class="p">(</span><span class="n">old_mean</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="n">next_n</span>
</span><span class="line">        <span class="c"># var_sum should never be negative, clamp it so.</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">-</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Before going any further, let’s test this.</p>

<h2 id="testing-the-variancestack">Testing the <code>VarianceStack</code></h2>

<p>The best way to test the <code>VarianceStack</code> is to execute a series of
<code>push</code> and <code>pop</code> calls, and compare the results of <code>get_mean</code> and
<code>get_variance</code> with batch reference implementations.</p>

<p>I could hardcode calls in unit tests.
However, that quickly hits diminishing returns in terms of
marginal coverage VS developer time.
Instead, I’ll be lazy, completely skip unit tests,
and rely on <a href="https://hypothesis.works/">Hypothesis</a>,
its <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html">high level “stateful” testing API</a>
in particular.</p>

<p>We’ll keep track of the values pushed and popped off the observation stack
in the driver: we must make sure they’re matched in LIFO order,
and we need the stack’s contents to compute the reference mean and variance.
We’ll also want to compare the results with reference implementations,
modulo some numerical noise.  Let’s try to be aggressive and bound
the number of float values between the reference and the actual results.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="kn">import</span> <span class="nn">struct</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">unittest</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">hypothesis.stateful</span> <span class="kn">import</span> <span class="n">RuleBasedStateMachine</span><span class="p">,</span> <span class="n">invariant</span><span class="p">,</span> <span class="n">precondition</span><span class="p">,</span> <span class="n">rule</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span class="line">    <span class="n">bits</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="s">&#39;=q&#39;</span><span class="p">,</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s">&#39;=d&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">significand</span> <span class="o">=</span> <span class="n">bits</span> <span class="o">%</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">63</span><span class="p">)</span>
</span><span class="line">    <span class="c"># ~significand = -1 - significand. We need that instead of just</span>
</span><span class="line">    <span class="c"># -significand to handle signed zeros.</span>
</span><span class="line">    <span class="k">return</span> <span class="n">significand</span> <span class="k">if</span> <span class="n">bits</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">~</span><span class="n">significand</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_DISTANCE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">):</span>
</span><span class="line">    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="n">max_delta</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceStackDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">allow_nan</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">allow_infinity</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Don&#39;t generate `pop()` calls when the stack is empty.</span>
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">StackTest</span> <span class="o">=</span> <span class="n">VarianceStackDriver</span><span class="o">.</span><span class="n">TestCase</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
</span><span class="line">    <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This initial driver does not even use the <code>VarianceStack</code> yet.
All it does is push values to the reference stack,
pop values when the stack has something to pop,
and check that the reference implementations match themselves after each call:
I want to first shake out any bug in the test harness itself.</p>

<p><a href="https://twitter.com/DRMacIver/status/1095662615223848960">Not surprisingly</a>,
Hypothesis does find an issue in the reference implementation:</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=0.0)
state.push(v=2.6815615859885194e+154)
state.teardown()
</code></pre>

<p>We get a numerical <code>OverflowError</code> in <code>reference_variance</code>: <code>2.68...e154 / 2</code>
is slightly greater than <code>sqrt(sys.float_info.max) = 1.3407807929942596e+154</code>,
so taking the square of that value errors out instead of returning infinity.</p>

<p>Let’s start by clamping the range of the generated floats.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="o">...</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">MAX_RANGE</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_STRATEGY</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=-</span><span class="n">MAX_RANGE</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="n">MAX_RANGE</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Now that the test harness doesn’t find fault in itself,
let’s hook in the <code>VarianceStack</code>, and see what happens
when only <code>push</code> calls are generated (i.e., first test
only the standard streaming variance algorithm).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">):</span>
</span><span class="line">    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span><span class="line">    <span class="c"># Print out some useful information on failure.</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">distance</span> <span class="o">&lt;=</span> <span class="n">max_delta</span><span class="p">,</span> <span class="s">&#39;</span><span class="si">%.18g</span><span class="s"> != </span><span class="si">%.18g</span><span class="s"> (</span><span class="si">%f</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceStackDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span> <span class="o">=</span> <span class="n">VarianceStack</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Never generate `pop()`</span>
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="ow">and</span> <span class="bp">False</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">get_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">get_variance</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This already fails horribly.</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=1.0)
state.push(v=1.488565707357403e+138)
state.teardown()
F
</code></pre>

<p>The reference finds a variance of <code>5.54e275</code>,
which is very much not the streaming computation’s <code>1.108e276</code>.
We can manually check that the reference is wrong:
it’s missing the <code>n - 1</code> correction term in the denominator.</p>

<p>We should use this updated reference.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let’s now re-enable calls to <code>pop()</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And now things fail in new and excitingly numerical ways.</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=0.0)
state.push(v=0.00014142319560050964)
state.push(v=14188.9609375)
state.pop()
state.teardown()
F
</code></pre>

<p>This counter-example fails with the online variance returning <code>0.0</code> instead of <code>1e-8</code>.
That’s not unexpected:
removing (the square of) a large value from a running sum
spells catastrophic cancellation.
It’s also not <em>that</em> bad for my use case,
where I don’t expect to observe very large values.</p>

<p>Another problem for our test harness is that
floats are very dense around <code>0.0</code>, and 
I’m ok with small (around <code>1e-8</code>) absolute error
because the input and output will be single floats.</p>

<p>Let’s relax <code>assert_almost_equal</code>, and
restrict generated observations to fall
in \([-2\sp{-12}, 2\sp{12}].\)</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="c"># Let values be off by ~1 single float ULP</span>
</span><span class="line"><span class="n">FLOAT_DISTANCE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
</span><span class="line">
</span><span class="line"><span class="c"># or by 1e-8</span>
</span><span class="line"><span class="n">ABSOLUTE_EPS</span> <span class="o">=</span> <span class="mf">1e-8</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">,</span> <span class="n">abs_eps</span><span class="o">=</span><span class="n">ABSOLUTE_EPS</span><span class="p">):</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">distance</span> <span class="o">&lt;=</span> <span class="n">max_delta</span> <span class="ow">or</span> <span class="n">delta</span> <span class="o">&lt;=</span> <span class="n">abs_eps</span><span class="p">,</span> <span class="s">&#39;</span><span class="si">%.18g</span><span class="s"> != </span><span class="si">%.18g</span><span class="s"> (</span><span class="si">%f</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span><span class="line">        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="c"># Avoid generating very large observations.</span>
</span><span class="line"><span class="n">MAX_RANGE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">12</span>
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_STRATEGY</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=-</span><span class="n">MAX_RANGE</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="n">MAX_RANGE</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With all these tweaks to make sure we generate easy (i.e., <a href="https://twitter.com/alexwlchan/status/1095663620422332416">interesting</a>)
test cases, Hypothesis fails to find a failure after its default time budget.</p>

<p>I’m willing to call that a victory.</p>

<h2 id="from-stack-to-full-multiset">From stack to full multiset</h2>

<p>We have tested code to undo updates in Welford’s classic streaming variance algorithm.
Unfortunately, inverting <code>push</code>es away only works for LIFO edits,
and we’re looking for arbitrary inserts and removals (and updates) to a multiset
of observations.</p>

<p>However, both the mean \(\hat{x} = \sum\sb{x\in X} x/n\) and 
the centered second moment \(\sum\sb{x\in X}(x - \hat{x})\sp{2}\)
are order-independent:
they’re just sums over all observations.
Disregarding round-off, we’ll find the same mean and second moment regardless
of the order in which the observations were pushed in.
Thus, whenever we wish to remove an observation from the multiset,
we can assume it was the last one added to the estimates,
and pop it off.</p>

<p>We think we know how to implement running mean and variance for a multiset of observations.
How do we test that with Hypothesis?</p>

<p>The hardest part about testing dictionary (map)-like interfaces
is making sure to generate valid identifiers when removing values.
As it turns out, Hypothesis has built-in support for this important use case,
with its <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html#rule-based-state-machines">Bundles</a>.
We’ll use that to test a dictionary from observation name to observation value,
augmented to keep track of the current mean and variance of all values.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_multiset_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceBag</span><span class="p">(</span><span class="n">VarianceStack</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span><span class="p">):</span>
</span><span class="line">        <span class="c"># Replace one instance of `old` with `new` by</span>
</span><span class="line">        <span class="c"># removing `old` and inserting `new`.</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceBagDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="n">keys</span> <span class="o">=</span> <span class="n">Bundle</span><span class="p">(</span><span class="s">&quot;keys&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceBagDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span> <span class="o">=</span> <span class="n">VarianceBag</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">keys</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">binary</span><span class="p">(),</span> <span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">add_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">update_entry</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span class="line">            <span class="k">return</span> <span class="n">multiple</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">k</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">consumes</span><span class="p">(</span><span class="n">keys</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">del_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
</span><span class="line">        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">keys</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">get_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">get_variance</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">BagTest</span> <span class="o">=</span> <span class="n">VarianceBagDriver</span><span class="o">.</span><span class="n">TestCase</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Each call to <code>add_entry</code> will either go to <code>update_entry</code> if
the key already exists, or add an observation to the dictionary
and streaming estimator.  If we have a new key, it is added
to the <code>keys</code> Bundle; calls to <code>del_entry</code> and <code>update_entry</code>
draw keys from this Bundle.  When we remove an entry, it’s
also consumed from the <code>keys</code> Bundle.</p>

<p>Hypothesis finds no fault with our new implementation of dictionary-with-variance,
but <code>update</code> seems like it could be much faster and numerically stable,
and I intend to mostly use this data structure for calls to <code>update</code>.</p>

<h2 id="faster-multiset-updates">Faster multiset updates</h2>

<p>The key operation for my use-case is to update one observation
by replacing its <code>old</code> value with a <code>new</code> one.
We can maintain the estimator by popping <code>old</code> away and pushing <code>new</code> in,
but this business with updating the number of observation <code>n</code> and
rescaling everything seems like a lot of numerical trouble.</p>

<p>We should be able to do better.</p>

<p>We’re replacing the multiset of sampled observations \(X\) with
\(X\sp{\prime} = X \setminus \{\textrm{old}\} \cup \{\textrm{new}\}.\)
It’s easy to maintain the mean after this update: \(\hat{x}\sp{\prime} = \hat{x} + (\textrm{new} - \textrm{old})/n.\)</p>

<p>The update to <code>self.var_sum</code>, the sum of squared differences from the mean, is trickier.
We start with \(v = \sum\sb{x\in X} (x - \hat{x})\sp{2},\)
and we wish to find \(v\sp{\prime} = \sum\sb{x\sp{\prime}\in X\sp{\prime}} (x\sp{\prime} - \hat{x}\sp{\prime})\sp{2}.\)</p>

<p>Let \(\delta = \textrm{new} - \textrm{old}\) and \(\delta\sb{\hat{x}} = \delta/n.\)
We have
\[\sum\sb{x\in X} (x - \hat{x}\sp{\prime})\sp{2} = \sum\sb{x\in X} [(x - \hat{x}) - \delta\sb{\hat{x}}]\sp{2},\]
and
\[[(x - \hat{x}) - \delta\sb{\hat{x}}]\sp{2} = (x - \hat{x})\sp{2} - 2\delta\sb{\hat{x}} (x - \hat{x}) + \delta\sb{\hat{x}}\sp{2}.\]</p>

<p>We can reassociate the sum, and find</p>

<p>\[\sum\sb{x\in X} (x - \hat{x}\sp{\prime})\sp{2} = \sum\sb{x\in X} (x - \hat{x})\sp{2} - 2\delta\sb{\hat{x}} \left(\sum\sb{x \in X} x - \hat{x}\right) + n \delta\sb{\hat{x}}\sp{2}\]</p>

<p>Once we notice that \(\hat{x} = \sum\sb{x\in X} x/n,\)
it’s clear that the middle term sums to zero, and we find
the very reasonable</p>

<p>\[v\sb{\hat{x}\sp{\prime}} = \sum\sb{x\in X} (x - \hat{x})\sp{2} + n \delta\sb{\hat{x}}\sp{2} = v + \delta \delta\sb{\hat{x}}.\]</p>

<p>This new accumulator \(v\sb{\hat{x}\sp{\prime}}\) corresponds to the sum of the
squared differences between the old observations \(X\) and the new mean \(\hat{x}\sp{\prime}\).
We still have to update one observation from <code>old</code> to <code>new</code>.
The remaining adjustment to \(v\) (<code>self.var_sum</code>) corresponds to
going from \((\textrm{old} - \hat{x}\sp{\prime})\sp{2}\)
to \((\textrm{new} - \hat{x}\sp{\prime})\sp{2},\)
where \(\textrm{new} = \textrm{old} + \delta.\)</p>

<p>After a bit of algebra, we get
\[(\textrm{new} - \hat{x}\sp{\prime})\sp{2} = [(\textrm{old} - \hat{x}\sp{\prime}) + \delta]\sp{2} = (\textrm{old} - \hat{x}\sp{\prime})\sp{2} + \delta (\textrm{old} - \hat{x} + \textrm{new} - \hat{x}\sp{\prime}).\]</p>

<p>The adjusted \(v\sb{\hat{x}\sp{\prime}}\) already includes
\((\textrm{old} - \hat{x}\sp{\prime})\sp{2}\)
in its sum, so we only have to add the last term
to obtain the final updated <code>self.var_sum</code></p>

<p>\[v\sp{\prime} = v\sb{\hat{x}\sp{\prime}} + \delta (\textrm{old} - \hat{x} + \textrm{new} - \hat{x}\sp{\prime}) = v + \delta [2 (\textrm{old} - \hat{x}) + \textrm{new} - \hat{x}\sp{\prime}].\]</p>

<p>That’s our final implementation for <code>VarianceBag.update</code>,
for which Hypothesis also fails to find failures.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>VarianceBag.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceBag</span><span class="p">(</span><span class="n">VarianceStack</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">new</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">delta</span> <span class="o">=</span> <span class="n">new</span> <span class="o">-</span> <span class="n">old</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="n">delta_mean</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="n">delta_mean</span>
</span><span class="line">
</span><span class="line">        <span class="n">adjustment</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">old</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta</span> <span class="o">-</span> <span class="n">delta_mean</span><span class="p">))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+</span> <span class="n">adjustment</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="how-much-do-you-trust-testing">How much do you trust testing?</h2>

<p>We have automated property-based tests and some human-checked proofs.
Ship it?</p>

<p>I was initially going to ask a <a href="https://en.wikipedia.org/wiki/Computer_algebra_system">CAS</a>
to check my reformulations,
but the implicit \(\forall\) looked messy.
Instead, I decided to check the induction hypothesis implicit in
<code>VarianceBag.update</code>, and enumerate all cases up to a certain number
of values with <a href="https://github.com/Z3Prover/z3/wiki">Z3</a> in IPython.</p>

<pre><code>In [1]: from z3 import *
In [2]: x, y, z, new_x = Reals("x y z new_x")
In [3]: mean = (x + y + z) / 3
In [4]: var_sum = sum((v - mean) * (v - mean) for v in (x, y, z))
In [5]: delta = new_x - x
In [6]: new_mean = mean + delta / 3
In [7]: delta_mean = delta / 3
In [8]: adjustment = delta * (2 * (x - mean) + (delta - delta_mean))
In [9]: new_var_sum = var_sum + adjustment

# We have our expressions. Let's check equivalence for mean, then var_sum
In [10]: s = Solver() 
In [11]: s.push()
In [12]: s.add(new_mean != (new_x + y + z) / 3)
In [13]: s.check()
Out[13]: unsat  # No counter example of size 3 for the updated mean
In [14]: s.pop()

In [15]: s.push()
In [16]: s.add(new_mean == (new_x + y + z) / 3)  # We know the mean matches
In [17]: s.add(new_var_sum != sum((v - new_mean) * (v - new_mean) for v in (new_x, y, z)))
In [18]: s.check()
Out[18]: unsat  # No counter example of size 3 for the updated variance
</code></pre>

<p>Given this script, it’s a small matter of programming to generalise
from 3 values (<code>x</code>, <code>y</code>, and <code>z</code>) to any fixed number of values, and
generate all small cases up to, e.g., 10 values.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>z3-check.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">def</span> <span class="nf">updated_expressions</span><span class="p">(</span><span class="nb">vars</span><span class="p">,</span> <span class="n">new_x</span><span class="p">):</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">num_var</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">vars</span><span class="p">)</span>
</span><span class="line">    <span class="n">mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span>
</span><span class="line">    <span class="n">var_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">v</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">)</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">new_x</span> <span class="o">-</span> <span class="n">x</span>
</span><span class="line">    <span class="n">delta_mean</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">/</span> <span class="n">num_var</span>
</span><span class="line">    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">delta_mean</span>
</span><span class="line">    <span class="n">adjustment</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta</span> <span class="o">-</span> <span class="n">delta_mean</span><span class="p">))</span>
</span><span class="line">    <span class="n">new_var_sum</span> <span class="o">=</span> <span class="n">var_sum</span> <span class="o">+</span> <span class="n">adjustment</span>
</span><span class="line">    <span class="k">return</span> <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var_sum</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">test_num_var</span><span class="p">(</span><span class="n">num_var</span><span class="p">):</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">num_var</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">    <span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">Real</span><span class="p">(</span><span class="s">&#39;x_</span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_var</span><span class="p">)]</span>
</span><span class="line">    <span class="n">new_x</span> <span class="o">=</span> <span class="n">Real</span><span class="p">(</span><span class="s">&#39;new_x&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var_sum</span> <span class="o">=</span> <span class="n">updated_expressions</span><span class="p">(</span><span class="nb">vars</span><span class="p">,</span> <span class="n">new_x</span><span class="p">)</span>
</span><span class="line">    <span class="n">new_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_x</span><span class="p">]</span> <span class="o">+</span> <span class="nb">vars</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</span><span class="line">    <span class="n">s</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">push</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_mean</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">new_vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span><span class="p">)</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">check</span><span class="p">()</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;updated mean </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">result</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="n">unsat</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">model</span><span class="p">())</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">False</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">push</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_mean</span> <span class="o">==</span> <span class="nb">sum</span><span class="p">(</span><span class="n">new_vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span><span class="p">)</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_var_sum</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span>
</span><span class="line">        <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">new_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">new_mean</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">new_vars</span><span class="p">))</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">check</span><span class="p">()</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;updated variance </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">result</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="n">unsat</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">model</span><span class="p">())</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">False</span>
</span><span class="line">    <span class="k">return</span> <span class="bp">True</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;testing n=</span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">test_num_var</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="s">&#39;OK&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">else</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="s">&#39;FAIL </span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
</span><span class="line">        <span class="k">break</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I find the most important thing when it comes to using automated proofs
is to insert errors and confirm we can find the bugs we’re looking for.</p>

<p>I did that by manually mutating the expressions for <code>new_mean</code> and <code>new_var_sum</code>
in <code>updated_expressions</code>.  This let me find a simple bug in the initial
implementation of <code>test_num_var</code>: I used <code>if not result</code> instead of <code>result != unsat</code>,
and both <code>sat</code> and <code>unsat</code> are truthy.  The code initially failed to flag a failure
when <code>z3</code> found a counter-example for our correctness condition!</p>

<h2 id="and-now-im-satisfied">And now I’m satisfied</h2>

<p>I have code to augment an arbitrary multiset or dictionary with
a running estimate of the mean and variance;
that code is based on a classic recurrence,
with some new math checked by hand,
with automated tests,
and with some exhaustive checking of small inputs (to which I claim most bugs can be reduced).</p>

<p>I’m now pretty sure the code works, but there’s another more obviously correct way to solve that update problem.
This <a href="https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2008/086212.pdf">2008 report by Philippe Pébay</a><sup id="fnref:pebay2016"><a href="#fn:pebay2016" class="footnote">1</a></sup>
presents formulas to compute the mean, variance, and arbitrary moments
in one pass,
and shows how to combine accumulators,
a useful operation in parallel computing.</p>

<p>We could use these formulas to <a href="http://blog.sigfpe.com/2010/11/statistical-fingertrees.html">augment an arbitrary \(k\)-ary tree</a>
and re-combine the merged accumulator as we go back up the (search)
tree from the modified leaf to the root.
The update would be much more stable (we only add and merge observations),
and incur logarithmic time overhead (with linear space overhead).
However, given the same time budget, and a <em>logarithmic</em> space overhead,
we could also implement the constant-time update with arbitrary precision
software floats, and probably guarantee even better precision.</p>

<p>The constant-time update I described in this post demanded more effort to convince myself
of its correctness, but I think it’s always a better option than
an augmented tree for serial code, especially if initial values
are available to populate the accumulators with batch-computed
mean and variance.
I’m pretty sure the code works, and <a href="https://gist.github.com/pkhuong/549106fc8194c0d1fce85b00c9e192d5">it’s up in this gist</a>.
I’ll be re-implementing it in C++
because that’s the language used by the project that lead me to this problem;
feel free to steal that gist.</p>
<div class="footnotes">
  <ol>
    <li id="fn:pebay2016">
      <p>There’s also a <a href="https://www.osti.gov/biblio/1426900">2016 journal article by Pébay and others</a> with numerical experiments, but I failed to implement their simpler-looking scalar update… <a href="#fnref:pebay2016" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A couple of (probabilistic) worst-case bounds for Robin Hood linear probing]]></title>
    <link href="https://www.pvk.ca/Blog/2019/09/29/a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/"/>
    <updated>2019-09-29T15:44:08-04:00</updated>
    <id>https://www.pvk.ca/Blog/2019/09/29/a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing</id>
    <content type="html"><![CDATA[<p>I like to think of Robin Hood hash tables with linear probing as
arrays sorted on uniformly distributed keys, with gaps.  That makes it
clearer that we can use these tables to implement algorithms based on
merging sorted streams in bulk, as well as ones that rely on fast
point lookups.  A key question with randomised data structures is how
badly we can expect them to perform.</p>

<h2 id="a-bound-on-the-length-of-contiguous-runs-of-entries">A bound on the length of contiguous runs of entries</h2>

<p>There’s a lot of work on the <em>expected</em> time complexity of operations
on linear probing Robin Hood hash tables.  <a href="https://dblp.uni-trier.de/pers/hd/v/Viola:Alfredo">Alfredo Viola</a>,
along with a
<a href="http://algo.inria.fr/flajolet/Publications/FlPoVi98.pdf">few collaborators</a>,
has <a href="https://cs.uwaterloo.ca/research/tr/1995/50/CS95-50.pdf">long</a>
been exploring the <a href="http://www2.math.uu.se/~svante/papers/sj297-aofa.pdf">distribution of displacements</a> (i.e., search times)
for random elements.  The <a href="https://www3.cs.stonybrook.edu/~bender/newpub/BenderHu07-TODS.pdf">packed memory array</a> angle has also been around
for <a href="https://www.cs.cmu.edu/~jcreed/p251-willard.pdf">a while</a>.<sup id="fnref:waterloo"><a href="#fn:waterloo" class="footnote">1</a></sup></p>

<p>I’m a bit wary of the “random element” aspect of the linear probing
bounds: while I’m comfortable with an expectation over the hash
function (i.e., over the uniformly distributed hash values), a program
could repeatedly ask for the same key, and consistently experience
worse-than-expected performance.  I’m more interested in bounding the
worst-case displacement (the distance between the ideal location for
an element, and where it is actually located) across all values in a
randomly generated<sup id="fnref:memory-less"><a href="#fn:memory-less" class="footnote">2</a></sup> Robin Hood table, with high enough
probability.  That probability doesn’t have to be extremely high:
\(p = 0.9\) or even \(p = 0.5\) is good enough, as long as we can
either rehash with an independent hash function, or the probability of
failure drops exponentially enough as the displacement leeway grows.</p>

<p>The people who study hashing with buckets, or hashing as load
balancing, seem more interested in these probable worst-case bounds:
as soon as one bucket overflows, it’s game over for that hash table!
In that context, we wish to determine how much headroom we must
reserve in each bucket, on top of the expected occupancy, in order to
make sure failures are rare enough.  That’s a <a href="https://en.wikipedia.org/wiki/Balls_into_bins_problem">balls into bins problem</a>, where
the \(m\) balls are entries in the hash table, and the \(n\) bins
its hash buckets.</p>

<p><a href="http://www.dblab.ntua.gr/~gtsat/collection/scheduling/Balls%20into%20Bins%20A%20Simple%20and%20Tight%20Anal.pdf">Raab and Steger’s Simple and Tight Analysis</a>
of the “balls into bins” problem shows that the case where the average occupancy grows
with \((\log m)\sp{k}\) and \(k &gt; 1\) has potential, when it comes
to worst-case bounds that shrink quickly enough: we only need headroom
that grows with \(\sqrt{\log\sp{k+1} m} = \log\sp{(k+1)/2} m\), 
slightly more than the square root of the average occupancy.</p>

<p>The only issue is that the balls-into-bins analysis is asymptotic,
and, more importantly, doesn’t apply at all to linear probing!</p>

<p>One could propose a form of packed memory array, where the sorted set
is subdivided in chunks such that the expected load per chunk is in
\(\Theta(\log\sp{k} m)\), and the size of each chunk
multiplicatively larger (more than \(\log\sp{(k+1)/2}m\))…</p>

<p>Can we instead derive similar bounds with regular linear probing?  It
turns out that Raab and Steger’s bounds are indeed simple: they
find the probability of overflow for one bucket, and derive a
<a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">union bound</a> for the probability of overflow for any bucket by
multiplying the single-bucket failure probability by the number of
buckets.  Moreover, the single-bucket case itself is a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a> confidence interval.</p>

<p>We can use the same approach for linear probing; I don’t expect a tight
result, but it might be useful.</p>

<p>Let’s say we want to determine how unlikely it is to observe a clump
of \(\log\sp{k}n\) entries, where \(n\) is the capacity of the hash
table.  We can bound the probability of observing such a clump
starting at index 0 in the backing array, and multiply by the size of
the array for our union bound (the clump could start anywhere in the array).</p>

<p>Given density \(d = m / n\), where \(m\) is the number of
entries and \(n\) is the size of the array that backs the hash
table, the probability that any given element falls in a range of size
\(\log\sp{k}n\) is \(p = d/n \log\sp{k}n\).  The number of entries
in such a range follows a Binomial distribution \(B(dn, p)\), with
expected value \(d \log\sp{k}n\).  We want to determine the maximum
density \(d\) such that 
\(\mathrm{Pr}[B(dn, p) &gt; \log\sp{k}n] &lt; \frac{\alpha}{dn}\), where
\(\alpha\) is our overall failure rate. If rehashing is acceptable,
we can let \(\alpha = 0.5\), and expect to find a
suitably uniform hash function after half a rehash on average.</p>

<p>We know we want \(k &gt; 1\) for the tail to shrink rapidly enough as
\(n\) grows, but even \(\log\sp{2}n\) doesn’t shrink very rapidly.
After some trial and error, I settled on a chunk size
\(s(n) = 5 \log\sb{2}\sp{3/2} n\).  That’s not great for small or medium sized
tables (e.g., \(s(1024) = 158.1\)), but grows slowly, and reflects
extreme worst cases; in practice, we can expect the worst case for
any table to be more reasonable.</p>

<p>Assuming we have a quantile function for the Binomial distribution, we
can find the occupancy of our chunk, at \(q = 1 - \frac{\alpha}{n}\). 
The occupancy is a monotonic function of the density, so we can use,
e.g., bisection search to find the maximum density such that the
probability that we saturate our chunk is \(\frac{\alpha}{n}\),
and thus the probability that any continuous run of entries has size
at least \(s(n) =  5 \log\sb{2}\sp{3/2} n\) is less than \(\alpha\).</p>

<p>For \(\alpha = 0.5\), the plot of densities looks like the following.</p>

<p><img class="center" src="/images/2019-09-29-a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/alpha-half.png" /></p>

<!-- sizer <- function(x) { 5 * (log(x, 2)^1.5) };
     max_overflow <- function(sizer, density, n, p) { n <- ceiling(n); run <- floor(sizer(n)); num_starts = max(1, 1 + n - run); qbinom(1 - (p / num_starts), n, density * run / n) / run }
     density <- sapply(n, function (n) uniroot(function (density) max_overflow(sizer, density, n, 0.5) - 1, c(0, 1))$root)
     ggplot(data=data.frame(n, density), aes(x = n, y = density)) + geom_line()
     -->

<p>This curve roughly matches the shape of some
<a href="https://www.pvk.ca/Blog/more_numerical_experiments_in_hashing.html">my older purely numerical experiments with Robin Hood hashing</a>.
When the table is small (less than \(\approx 1000\)), \(\log n\) is
a large fraction of \(n\), so the probability of finding a run of
size \(s(n) = 5 \log\sb{2}\sp{3/2} n\) is low.  When the table is
much larger, the asymptotic result kicks in, and the probabiliy slowly
shrinks.  However, even around the worst case \(n \approx 4500\),
we can exceed \(77\%\) density and only observe a run of length
\(s(n)\) half the time.</p>

<p>If we really don’t want to rehash, we can let \(\alpha = 10\sp{-10}\),
which compresses the curve and shifts it down: the minimum value is
now slightly above \(50\%\) density, and we can clearly see the
growth in permissible density as the size of the table grows.</p>

<!-- density <- sapply(n, function (n) uniroot(function (density) max_overflow(sizer, density, n, 1e-10) - 1, c(0, 1))$root)
     ggplot(data=data.frame(n, density), aes(x = n, y = density)) + geom_line()
     -->

<p><img class="center" src="/images/2019-09-29-a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/alpha-marginal.png" /></p>

<p>In practice, we can dynamically compute the worst-case displacement,
which is always less than the longest run (i.e., less than \(s(n) = 5
\log\sb{2}\sp{3/2} n\)).  However, having non-asymptotic bounds lets
us write size-specialised code and know that its assumptions are
likely to be satisfied in real life.</p>

<h2 id="bounding-buffer-sizes-for-operations-on-sorted-hashed-streams">Bounding buffer sizes for operations on sorted hashed streams</h2>

<p>I mentioned at the beginning of this post that we can also manipulate
Robin Hood hash tables as sorted sets, where the sort keys are
uniformly distributed hash values.</p>

<p>Let’s say we wished to merge the immutable source table <code>S</code> into the
larger destination table <code>D</code> in-place, without copying all of <code>D</code>.
For example, from</p>

<pre><code>S = [2, 3, 4]
D = [1, 6, 7, 9, 10];
</code></pre>

<p>we want the merged result</p>

<pre><code>D' = [1, 2, 3, 4, 6, 7, 9, 10].
</code></pre>

<p>The issue here is that, even with gaps, we might have to overwrite
elements of <code>D</code>, and buffer them in some scratch space until we get to
their final position.  In this case, all three elements of <code>S</code> must be
inserted between the first and second elements of <code>D</code>, so we could
need to buffer <code>D[1:4]</code>.</p>

<p>How large of a merge buffer should we reasonably plan for?</p>

<p>In general, we might have to buffer as many elements as there are in
the smaller table of <code>S</code> and <code>D</code>.  However, we’re working with hash
values, so we can expect them to be distributed uniformly.  That
should give us some grip on the problem.</p>

<p>We can do even better and only assume that both sorted sets were
sampled from the same underlying distribution.  The key idea
is that the rank of an element in <code>S</code> is equal to the
value of <code>S</code>’s
<a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution function</a>
for that element, multiplied by the size of <code>S</code> (similarly for
<code>D</code>).</p>

<p>The amount of buffering we might need is simply a measure of the
worst-case difference between the two empirical DFs: the more <code>S</code> get
ahead of <code>D</code>, the more we need to buffer values of <code>D</code> before
overwriting them (if we’re very unlucky, we might need a buffer the
same size as <code>S</code>).  That’s the
two-sample <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic">Kolmogorov-Smirnov statistic</a>, and we have 
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test">simple bounds for that distance</a>.</p>

<p>With probability \(1 - \alpha\), we’ll consume from <code>S</code> and <code>D</code>
at the same rate \(\pm \sqrt{-\frac{(|S| + |D|) \ln \alpha}{2 |S| |D|}}\).
We can let \(\alpha = 10\sp{-10}\) and pre-allocate a buffer of size</p>

<table>
  <tbody>
    <tr>
      <td>\[</td>
      <td>S</td>
      <td>\sqrt{-\frac{(</td>
      <td>S</td>
      <td>+</td>
      <td>D</td>
      <td>) \ln \alpha}{2</td>
      <td>S</td>
      <td> </td>
      <td>D</td>
      <td>}} &lt; \sqrt{\frac{23.03</td>
      <td>S</td>
      <td>(</td>
      <td>S</td>
      <td>+</td>
      <td>D</td>
      <td>)}{2</td>
      <td>D</td>
      <td>}}.\]</td>
    </tr>
  </tbody>
</table>

<p>In the worst case, \(|S| = |D|\), and we can preallocate a buffer of
size \(\sqrt{23.03 |D|} &lt; 4.8 \sqrt{|D|}\) and only need to grow
the buffer every ten billion (\(\alpha\sp{-1}\)) merge.</p>

<p>The same bound applies in a stream processing setting; I assume this
is closer to what <a href="https://twitter.com/frankmcsherry">Frank</a> had in
mind when he brought up this question.</p>

<p>Let’s assume a “push” dataflow model, where we still work on sorted
sets of uniformly distributed hash values (and the data tuples
associated with them), but now in streams that generate values every
tick.  The buffer size problem now sounds as follows.  We wish to
implement a sorted merge operator for two input streams that generate
one value every tick, and we can’t tell our sources to cease producing
values; how much buffer space might we need in order to merge them
correctly?</p>

<p>Again, we can go back to the Kolmogorov-Smirnov statistic.  In this
case however, we could buffer each stream independently, so we’re
looking for critical values for the one-sided one-sample
Kolmogorov-Smirnov test (how much one stream might get ahead of the
hypothetical exactly uniform stream).  We have recent (<a href="https://projecteuclid.org/euclid.aop/1176990746">1990</a>)
<a href="https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality">simple and tight bounds</a>
for this case as well.</p>

<p>The critical values for the one-sided case are stronger than the
two-sided two-sample critical values we used earlier: given an
overflow probability of \(1 - \alpha\), we need to buffer at most
\(\sqrt{-\frac{n \ln \alpha}{2}},\) elements.  For 
\(\alpha = 10\sp{-20}\) that’s less than \(4.8 \sqrt{n}\).<sup id="fnref:three-centuries"><a href="#fn:three-centuries" class="footnote">3</a></sup>
This square root scaling is pretty good news in practice: shrinking
\(n\) to \(\sqrt{n}\) tends to correspond to going down a rung or two in
the storage hierarchy.  For example, \(10\sp{15}\) elements is
clearly in the range of distributed storage; however, such a humongous
stream calls for a buffer of fewer than \(1.5 \cdot 10\sp{8}\)
elements, which, at a couple gigabytes, should fit in RAM on one large
machine.  Similarly, \(10\sp{10}\) elements might fill the RAM on
one machine, but the corresponding buffer of less than half a
million elements could fit in L3 cache, while one million elements
could fill the L3, and 4800 elements fit in L1 or L2.</p>

<p>What I find neat about this (probabilistic) bound on the buffer size
is its independence from the size of the other inputs to the merge
operator.  We can have a shared \(\Theta(\sqrt{n})\)-size buffer in
front of each stream, and do all our operations without worrying about
getting stuck (unless we’re extremely unlucky, in which case we can
grow the buffer a bit and resume or restart the computation).</p>

<p>Probably of more theoretical interest is the fact that these bounds do
not assume a uniform distribution, only that all the input streams are
identically and independently sampled from the same underlying
distribution. That’s the beauty of working in terms of the (inverse)
distribution functions.</p>

<h2 id="i-dont-think-theres-anything-deeper">I don’t think there’s anything deeper</h2>

<p>That’s it. Two cute tricks that use well-understood statistical
distributions in hashed data structure and algorithm design.  I doubt
there’s anything to generalise from either bounding approach.</p>

<p>However, I definitely believe they’re useful in practice. I like knowing
that I can expect the maximum displacement for a table of \(n\)
elements with Robin Hood linear probing to be less than \(5
\log\sb{2}^{3/2} n\), because that lets me select an appropriate
option for each table, as a function of that table’s maximum
displacement, while knowing the range of displacements I might have to
handle.  Having a strong bound on how much I might have to buffer for
stream join operators feels even more useful: I can pre-allocate a
single buffer per stream and not think about efficiently growing the
buffer, or signaling that a consumer is falling behind.  The
probability that I’ll need a larger buffer is so low that I just need
to handle it, however inefficiently.  In a replicated system, where
each node picks an independent hash function, I would even consider
crashing when the buffer is too small!</p>
<div class="footnotes">
  <ol>
    <li id="fn:waterloo">
      <p>I swear the Waterloo theme isn’t some <a href="https://en.wikipedia.org/wiki/Canadian_content">CanCon</a> thing. <a href="#fnref:waterloo" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:memory-less">
      <p>With consistent tie-breaking, the layout of entries in a Robin hood hash table is a function of the set of entries, independently of the sequence of add and remove operations. <a href="#fnref:memory-less" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:three-centuries">
      <p>Assuming we consume ten streams per nanosecond, we expect to experience underbuffering once every 316 years. <a href="#fnref:three-centuries" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fractional set covering with experts]]></title>
    <link href="https://www.pvk.ca/Blog/2019/04/23/fractional-set-covering-with-experts/"/>
    <updated>2019-04-23T18:05:09-04:00</updated>
    <id>https://www.pvk.ca/Blog/2019/04/23/fractional-set-covering-with-experts</id>
    <content type="html"><![CDATA[<p>Last winter break, I played with one of the annual
<a href="https://en.wikipedia.org/wiki/Vehicle_routing_problem">capacitated vehicle routing problem</a>
(CVRP) “Santa Claus” contests.  Real world family stuff
took precedence,  so, after the
obvious <a href="http://webhotel4.ruc.dk/~keld/research/LKH-3/">LKH</a>
with <a href="http://www.math.uwaterloo.ca/tsp/concorde.html">Concorde</a>
polishing for individual tours, I only had enough time for
one diversification moonshot.  I decided to treat the
high level problem of assembling prefabricated routes as
a <a href="https://en.wikipedia.org/wiki/Set_cover_problem">set covering problem</a>:
I would solve the linear programming (LP) relaxation for the
min-cost set cover, and use randomised rounding to feed new starting
points to LKH.  Add a lot of luck, and that might
just strike the right balance between solution quality and diversity.</p>

<p>Unsurprisingly, luck failed to show up, but I had ulterior motives:
I’m much more interested in exploring first order methods for
relaxations of combinatorial problems than in solving CVRPs.  The
routes I had accumulated after a couple days turned into a
<a href="https://archive.org/details/santa-cvrp-set-cover-instance">set covering LP with 1.1M decision variables, 10K constraints, and 20M nonzeros</a>.
That’s maybe denser than most combinatorial LPs (the aspect ratio
is definitely atypical), but 0.2% non-zeros is in the right ballpark.</p>

<p>As soon as I had that fractional set cover instance, I tried to solve
it with a simplex solver.  Like any good Googler, I used <a href="https://developers.google.com/optimization/lp/glop">Glop</a>… and stared at a blank terminal for more than one hour.</p>

<p>Having observed that lack of progress, I implemented the toy I really
wanted to try out: first order online “learning with experts”
(specifically, <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a>) applied to
LP <em>optimisation</em>.  I let this <a href="https://gist.github.com/pkhuong/c508849180c6cf612f7335933a88ffa6">not-particularly-optimised serial CL code</a>
run on my 1.6 GHz laptop for 21 hours, at which point the first
order method had found a 4.5% infeasible solution (i.e., all the
constraints were satisfied with \(\ldots \geq 0.955\) instead of
\(\ldots \geq 1\)).  I left Glop running long after the contest was
over, and finally stopped it with no solution after more than 40 <em>days</em>
on my 2.9 GHz E5.</p>

<p>Given the shape of the constraint matrix, I would have loved to try an
interior point method, but all my licenses had expired, and I didn’t
want to risk OOMing my workstation.  <a href="https://twitter.com/e_d_andersen">Erling Andersen</a>
was later kind enough to test Mosek’s interior point solver on it.
The runtime was much more reasonable: 
<a href="https://twitter.com/e_d_andersen/status/1120579664806842368">10 minutes on 1 core, and 4 on 12 cores</a>, with the sublinear speed-up mostly caused by the serial
crossover to a simplex basis.</p>

<p>At 21 hours for a naïve implementation, the “learning with experts”
first order method isn’t practical yet, but also not obviously
uninteresting, so I’ll write it up here.</p>

<p>Using online learning algorithms for the “experts problem” (e.g.,
<a href="https://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">Freund and Schapire’s Hedge algorithm</a>)
to solve linear programming <em>feasibility</em> is now a classic result;
<a href="https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/">Jeremy Kun has a good explanation on his blog</a>.  What’s
new here is:</p>

<ol>
  <li>Directly solving the optimisation problem.</li>
  <li>Confirming that the parameter-free nature of <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a> helps.</li>
</ol>

<p>The first item is particularly important to me because it’s a simple
modification to the LP feasibility meta-algorithm, and might make the
difference between a tool that’s only suitable for theoretical
analysis and a practical approach.</p>

<p>I’ll start by reviewing the experts problem, and how LP feasibility is
usually reduced to the former problem.  After that, I’ll
cast the reduction as a <a href="https://smartech.gatech.edu/bitstream/handle/1853/24230/karwan_mark_h_197612_phd_154133.pdf">surrogate relaxation</a>
method, rather than a <a href="https://en.wikipedia.org/wiki/Lagrangian_relaxation">Lagrangian relaxation</a>;
optimisation should flow naturally from that
point of view.  Finally, I’ll guess why I had more success
with <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a> this time than with 
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">Multiplicative Weight Update</a>
eight years ago.<sup id="fnref:wall"><a href="#fn:wall" class="footnote">1</a></sup></p>

<h2 id="the-experts-problem-and-lp-feasibility">The experts problem and LP feasibility</h2>

<p>I first heard about the experts problem while researching
dynamic sorted set data structures:
<a href="https://dspace.mit.edu/handle/1721.1/10639">Igal Galperin’s PhD dissertation</a>
describes <a href="http://user.it.uu.se/~arnea/abs/partb.html">scapegoat trees</a>, but is really about online learning with
experts.
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">Arora, Hazan, and Kale’s 2012 survey of multiplicative weight update methods</a>.
is probably a better introduction to the topic ;)</p>

<p>The experts problem comes in many variations.  The simplest form
sounds like the following.  Assume you’re playing a binary prediction
game over a predetermined number of turns, and have access to a fixed
finite set of experts at each turn.  At the beginning of every turn,
each expert offers their binary prediction (e.g., yes it will rain
today, or it will not rain today).  You then have to make a prediction
yourself, with no additional input.  The actual result (e.g., it
didn’t rain today) is revealed at the end of the turn.  In general,
you can’t expect to be right more often than the best expert at the
end of the game.  Is there a strategy that bounds the “regret,” how
many more wrong prediction you’ll make compared to the expert(s) with
the highest number of correct predictions, and in what circumstances?</p>

<p>Amazingly enough, even with an omniscient adversary that has access to
your strategy and determines both the experts’ predictions and the
actual result at the end of each turn, a stream of random bits (hidden
from the adversary) suffice to bound our expected regret in 
\(\mathcal{O}(\sqrt{T}\,\lg n)\), where \(T\) is the number of 
turns and \(n\) the number of experts.</p>

<p>I long had trouble with that claim: it just seems too good of a magic
trick to be true.  The key realisation for me was that we’re only
comparing against invidivual experts.  If each expert is a move in a
<a href="https://www.encyclopediaofmath.org/index.php/Matrix_game">matrix game</a>,
that’s the same as claiming you’ll never do much worse than any pure
strategy.  One example of a pure strategy is always playing rock in 
Rock-Paper-Scissors; pure strategies are really bad!  The trick is
actually in making that regret bound useful.</p>

<p>We need a more continuous version of the experts problem for LP
feasibility.  We’re still playing a turn-based game, but, this time,
instead of outputting a prediction, we get to “play” a mixture of the
experts (with non-negative weights that sum to 1).  At the beginning
of each turn, we describe what weight we’d like to give to each
experts (e.g., 60% rock, 40% paper, 0% scissors).  The cost
(equivalently, payoff) for each expert is then revealed (e.g.,
\(\mathrm{rock} = -0.5\), \(\mathrm{paper} = 0.5\), 
\(\mathrm{scissors} = 0\)), and we incur the weighted average
from our play (e.g., \(60\% \cdot -0.5 + 40\% \cdot 0.5 = -0.1\))
before playing the next round.<sup id="fnref:equivalent"><a href="#fn:equivalent" class="footnote">2</a></sup>  The goal is to minimise
our worst-case regret, the additive difference between the total cost
incurred by our mixtures of experts and that of the a posteriori best single
expert.  In this case as well, online learning
algorithms guarantee regret in \(\mathcal{O}(\sqrt{T} \, \lg n)\)</p>

<p>This line of research is interesting because simple algorithms achieve
that bound, with explicit constant factors on the order of 1,<sup id="fnref:which-log"><a href="#fn:which-log" class="footnote">3</a></sup>
and <a href="http://drops.dagstuhl.de/opus/volltexte/2017/7499/pdf/LIPIcs-ICALP-2017-48.pdf">those bounds are known to be non-asymptotically tight for a large class of algorithms</a>.
Like dense linear algebra or fast Fourier transforms, where algorithms
are often compared by counting individual floating point operations,
online learning has matured into such tight bounds that worst-case
regret is routinely presented without Landau notation.  Advances improve
constant factors in the worst case, or adapt to easier inputs in order
to achieve “better than worst case” performance.</p>

<p>The <a href="https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/">reduction below</a>
lets us take any learning algorithm with an additive regret bound,
and convert it to an algorithm with a corresponding worst-case
iteration complexity bound for \(\varepsilon\)-approximate LP feasibility.
An algorithm that promises low worst-case regret in \(\mathcal{O}(\sqrt{T})\)
gives us an algorithm that needs at most \(\mathcal{O}(1/\varepsilon\sp{2})\)
iterations to return a solution that almost satisfies every constraint in the
linear program, where each constraint is violated by \(\varepsilon\) or less (e.g.,
\(x \leq 1\) is actually \(x \leq 1 + \varepsilon\)).</p>

<p>We first split the linear program in two components, a simple domain
(e.g., the non-negative orthant or the \([0, 1]\sp{d}\) box) and the
actual linear constraints.  We then map each of the latter constraints
to an expert, and use an arbitrary algorithm that solves our
continuous version of the experts problem as a black box.  At each
turn, the black box will output a set of non-negative weights for the
constraints (experts).  We will average the constraints using these
weights, and attempt to find a solution in the intersection of our
simple domain and the weighted average of the linear constraints.  We
can do so in the “experts problem” setting by consider each linear
constraint’s violation as a <em>payoff</em>, or, equivalently, satisfaction
as a loss.</p>

<p>Let’s use Stigler’s <a href="https://neos-guide.org/content/diet-problem">Diet Problem with three foods and two constraints</a>
as a small example, and further simplify it by disregarding the
minimum value for calories, and the maximum value for vitamin A.  Our
simple domain here is at least the non-negative orthant: we can’t
ingest negative food.  We’ll make things more interesting by also
making sure we don’t eat more than 10 servings of any food per day.</p>

<p>The first constraint says we mustn’t get too many calories</p>

<p>\[72 x\sb{\mathrm{corn}} + 121 x\sb{\mathrm{milk}} + 65 x\sb{\mathrm{bread}} \leq 2250,\]</p>

<p>and the second constraint (tweaked to improve this example) ensures
we ge enough vitamin A</p>

<p>\[107 x\sb{\mathrm{corn}} + 400 x\sb{\mathrm{milk}} \geq 5000,\]</p>

<p>or, equivalently,</p>

<p>\[-107 x\sb{\mathrm{corn}} - 400 x\sb{\mathrm{milk}} \leq -5000,\]</p>

<p>Given weights \([3/4, 1/4]\), the weighted average of the two constraints is</p>

<p>\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]</p>

<p>where the coefficients for each variable and for the right-hand side
were averaged independently.</p>

<p>The subproblem asks us to find a feasible point in the intersection
of these two constraints:
\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Classically, we claim that this is just Lagrangian relaxation, and
find a solution to</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>In the next section, I’ll explain why I think this analogy is wrong
and worse than useless.  For now, we can easily find the minimum one
variable at a time, and find the solution 
\(x\sb{\mathrm{corn}} = 0\), \(x\sb{\mathrm{milk}} = 10\),
\(x\sb{\mathrm{bread}} = 0\), with objective value \(-92.5\) (which
is \(530\) less than \(437.5\)).</p>

<p>In general, three things can happen at this point.  We could discover
that the subproblem is infeasible.  In that case, the original
non-relaxed linear program itself is infeasible: any solution to the
original LP satisfies all of its constraints, and thus would also
satisfy any weighted average of the same constraints.  We could also
be extremely lucky and find that our optimal solution to the relaxation is
(\(\varepsilon\)-)feasible for the original linear program; we can stop
with a solution.  More commonly, we have a solution that’s feasible for the
relaxation, but not for the original linear program.</p>

<p>Since that solution satisfies the weighted average constraint and
payoffs track constraint violation, the black box’s payoff for this
turn (and for every other turn) is non-positive.  In the current case,
the first constraint (on calories) is satisfied by \(1040\), while
the second (on vitamin A) is violated by \(1000\).  On weighted
average, the constraints are satisfied by \(\frac{1}{4}(3 \cdot
1040 - 1000) = 530.\) Equivalently, they’re violated by \(-530\) on
average.</p>

<p>We’ll add that solution to an accumulator vector that will come in
handy later.</p>

<p>The next step is the key to the reduction: we’ll derive payoffs
(negative costs) for the black box from the solution to the last
relaxation.  Each constraint (expert) has a payoff equal to its level
of violation in the relaxation’s solution.  If a constraint is
strictly satisfied, the payoff is negative; for example, the constraint
on calories is satisfied by \(1040\), so its payoff this turn is
\(-1040\).  The constraint on vitamin A is violated by \(1000\),
so its payoff this turn is \(1000\).  Next turn, we expect the
black box to decrease the weight of the constraint on calories,
and to increase the weight of the one on vitamin A.</p>

<p>After \(T\) turns, the total payoff for each constraint is equal to
the sum of violations by all solutions in the accumulator.  Once we
divide both sides by \(T\), we find that the divided payoff for each
constraint is equal to its violation by the average of the solutions
in the accumulator.  For example, if we have two solutions, one that
violates the calories constraint by \(500\) and another that
satisfies it by \(1000\) (violates it by \(-1000\)), the total
payoff for the calories constraint is \(-500\), and the average
of the two solutions does strictly satisfy the linear constraint by
\(\frac{500}{2} = 250\)!</p>

<p>We also know that we only generated feasible solutions to the relaxed
subproblem (otherwise, we’d have stopped and marked the original LP as
infeasible), so the black box’s total payoff is \(0\) or negative.</p>

<p>Finally, we assumed that the black box algorithm guarantees an additive
regret in \(\mathcal{O}(\sqrt{T}\, \lg n)\), so the black box’s payoff
of (at most) \(0\) means that any constraint’s payoff is at most
\(\mathcal{O}(\sqrt{T}\, \lg n)\).  After dividing by \(T\), we obtain
a bound on the violation by the arithmetic mean of all solutions in
the accumulator: for all constraint, that violation is in 
\(\mathcal{O}\left(\frac{\lg n}{\sqrt{T}}\right)\).  In other words, the number
of iteration \(T\) must scale with
\(\mathcal{O}\left(\frac{\lg n}{\varepsilon\sp{2}}\right)\), 
which isn’t bad when \(n\) is in the millions but
\(\varepsilon \approx 0.01\).</p>

<p>Theoreticians find this reduction interesting because there are
concrete implementations of the black box, e.g., the
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">multiplicative weight update (MWU) method</a>
with non-asymptotic bounds.  For many problems, this makes it
possible to derive the exact number of iterations necessary
to find an \(\varepsilon-\)feasible fractional solution, given
\(\varepsilon\) and the instance’s size (but not the instance
itself).</p>

<p>That’s why algorithms like MWU are theoretically useful tools for
fractional approximations, when we already have subgradient methods
that only need \(\mathcal{O}\left(\frac{1}{\varepsilon}\right)\) iterations:
state-of-the-art algorithms for learning with experts explicit
non-asymptotic regret bounds that yield, for many problems, iteration
bounds that only depend on the instance’s size, but not its data.
While the iteration count when solving LP feasibility with MWU scales
with \(\frac{1}{\varepsilon\sp{2}}\), it is merely proportional to
\(\lg n\), the log of the the number of linear constraints.  That’s
attractive, compared to subgradient methods for which the iteration
count scales with \(\frac{1}{\varepsilon}\), but also scales
linearly with respect to instance-dependent values like the distance
between the initial dual solution and the optimum, or the Lipschitz
constant of the Lagrangian dual function; these values are hard to
bound, and are often proportional to the square root of the number
of constraints.  Given the choice between
\(\mathcal{O}\left(\frac{\lg n}{\varepsilon\sp{2}}\right)\) 
iterations with explicit constants, and a looser
\(\mathcal{O}\left(\frac{\sqrt{n}}{\varepsilon}\right)\), it’s 
obvious why MWU and online learning are powerful additions to 
the theory toolbox.</p>

<p>Theoreticians are otherwise not concerned with efficiency, so the
usual answer to someone asking about optimisation is to tell them they
can always reduce linear optimisation to feasibility with a binary
search on the objective value.  I once made the mistake of
implementing that binary search last strategy.  Unsurprisingly, it
wasn’t useful.  I also tried another theoretical reduction, where I
looked for a pair of primal and dual -feasible solutions that happened
to have the same objective value.  That also failed, in a more
interesting manner: since the two solution had to have almost the same
value, the universe spited me by sending back solutions that were
primal and dual infeasible in the worst possible way.  In the end, the
second reduction generated fractional solutions that were neither
feasible nor superoptimal, which really isn’t helpful.</p>

<h2 id="direct-linear-optimisation-with-experts">Direct linear optimisation with experts</h2>

<p>The reduction above works for any “simple” domain, as long as it’s
convex and we can solve the subproblems, i.e., find a point in the
intersection of the simple domain and a single linear constraint or
determine that the intersection is empty.</p>

<p>The set of (super)optimal points in some initial simple domain is
still convex, so we could restrict our search to the search of the
domain that is superoptimal for the linear program we wish to
optimise, and directly reduce optimisation to the feasibility problem
solved in the last section, without binary search.</p>

<p>That sounds silly at first: how can we find solutions that are
superoptimal when we don’t even know the optimal value?</p>

<p>Remember that the subproblems are always relaxations of the original
linear program.  We can port the objective function from the original
LP over to the subproblems, and optimise the relaxations.  Any
solution that’s optimal for a realxation must have an optimal or
superoptimal value for the original LP.</p>

<p>Rather than treating the black box online solver as a generator of 
<a href="https://en.wikipedia.org/wiki/Duality_\(optimization\)#The_strong_Lagrangian_principle:_Lagrange_duality">Lagrangian dual</a>
vectors, we’re using its weights as solutions to the
<a href="https://smartech.gatech.edu/bitstream/handle/1853/24230/karwan_mark_h_197612_phd_154133.pdf"><em>surrogate</em> relaxation dual</a>.
The latter interpretation isn’t just more powerful by handling
objective functions.  It also makes more sense: the weights generated
by algorithms for the experts problem are probabilities, i.e., they’re
non-negative and sum to \(1\).  That’s also what’s expected for surrogate
dual vectors, but definitely not the case for Lagrangian dual vectors,
even when restricted to \(\leq\) constraints.</p>

<p>We can do even better!</p>

<p>Unlike Lagrangian dual solvers, which only converge when fed
(approximate) subgradients and thus make us (nearly) optimal solutions
to the relaxed subproblems, our reduction to the experts problem only
needs feasible solutions to the subproblems.  That’s all we need to
guarantee an \(\varepsilon-\)feasible solution to the initial problem
in a bounded number of iterations.  We also know exactly how that
\(\varepsilon-\)feasible solution is generated: it’s the arithmetic
mean of the solutions for relaxed subproblems.</p>

<p>This lets us decouple finding lower bounds from generating feasible
solutions that will, on average, \(\varepsilon-\)satisfy the
original LP.  In practice, the search for an \(\varepsilon-\)feasible
solution that is also superoptimal will tend to improve the lower
bound.  However, nothing forces us to evaluate lower bounds
synchronously, or to only use the experts problem solver to improve
our bounds.</p>

<p>We can find a new bound from any vector of non-negative constraint
weights: they always yield a valid surrogate relaxation.  We can solve
that relaxation, and update our best bound when it’s improved.  The
Diet subproblem earlier had</p>

<p>\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Adding the original objective function back yields the linear program</p>

<p>\[\min 0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\]
subject to
\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10,\]</p>

<p>which has a trivial optimal solution at \([0, 0, 0]\).</p>

<p>When we generate a feasible solution for the same subproblem, we can
use any valid bound on the objective value to find the most feasible
solution that is also assuredly (super)optimal.  For example, if some
oracle has given us a lower bound of \(2\) for the original Diet
problem, we can solve for</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\leq 2\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>We can relax the objective value constraint further, since we know
that the final \(\varepsilon-\)feasible solution is a simple
arithmetic mean.  Given the same best bound of \(2\), and, e.g., a
current average of \(3\) solutions with a value of \(1.9\), a new
solution with an objective value of \(2.3\) (more than our best
bound, so not necessarily optimal!) would yield a new average solution
with a value of \(2\), which is still (super)optimal.  This means
we can solve the more relaxed subproblem</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\leq 2.3\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Given a bound on the objective value, we swapped the constraint and
the objective; the goal is to maximise feasibility, while generating a
solution that’s “good enough” to guarantee that the average solution
is still (super)optimal.</p>

<p>For box-constrained linear programs where the box is the convex
domain, subproblems are bounded linear knapsacks, so we can simply
stop the greedy algorithm as soon as the objective value constraint is
satisfied, or when the knapsack constraint becomes active (we found a
better bound).</p>

<p>This last tweak doesn’t just accelerate convergence to
\(\varepsilon-\)feasible solutions.  More importantly for me, it
pretty much guarantees that out \(\varepsilon-\)feasible solution
matches the best known lower bound, even if that bound was provided by
an outside oracle.  <a href="http://www.inrialpes.fr/bipop/people/malick/Docs/05-frangioni.pdf">Bundle methods</a>
and the <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.8194&amp;rep=rep1&amp;type=pdf">Volume algorithm</a>
can also mix solutions to relaxed subproblems in order to generate
\(\varepsilon-\)feasible solutions, but the result lacks the last
guarantee: their fractional solutions are even more superoptimal
than the best bound, and that can make bounding and variable fixing
difficult.</p>

<h2 id="the-secret-sauce-adahedge">The secret sauce: AdaHedge</h2>

<p>Before last Christmas’s CVRP set covering LP, I had always used the 
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">multiplicative weight update (MWU) algorithm</a>
as my black box online learning algorithm:  it wasn’t great, but I
couldn’t find anything better. The two main downsides for me
were that I had to know a “width” parameter ahead of time, as well
as the number of iterations I wanted to run.</p>

<p>The width is essentially the range of the payoffs; in our case, the
potential level of violation or satisfaction of each constraints by
any solution to the relaxed subproblems.  The dependence isn’t
surprising: folklore in Lagrangian relaxation also says that’s a big
factor there.  The problem is that the most extreme violations and
satisfactions are initialisation parameters for the MWU algorithm,
and the iteration count for a given \(\varepsilon\) is quadratic
in the width (\(\mathrm{max}\sb{violation} \cdot \mathrm{max}\sb{satisfaction}\)).</p>

<p>What’s even worse is that the MWU is explicitly tuned for a specific
iteration count.  If I estimate that, give my worst-case width estimate,
one million iterations will be necessary to achieve \(\varepsilon-\)feasibility,
MWU tuned for 1M iterations will need 1M iterations, even if the actual
width is narrower.</p>

<p><a href="https://arxiv.org/abs/1301.0534">de Rooij and others published AdaHedge in 2013</a>,
an algorithm that addresses both these issues by smoothly estimating
its parameter over time, without using the doubling trick.<sup id="fnref:doubling"><a href="#fn:doubling" class="footnote">4</a></sup>
AdaHedge’s loss (convergence rate to an \(\varepsilon-\)solution)
still depends on the relaxation’s width.  However, it depends on the
maximum width actually observed during the solution process, and not
on any explicit worst-case bound.  It’s also not explicily tuned for a
specific iteration count, and simply keeps improving at a rate that
roughly matches MWU.  If the instance happens to be easy, we will find
an \(\varepsilon-\)feasible solution more quickly.  In the worst
case, the iteration count is never much worse than that of an
optimally tuned MWU.</p>

<p>These <a href="https://gist.github.com/pkhuong/c508849180c6cf612f7335933a88ffa6">400 lines of Common Lisp</a>
implement AdaHedge and use it to optimise the set covering LP.  AdaHedge acts
as the online black box solver for the surrogate dual problem, the relaxed
set covering LP is a linear knapsack, and each subproblem attempts
to improve the lower bound before maximising feasibility.</p>

<p>When I ran the code, I had no idea how long it would take to find a
feasible enough solution: covering constraints can never be violated
by more than \(1\), but some points could be covered by hundreds of
tours, so the worst case satisfaction width is high. I had to rely on
the way AdaHedge adapts to the actual hardness of the problem.  In the
end, \(34492\) iterations sufficed to find a solution that was \(4.5\%\)
infeasible.<sup id="fnref:wrong-log"><a href="#fn:wrong-log" class="footnote">5</a></sup>  This corresponds to a worst case with a width
of less than \(2\), which is probably not what happened.  It seems
more likely that the surrogate dual isn’t actually an omniscient
adversary, and AdaHedge was able to exploit some of that “easiness.”</p>

<p>The iterations themselves are also reasonable: one sparse matrix /
dense vector multiplication to convert surrogate dual weights to an
average constraint, one solve of the relaxed LP, and another sparse
matrix / dense vector multiplication to compute violations for each
constraint.  The relaxed LP is a fractional \([0, 1]\) knapsack, so
the bottleneck is sorting double floats.  Each iteration took 1.8
seconds on my old laptop; I’m guessing that could easily be 10-20
times faster with vectorisation and parallelisation.</p>

<p>In another post, I’ll show how using the same surrogate dual optimisation
algorithm to mimick <a href="https://link.springer.com/article/10.1007/BF02592954">Lagrangian decomposition</a>
<a href="https://perso.ensta-paristech.fr/~diam/ro/online/Monique.Guignard-top11201.pdf">instead of Lagrangian relaxation</a>
guarantees an iteration count in \(\mathcal{O}\left(\frac{\lg \#\mathrm{nonzero}}{\varepsilon\sp{2}}\right)\) independently of luck or the specific linear constraints.</p>
<div class="footnotes">
  <ol>
    <li id="fn:wall">
      <p>Yes, I have been banging my head against that wall for a while. <a href="#fnref:wall" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:equivalent">
      <p>This is equivalent to minimising expected loss with random bits, but cleans up the reduction. <a href="#fnref:equivalent" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:which-log">
      <p>When was the last time you had to worry whether that log was natural or base-2? <a href="#fnref:which-log" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:doubling">
      <p>The doubling trick essentially says to start with an estimate for some parameters (e.g., width), then adjust it to at least double the expected iteration count when the parameter’s actual value exceeds the estimate. The sum telescopes and we only pay a constant multiplicative overhead for the dynamic update. <a href="#fnref:doubling" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:wrong-log">
      <p>I think I computed the \(\log\) of the number of decision variables instead of the number of constraints, so maybe this could have gone a bit better. <a href="#fnref:wrong-log" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
