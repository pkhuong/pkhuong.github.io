<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2020-01-22T17:13:13-05:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Lazy Linear Knapsack]]></title>
    <link href="https://www.pvk.ca/Blog/2020/01/20/lazy-linear-knapsack/"/>
    <updated>2020-01-20T23:10:19-05:00</updated>
    <id>https://www.pvk.ca/Blog/2020/01/20/lazy-linear-knapsack</id>
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Continuous_knapsack_problem">continuous knapsack problem</a> may be the simplest non-trivial linear
programming problem:</p>

<p>\[\max_{x \in [0, 1]^n} p‚Äôx\]
subject to
\[w‚Äôx \leq b.\]</p>

<p>It has a linear objective, one constraint, and each decision variable
is bounded to ensure the optimum exists.  Note the key difference from
the <a href="https://en.wikipedia.org/wiki/Knapsack_problem#0/1_knapsack_problem">binary knapsack problem</a>: decision variables are allowed to take any
value between 0 and 1.  In other words, we can, e.g., stick half of
a profitable but large item in the knapsack. That‚Äôs why this knapsack
problem can be solved in linear time.</p>

<h2 id="dual-to-primal-is-reasonable">Dual to primal is reasonable</h2>

<p>Duality also lets us determine the shape of all optimal solutions to
this problem.  For each item \(i\) with weight \(w_i\) and profit
\(p_i\), let its profit ratio be \(r_i = p_i / w_i,\)
and let \(\lambda^\star\) be the optimal dual (Lagrange or linear)
multiplier associated with the capacity constraint \(w‚Äôx \leq b.\)
If \(\lambda^\star = 0,\) we simply take all items with a positive
profit ratio (\(r_i &gt; 0\)) and a non-negative weight \(w_i \geq 0.\)
Otherwise, every item with a profit ratio \(r_i &gt; \lambda^\star\)
will be at its weight upper bound (1 if \(w_i \geq 0\), 0
otherwise), and items with \(r_i &lt; \lambda^\star\) will instead be
at their lower bound (0 of \(w_i \leq 0\), and 1 otherwise).</p>

<p>Critical items, items with \(r_i = \lambda^\star,\) will take any value
that results in \(w‚Äôx = b.\) Given \(\lambda^\star,\) we can
derive the sum of weights for non-critical items; divide the
remaining capacity for critical items by the total weight of critical
items, and let that be the value for every critical item (with the
appropriate sign for the weight).</p>

<p>For example, if we have capacity \(b = 10,\) and the sum of weights
for non-critical items in the knsapsack is \(8,\) we‚Äôre left with another
two units of capacity to distribute however we want among
critical items (they all have the same profit ratio \(r_i =
\lambda^\star,\) so it doesn‚Äôt matter where that capacity goes).  Say
critical items with a positive weight have a collective weight of 4;
we could then assign a value of \(2 / 4 = 0.5\) to the corresponding
decision variable (and 0 for critical items with a non-positive
weight).</p>

<p>We could instead have \(b = 10,\) and the sum of weights for
non-critical items in the knapsack \(12\): we must find two units of
capacity among critical items (they all cost \(r_i = \lambda^\star\)
per unit, so it doesn‚Äôt matter which).  If critical items with a
negative weight have a collective weight of \(-3,\) we could assign
a value of \(-2 / -3 = 0.6\overline{6}\) to the corresponding decision
variables, and 0 for critical items with a non-negative weight.</p>

<p>The last case highlights something important about the knapsack: in
general, we can‚Äôt assume that the weights <em>or profits</em> are positive.
We could have an item with a non-positive weight and non-negative
profit (that‚Äôs always worth taking), an item with positive weight and
negative profit (never interesting), or weights and profits of the
same sign.  The last case is the only one that calls for actual
decision making.  Classically, items with negative weight and profit
are rewritten away, by assuming they‚Äôre taken in the knapsack, and
replacing them with a decision variable for the complementary decision
of removing that item from the knapsack (i.e., removing the additional
capacity in order to improve the profit).  I‚Äôll try to treat them
directly as much as possible, because that reduction can be a
significant fraction of solve times in practice.</p>

<p>The characterisation of optimal solutions above makes it easy to
directly handle elements with a negative weight: just find the optimal
multiplier, compute the contribution of non-critical elements (with
decision variables at a bound) to the left-hand side of the capacity
constraint, separately sums the negative and positive weights for
critical elements, then do a final pass to distribute the remaining
capacity to critical elements (and 0-weight / 0-value elements if one
wishes).</p>

<h2 id="solving-the-dual-looks-like-selection">Solving the dual looks like selection</h2>

<p>Finding the optimal multiplier \(\lambda^\star\) is similar to a
selection problem: the value is either 0 (the capacity constraint is
redundant), or one of the profit ratios \(r_i,\) and, given a
multiplier value \(\lambda,\) we can determine if it‚Äôs too high or
too low in linear time.  If the non-critical elements yield a left-hand
side such that critical elements
can‚Äôt add enough capacity (i.e., no solution with the optimal form can
be feasible), \(\lambda\) is too low.  If the maximum weight of
potentially optimal solutions is too low, \(\lambda\) is too high.</p>

<p>We can thus sort the items by profit ratio \(r_i\), compute the
total weight corresponding to each ratio with a prefix sum (with a
pre-pass to sum all negative weights), and perform a linear (or
binary) search to find the critical profit ratio.
Moreover, the status of non-critical items is monotonic as
\(\lambda\) grows: if an item with positive weight is taken at
\(\lambda_0\), it is also taken for every \(\lambda \leq
\lambda_0\), and a negative-weight item that‚Äôs taken at
\(\lambda_0\) is also taken for every \(\lambda \geq \lambda_0.\)
This means we can adapt selection algorithms like <a href="https://en.wikipedia.org/wiki/Quickselect">Quickselect</a> to solve
the continuous knapsack problem in linear time.</p>

<p>I‚Äôm looking at large instances, so I would like to run these
algorithms in parallel or even distributed on multiple machines, and
ideally use GPUs or SIMD extensions.  Unfortunately, selection doesn‚Äôt
parallelise very well: we can run a distributed quickselect where
every processor partitions the data in its local RAM, but that still
requires a logarithmic number of iterations.</p>

<h2 id="selection-looks-like-quantile-estimation-does-the-dual">Selection looks like quantile estimation; does the dual?</h2>

<p><a href="https://cs.stackexchange.com/questions/27685/can-someone-explain-lazyselect">Lazy Select</a> offers a completely different angle for the selection
problem.  Selecting the \(k\)th smallest element from a list of
\(n\) elements is the same as finding the \(k / n\)th quantile<sup id="fnref:abuse-of-language"><a href="#fn:abuse-of-language" class="footnote">1</a></sup> in
that list of \(n\) elements.  We can use <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">concentration bounds</a><sup id="fnref:binomial"><a href="#fn:binomial" class="footnote">2</a></sup> to estimate quantiles from a sample of, e.g.,
\(m = n^{3/4}\) elements: the population quantile value is very
probably between the \(qm - \frac{\log m}{\sqrt{m}}\)th and \(qm +
\frac{\log m}{\sqrt{m}}\)th values of the sample.  Moreover, this
range very probably includes at most \(\mathcal{O}(n^{3/4})\)
elements<sup id="fnref:same-bounds"><a href="#fn:same-bounds" class="footnote">3</a></sup>, so a second pass suffices to buffer all the
elements around the quantile, and find the exact quantile.  Even with
a much smaller sample size \(m = \sqrt{n},\) we would only need four
passes.</p>

<p>Unfortunately, we can‚Äôt directly use that correspondence between
selection and quantile estimation for the continuous knapsack.</p>

<p>I tried to apply a similar idea by sampling the knapsack elements
equiprobably, and extrapolating from a solution to the sample.  For
every \(\lambda,\) we can derive a selection function 
\(f_\lambda (i) = I[r_i \geq \lambda]w_i\) 
(invert the condition if the weight is negative),
and scale up \(\sum_i f(i)\) from the sample to the population).
As long as we sample independently of \(f\), we can reuse the
same sample for all \(f_\lambda.\)
The difficulty here is that, while the error for Lazy Select
scales as a function of \(n,\) the equivalent bounds with
variable weights are a function of \(n(|\max_i w_i| + |\min_i w_i|)^2.\)
That doesn‚Äôt seem necessarily practical; scaling with \(\sum_i |w_i|\)
would be more reasonable.</p>

<p>Good news: we can hit that, thanks to linearity.</p>

<p>Let‚Äôs assume weights are all integers.  Any item with weight \(w_i\)
is equivalent to \(w_i\) subitems with unit weight (or \(-w_i\)
elements with negative unit weight), and the same profit ratio \(r_i\),
i.e., profit \(p_i / |w_i|\).  The range of <em>subitem</em> weights is now a constant.</p>

<p>We could sample uniformly from the subitems with a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> for each
subitem, but that‚Äôs clearly linear time in the sum of weights, rather
than the number of elements.  If we wish to sample roughly \(m\) elements
from a total weight \(W = \sum_i |w_i|,\) we can instead determine how
many subitems (units of weight) to skip before sampling with a
<a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a> of
success probability \(m / W.\) This shows us how to lift the
integrality constraint on weights: sample from an <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential</a>
with the same parameter \(m / W!\)</p>

<p>That helps, but we could still end up spending much more than constant
time on very heavy elements.  The trick is to deterministically
special-case these elements: stash any element with large weight
\(w_i \geq W / m\) to the side, exactly once.  By <a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov‚Äôs inequality</a>,<sup id="fnref:pigeonhole"><a href="#fn:pigeonhole" class="footnote">4</a></sup>
we know there aren‚Äôt too many heavy elements: at most \(m.\)</p>

<h2 id="lets-test-this-out">Let‚Äôs test this out</h2>

<p>The heart of the estimation problem can be formalised as follows:
given a list of elements \(i \in [n]\) with weight \(w_i \geq 0\),
generate a sample of \(m \leq n\) elements ahead of time. After the
sample has been generated, we want to accept an arbitrary predicate
\(p \in \{0,1\}^n\) and estimate \(\sum_{i\in [n]} p(i) w_i.\)</p>

<p>We just had a sketch of an algorithm for this problem.  Let‚Äôs see
what it looks like in Python.  The initial sample logic has to
determine the total weight, and sample items with probability
proportional to their weight.  Items heavier than the cutoff are not
considered in the sample and instead saved to an auxiliary list.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>sample.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Samples from a list of (index, weight), with weight &gt;= 0.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Items with weight &gt;= cutoff are taken with probability one.</span>
</span><span class="line"><span class="sd">    Others are sampled with rate `rate` / unit of weight.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">next_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
</span><span class="line">        <span class="n">index</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">item</span>
</span><span class="line">        <span class="k">if</span> <span class="n">weight</span> <span class="o">&gt;=</span> <span class="n">cutoff</span><span class="p">:</span>
</span><span class="line">            <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">next_sample</span> <span class="o">-=</span> <span class="n">weight</span>
</span><span class="line">            <span class="k">while</span> <span class="n">next_sample</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</span><span class="line">                <span class="n">next_sample</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We can assemble the resulting sample (and list of ‚Äúlarge‚Äù elements) to
compute a lower bound on the weight of items that satisfy any
predicate that‚Äôs independent of the sampling decisions.  The value for
large elements is trivial: we have a list of all large elements.
We can subtract the weight of all large elements from the total item
weight, and determine how much we have to extrapolate up.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>extrapolate.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hoeffding</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Determines how much we can expect a sample of n i.i.d. values</span>
</span><span class="line"><span class="sd">    sampled from a Bernouli to differ, given an error rate of alpha.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Given a sample X of n i.i.d. values from a Bernoulli distribution,</span>
</span><span class="line"><span class="sd">    let delta be \bar{X} - E[\bar{X}], the one-sided difference</span>
</span><span class="line"><span class="sd">    between the sample average value and the expected sample average.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Hoeffding&#39;s upper bound (see below) is conservative when the</span>
</span><span class="line"><span class="sd">    empirical probability is close to 0 or 1 (trivially, it can yield</span>
</span><span class="line"><span class="sd">    confidence bounds that are outside [0, 1]!), but simple, and in</span>
</span><span class="line"><span class="sd">    general not much worse than tighter confidence interval.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    P(delta &gt;= eps) &lt;= exp(-2 eps^2 n) = alpha</span>
</span><span class="line"><span class="sd">      -&gt; -2 eps^2 n = ln alpha</span>
</span><span class="line"><span class="sd">     &lt;-&gt;        eps = sqrt[-(ln alpha) / 2n ]</span>
</span><span class="line">
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">eval_weight</span><span class="p">(</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Given a population&#39;s total weight, a memoryless sample (by weight)</span>
</span><span class="line"><span class="sd">    from the population&#39;s items, and large items that were</span>
</span><span class="line"><span class="sd">    deterministically picked, evaluates a lower bound for the sum of</span>
</span><span class="line"><span class="sd">    weights for items in the population that satisfy predicate.</span>
</span><span class="line"><span class="sd">    </span>
</span><span class="line"><span class="sd">    The lower bound is taken with error rate &lt;= alpha.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">large_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="c1"># The remainder was up for sampling, unit of weight at a time.</span>
</span><span class="line">    <span class="n">sampled_weight</span> <span class="o">=</span> <span class="n">total_weight</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">sampled_weight</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">sample</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">large_sum</span>
</span><span class="line">    <span class="c1"># Estimate the Binomial success rate with a Beta</span>
</span><span class="line">    <span class="n">successes</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">)</span>
</span><span class="line">    <span class="n">failures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">successes</span>
</span><span class="line">    <span class="c1"># We want a lower bound, and the uniform prior can result in a</span>
</span><span class="line">    <span class="c1"># (valid) bound that&#39;s higher than the empirical rate, so take the</span>
</span><span class="line">    <span class="c1"># min of the two.</span>
</span><span class="line">    <span class="n">empirical_rate</span> <span class="o">=</span> <span class="n">successes</span> <span class="o">/</span> <span class="n">sampled_weight</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">hoeffding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">large_sum</span> <span class="o">+</span> <span class="n">sampled_weight</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">empirical_rate</span> <span class="o">-</span> <span class="n">delta</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And finally, here‚Äôs how we can sample from an arbitrary list of items,
compure a lower bound on the weight of items that satisfy a predicate,
and compare that with the real lower bound.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>lower_bound.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">predicate</span><span class="p">):</span>
</span><span class="line">    <span class="n">total</span> <span class="o">=</span> <span class="n">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># We expect a sample size of roughly rate * len(items), and</span>
</span><span class="line">    <span class="c1"># at most rate * len(items) large items.</span>
</span><span class="line">    <span class="n">sample</span><span class="p">,</span> <span class="n">large</span> <span class="o">=</span> <span class="n">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">total</span><span class="p">)</span>
</span><span class="line">    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">eval_weight</span><span class="p">(</span><span class="n">total</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># Check if the lower bound is valid.</span>
</span><span class="line">    <span class="n">actual</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">lower_bound</span> <span class="o">&lt;=</span> <span class="n">actual</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">actual</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>How do we test that? Far too often, I see tests for randomised
algorithms where the success rate is computed over randomly generated
inputs.  That‚Äôs too weak!  For example, this approach could lead us to accept
that the identity function is a randomised sort function, with success
probability \(\frac{1}{n!}.\)</p>

<p>The property we‚Äôre looking for is that, for any input, the success
rate (with the expectation over the pseudorandom sampling decisions)
is as high as requested.</p>

<p>For a given input (list of items and predicate), we can use the <a href="http://pvk.ca/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method/">Confidence sequence method (CSM)</a>
to confirm that the lower bound is valid at least
\(1 - \alpha\) of the time.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>csm_test.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">items</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)]</span>
</span><span class="line">    <span class="n">chosen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span><span class="p">)</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="k">yield</span> <span class="n">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">chosen</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Test case is a list of pairs of weight and predicate value</span>
</span><span class="line"><span class="sd">       rate is the sample rate</span>
</span><span class="line"><span class="sd">       alpha is the confidence parameter for the lower bound.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">wanted</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>  <span class="c1"># The Hoeffding bound is conservative, so</span>
</span><span class="line">                        <span class="c1"># this should let csm_driver stop quickly.</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">csm</span><span class="o">.</span><span class="n">csm_driver</span><span class="p">(</span><span class="n">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">rate</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">alpha</span><span class="p">),</span>
</span><span class="line">                            <span class="n">wanted</span><span class="p">,</span>
</span><span class="line">                            <span class="mf">1e-6</span><span class="p">,</span>  <span class="c1"># Wrong conclusion with p &lt; 1e-6.</span>
</span><span class="line">                            <span class="nb">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>
</span><span class="line">                            <span class="p">)</span>
</span><span class="line">    <span class="n">stop</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">result</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">actual</span> <span class="o">&gt;=</span> <span class="n">wanted</span><span class="p">,</span> <span class="s2">&quot;Result: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With a false positive rate of at most one in a million,<sup id="fnref:lotta-errors"><a href="#fn:lotta-errors" class="footnote">5</a></sup> we can
run automated tests against <code>check_bounds</code>.  I‚Äôll use
<a href="https://hypothesis.works/">Hypothesis</a> to generate list of pairs of weight and predicate value:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">given</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">Verbosity</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line">
</span><span class="line"><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Bimodal inputs tend to be harder, so we can add a specialised test
generator.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bimodal_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">one_of</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bimodal_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, we use <a href="https://hypothesis.readthedocs.io/en/latest/data.html">Hypothesis</a> to generate inputs, and
the <a href="https://github.com/pkhuong/csm">Confidence sequence method (available in C, Common Lisp, and Python)</a> to check that the lower bound is
valid with probability at least \(1 - \alpha\).  The CSM tests
for this statistical property with power 1 and adjustable error rate
(in our case, one in a million): we only provide a generator
for success values, and the driver adaptively determines when it makes
sense to make a call and stop generating more data, while accounting
for multiple hypothesis testing.</p>

<p>TL;DR: the estimation algorithm for individual sampling passes works,
and the combination of <a href="https://hypothesis.works/">Hypothesis</a> and <a href="https://github.com/pkhuong/csm">Confidence Sequence Method</a>
lets us painlessly test for a statistical property.</p>

<p>We can iteratively use this sampling procedure to derive lower and
(symmetrically) upper bounds for the optimal Lagrange multiplier
\(\lambda^\star,\) and Hoeffding‚Äôs inequality lets us control the
probability that the lower and upper bounds are valid.  Typically,
we‚Äôd use a tolerance of \(\sqrt{\log(n) / n},\) for an error
rate of \(1 / n^2.\) I prefer to simply use something like \(7 /
\sqrt{n}:\) the error rate is then less than \(10^{-42},\)
orders of manitude smaller than the probability of hardware failure in any given
nanosecond.<sup id="fnref:memory-error"><a href="#fn:memory-error" class="footnote">6</a></sup>
We can still check for failure of our Las Vegas algorithm,
but if something went wrong, it‚Äôs much more likely that we detected
a hardware failure than anything else.  It‚Äôs like running <a href="https://en.wikipedia.org/wiki/Super_PI">SuperPi</a>
to stress test a computer, except the work is useful. üòâ</p>

<h2 id="repeat-as-necessary-to-solve-a-knapsack">Repeat as necessary to solve a knapsack</h2>

<p>How many sampling passes do we need? Our bounds are in terms of the
sum of item weight: if we let our sample size be in
\(\Theta(\sqrt{n}),\) the sum of weights \(\sum_i |w_i|\) for
unfathomed items (that may or may not be chosen depending on the exact
optimal multiplier \(\lambda^\star\) in the current range) will very
probably shrink by a factor of \(\Omega(n^{1/4}).\) The initial sum can, in
the worst case, be exponentially larger than the bitlength of the
input, so even a division by \(n^{1/4}\) isn‚Äôt necessarily that
great.</p>

<p>I intend to apply this Lazy Linear Knapsack algorithm on subproblems in
a more interesting solver, and I know that the sum of weights is
bounded by the size of the initial problem, so that‚Äôs good enough for
me!  After a constant (\(\approx 4\)) number of passes, the
difference in item weight between the lower and upper bound on
\(\lambda^\star\) should also be at most 1.  One or two additional
passes will get me near optimality (e.g., within \(10^{-4}\)),
and the lower bound on \(\lambda^\star\) should thus yield
a super-optimal solution that‚Äôs infeasible by at most \(10^{-4},\)
which is, for my intended usage (again), good enough.</p>

<p>Given an optimal enough \(\lambda^\star,\) we can construct an
explicit solution in one pass, plus a simple fixup for critical items.
This Lazy Knapsack seems pretty reasonable for parallel or GPU
computing: each sampling pass only needs to read the items (i.e., no
partitioning-like shuffling) before writing a fraction of the data to
a sample buffer, and we only need a constant number of passes (around
6 or 7) in the worst case.</p>

<hr style="width: 50%" />

<div class="footnotes">
  <ol>
    <li id="fn:abuse-of-language">
      <p>It‚Äôs more like a fractional percentile, but you know what I mean: the value such that the distribution function at that point equals \(k / n\).¬†<a href="#fnref:abuse-of-language" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:binomial">
      <p>Binomial bounds offer even stronger confidence intervals when the estimate is close to 0 or 1 (where Hoeffding‚Äôs bound would yield a confidence interval that juts outside \([0, 1]\)), but don‚Äôt impact worst-case performance.¬†<a href="#fnref:binomial" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:same-bounds">
      <p>Thanks to Hoeffding‚Äôs inequality, again.¬†<a href="#fnref:same-bounds" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pigeonhole">
      <p>That‚Äôs a troll. I think any self-respecting computer person would rather see it as a sort of pigeonhole argument.¬†<a href="#fnref:pigeonhole" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:lotta-errors">
      <p>We‚Äôre juggling a handful of error rates here. We‚Äôre checking whether the success rate for the Lazy Knapsack sampling subroutine is at least as high as \(1 - \alpha,\) as requested in the test parameters, and we‚Äôre doing so with another randomised procedure that will give an incorrect conclusion at most once every one million invocation.¬†<a href="#fnref:lotta-errors" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:memory-error">
      <p><a href="http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf">This classic Google study</a> found 8% of DIMMs hit at least one error per year; that‚Äôs more than one single-bit error every \(10^9\) DIMM-second, and they‚Äôre mostly hard errors.  <a href="https://users.ece.cmu.edu/~omutlu/pub/memory-errors-at-facebook_dsn15.pdf">More recently, Facebook</a> reported that uncorrectable errors affect 0.03% of servers each month; that‚Äôs more than one uncorrectable error every \(10^{10}\) server-second.  If we performed one statistical test every nanosecond, the probability of memory failure alone would still dominate statistical errors by \(10^{20}!\)¬†<a href="#fnref:memory-error" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A multiset of observations with constant-time sample mean and variance]]></title>
    <link href="https://www.pvk.ca/Blog/2019/11/30/a-multiset-of-observations-with-constant-time-sample-mean-and-variance/"/>
    <updated>2019-11-30T23:51:41-05:00</updated>
    <id>https://www.pvk.ca/Blog/2019/11/30/a-multiset-of-observations-with-constant-time-sample-mean-and-variance</id>
    <content type="html"><![CDATA[<p><small><em>Fixed notation issues in the ‚ÄúFaster multiset updates‚Äù section. Thank you Joonas.</em></small></p>

<p>Let‚Äôs say you have a <a href="https://en.wikipedia.org/wiki/Multiset">multiset (bag)</a> of ‚Äúreals‚Äù (floats or rationals),
where each value is a sampled observations.
It‚Äôs easy to augment any implementation of the multiset ADT
to also return the sample mean of the values in the multiset in constant time:
track the sum of values in the multiset, as they are individually added and removed.
This requires one accumulator
and a counter for the number of observations in the multiset (i.e., constant space),
and adds a constant time overhead to each update.</p>

<p>It‚Äôs not as simple when you also need the sample variance of the multiset \(X\), i.e.,</p>

<p>\[\frac{1}{n - 1} \sum\sb{x \in X} (x - \hat{x})\sp{2},\]</p>

<p>where \(n = |X|\) is the sample size and
\(\hat{x}\) is the sample mean \(\sum\sb{x\in X} x/n,\)
ideally with constant query time, 
and constant and update time overhead.</p>

<p>One could try to apply the textbook equality</p>

<p>\[s\sp{2} = \frac{1}{n(n-1)}\left[n\sum\sb{x\in X} x\sp{2} - \left(\sum\sb{x\in X} x\right)\sp{2}\right].\]</p>

<p>However, as <a href="https://books.google.com/books?id=Zu-HAwAAQBAJ&amp;printsec=frontcover&amp;dq=the+art+of+computer+programming+volume+2&amp;hl=en&amp;newbks=1&amp;newbks_redir=0&amp;sa=X&amp;ved=2ahUKEwja5aGCzpPmAhWjY98KHYCGBksQuwUwAXoECAQQBw#v=onepage&amp;q=welford%20technometrics&amp;f=false">Knuth notes in TAoCP volume 2</a>,
this expression loses a lot of precision to round-off in floating point:
in extreme cases, the difference might be negative
(and we know the variance is never negative).
More commonly, we‚Äôll lose precision
when the sampled values are clustered around a large mean.
For example, the sample standard deviation of <code>1e8</code> and <code>1e8 - 1</code>
is <code>1</code>, same as for <code>0</code> and <code>1</code>.
However, the expression above would evaluate that to <code>0.0</code>, even in double precision:
while <code>1e8</code> is comfortably within range for double floats,
its square <code>1e16</code> is outside the range where all integers are represented exactly.</p>

<p>Knuth refers to a <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.302.7503&amp;rep=rep1&amp;type=pdf">better behaved recurrence by Welford</a>, where
a running sample mean is subtracted from each new observation
before squaring.
<a href="https://www.johndcook.com/blog/standard_deviation/">John Cook has a <code>C++</code> implementation</a>
of the recurrence that adds observations to a sample variance in constant time.
In Python, this streaming algorithm looks like this.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>streaming_variance.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">StreamingVariance</span><span class="p">:</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># centered 2nd moment (~variance of the sum of observations)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="k">def</span> <span class="nf">get_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>That‚Äôs all we need for insert-only multisets,
but does not handle removals;
if only we had removals,
we could always implement updates (replacement)
as a removal and an insertion.</p>

<p>Luckily, <code>StreamingVariance.observe</code> looks invertible.
It‚Äôs shouldn‚Äôt be hard to recover the previous sample mean, given <code>v</code>,
and, given the current and previous sample means,
we can re-evaluate <code>(v - old_mean) * (v - self.mean)</code> and
subtract it from <code>self.var_sum</code>.</p>

<p>Let \(\hat{x}\sp{\prime}\) be the sample mean after <code>observe(v)</code>.
We can derive the previous sample mean \(\hat{x}\) from \(v\):</p>

<p>\[(n - 1)\hat{x} = n\hat{x}\sp{\prime} - v \Leftrightarrow \hat{x} = \hat{x}\sp{\prime} + \frac{\hat{x}\sp{\prime} - v}{n-1}.\]</p>

<p>This invertibility means that we can undo calls to <code>observe</code> in
LIFO order.  We can‚Äôt handle arbitrary multiset updates, only a
stack of observation.  That‚Äôs still better than nothing.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStack</span><span class="p">:</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c"># variance of the sum</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">next_n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">old_mean</span> <span class="o">+</span> <span class="p">(</span><span class="n">old_mean</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="n">next_n</span>
</span><span class="line">        <span class="c"># var_sum should never be negative, clamp it so.</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">-</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-=</span> <span class="mi">1</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">get_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="mi">0</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Before going any further, let‚Äôs test this.</p>

<h2 id="testing-the-variancestack">Testing the <code>VarianceStack</code></h2>

<p>The best way to test the <code>VarianceStack</code> is to execute a series of
<code>push</code> and <code>pop</code> calls, and compare the results of <code>get_mean</code> and
<code>get_variance</code> with batch reference implementations.</p>

<p>I could hardcode calls in unit tests.
However, that quickly hits diminishing returns in terms of
marginal coverage VS developer time.
Instead, I‚Äôll be lazy, completely skip unit tests,
and rely on <a href="https://hypothesis.works/">Hypothesis</a>,
its <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html">high level ‚Äústateful‚Äù testing API</a>
in particular.</p>

<p>We‚Äôll keep track of the values pushed and popped off the observation stack
in the driver: we must make sure they‚Äôre matched in LIFO order,
and we need the stack‚Äôs contents to compute the reference mean and variance.
We‚Äôll also want to compare the results with reference implementations,
modulo some numerical noise.  Let‚Äôs try to be aggressive and bound
the number of float values between the reference and the actual results.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="kn">import</span> <span class="nn">struct</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">unittest</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">hypothesis.stateful</span> <span class="kn">import</span> <span class="n">RuleBasedStateMachine</span><span class="p">,</span> <span class="n">invariant</span><span class="p">,</span> <span class="n">precondition</span><span class="p">,</span> <span class="n">rule</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
</span><span class="line">    <span class="n">bits</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="s">&#39;=q&#39;</span><span class="p">,</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s">&#39;=d&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">significand</span> <span class="o">=</span> <span class="n">bits</span> <span class="o">%</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">63</span><span class="p">)</span>
</span><span class="line">    <span class="c"># ~significand = -1 - significand. We need that instead of just</span>
</span><span class="line">    <span class="c"># -significand to handle signed zeros.</span>
</span><span class="line">    <span class="k">return</span> <span class="n">significand</span> <span class="k">if</span> <span class="n">bits</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">~</span><span class="n">significand</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_DISTANCE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">10</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">):</span>
</span><span class="line">    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="n">max_delta</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceStackDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">allow_nan</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">allow_infinity</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Don&#39;t generate `pop()` calls when the stack is empty.</span>
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">StackTest</span> <span class="o">=</span> <span class="n">VarianceStackDriver</span><span class="o">.</span><span class="n">TestCase</span>
</span><span class="line">
</span><span class="line"><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
</span><span class="line">    <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This initial driver does not even use the <code>VarianceStack</code> yet.
All it does is push values to the reference stack,
pop values when the stack has something to pop,
and check that the reference implementations match themselves after each call:
I want to first shake out any bug in the test harness itself.</p>

<p><a href="https://twitter.com/DRMacIver/status/1095662615223848960">Not surprisingly</a>,
Hypothesis does find an issue in the reference implementation:</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=0.0)
state.push(v=2.6815615859885194e+154)
state.teardown()
</code></pre>

<p>We get a numerical <code>OverflowError</code> in <code>reference_variance</code>: <code>2.68...e154 / 2</code>
is slightly greater than <code>sqrt(sys.float_info.max) = 1.3407807929942596e+154</code>,
so taking the square of that value errors out instead of returning infinity.</p>

<p>Let‚Äôs start by clamping the range of the generated floats.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="o">...</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">MAX_RANGE</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">float_info</span><span class="o">.</span><span class="n">max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_STRATEGY</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=-</span><span class="n">MAX_RANGE</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="n">MAX_RANGE</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Now that the test harness doesn‚Äôt find fault in itself,
let‚Äôs hook in the <code>VarianceStack</code>, and see what happens
when only <code>push</code> calls are generated (i.e., first test
only the standard streaming variance algorithm).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">):</span>
</span><span class="line">    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span><span class="line">    <span class="c"># Print out some useful information on failure.</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">distance</span> <span class="o">&lt;=</span> <span class="n">max_delta</span><span class="p">,</span> <span class="s">&#39;</span><span class="si">%.18g</span><span class="s"> != </span><span class="si">%.18g</span><span class="s"> (</span><span class="si">%f</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceStackDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span> <span class="o">=</span> <span class="n">VarianceStack</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># Never generate `pop()`</span>
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span> <span class="ow">and</span> <span class="bp">False</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">get_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">get_variance</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This already fails horribly.</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=1.0)
state.push(v=1.488565707357403e+138)
state.teardown()
F
</code></pre>

<p>The reference finds a variance of <code>5.54e275</code>,
which is very much not the streaming computation‚Äôs <code>1.108e276</code>.
We can manually check that the reference is wrong:
it‚Äôs missing the <code>n - 1</code> correction term in the denominator.</p>

<p>We should use this updated reference.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let‚Äôs now re-enable calls to <code>pop()</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceStackDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And now things fail in new and excitingly numerical ways.</p>

<pre><code>Falsifying example:
state = VarianceStackDriver()
state.push(v=0.0)
state.push(v=0.00014142319560050964)
state.push(v=14188.9609375)
state.pop()
state.teardown()
F
</code></pre>

<p>This counter-example fails with the online variance returning <code>0.0</code> instead of <code>1e-8</code>.
That‚Äôs not unexpected:
removing (the square of) a large value from a running sum
spells catastrophic cancellation.
It‚Äôs also not <em>that</em> bad for my use case,
where I don‚Äôt expect to observe very large values.</p>

<p>Another problem for our test harness is that
floats are very dense around <code>0.0</code>, and 
I‚Äôm ok with small (around <code>1e-8</code>) absolute error
because the input and output will be single floats.</p>

<p>Let‚Äôs relax <code>assert_almost_equal</code>, and
restrict generated observations to fall
in \([-2\sp{-12}, 2\sp{12}].\)</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_stack_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="c"># Let values be off by ~1 single float ULP</span>
</span><span class="line"><span class="n">FLOAT_DISTANCE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
</span><span class="line">
</span><span class="line"><span class="c"># or by 1e-8</span>
</span><span class="line"><span class="n">ABSOLUTE_EPS</span> <span class="o">=</span> <span class="mf">1e-8</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">assert_almost_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="n">FLOAT_DISTANCE</span><span class="p">,</span> <span class="n">abs_eps</span><span class="o">=</span><span class="n">ABSOLUTE_EPS</span><span class="p">):</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</span><span class="line">    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">float_bits</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">float_bits</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">distance</span> <span class="o">&lt;=</span> <span class="n">max_delta</span> <span class="ow">or</span> <span class="n">delta</span> <span class="o">&lt;=</span> <span class="n">abs_eps</span><span class="p">,</span> <span class="s">&#39;</span><span class="si">%.18g</span><span class="s"> != </span><span class="si">%.18g</span><span class="s"> (</span><span class="si">%f</span><span class="s">)&#39;</span> <span class="o">%</span> <span class="p">(</span>
</span><span class="line">        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="c"># Avoid generating very large observations.</span>
</span><span class="line"><span class="n">MAX_RANGE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">12</span>
</span><span class="line">
</span><span class="line"><span class="n">FLOAT_STRATEGY</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">min_value</span><span class="o">=-</span><span class="n">MAX_RANGE</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="n">MAX_RANGE</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With all these tweaks to make sure we generate easy (i.e., <a href="https://twitter.com/alexwlchan/status/1095663620422332416">interesting</a>)
test cases, Hypothesis fails to find a failure after its default time budget.</p>

<p>I‚Äôm willing to call that a victory.</p>

<h2 id="from-stack-to-full-multiset">From stack to full multiset</h2>

<p>We have tested code to undo updates in Welford‚Äôs classic streaming variance algorithm.
Unfortunately, inverting <code>push</code>es away only works for LIFO edits,
and we‚Äôre looking for arbitrary inserts and removals (and updates) to a multiset
of observations.</p>

<p>However, both the mean \(\hat{x} = \sum\sb{x\in X} x/n\) and 
the centered second moment \(\sum\sb{x\in X}(x - \hat{x})\sp{2}\)
are order-independent:
they‚Äôre just sums over all observations.
Disregarding round-off, we‚Äôll find the same mean and second moment regardless
of the order in which the observations were pushed in.
Thus, whenever we wish to remove an observation from the multiset,
we can assume it was the last one added to the estimates,
and pop it off.</p>

<p>We think we know how to implement running mean and variance for a multiset of observations.
How do we test that with Hypothesis?</p>

<p>The hardest part about testing dictionary (map)-like interfaces
is making sure to generate valid identifiers when removing values.
As it turns out, Hypothesis has built-in support for this important use case,
with its <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html#rule-based-state-machines">Bundles</a>.
We‚Äôll use that to test a dictionary from observation name to observation value,
augmented to keep track of the current mean and variance of all values.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>variance_multiset_driver.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceBag</span><span class="p">(</span><span class="n">VarianceStack</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span><span class="p">):</span>
</span><span class="line">        <span class="c"># Replace one instance of `old` with `new` by</span>
</span><span class="line">        <span class="c"># removing `old` and inserting `new`.</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">new</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">VarianceBagDriver</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="n">keys</span> <span class="o">=</span> <span class="n">Bundle</span><span class="p">(</span><span class="s">&quot;keys&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">(</span><span class="n">VarianceBagDriver</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span> <span class="o">=</span> <span class="n">VarianceBag</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">keys</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">binary</span><span class="p">(),</span> <span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">add_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">update_entry</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span><span class="line">            <span class="k">return</span> <span class="n">multiple</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">k</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">consumes</span><span class="p">(</span><span class="n">keys</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">del_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
</span><span class="line">        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">keys</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">FLOAT_STRATEGY</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_mean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">reference_variance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="p">)</span>
</span><span class="line">        <span class="k">if</span> <span class="n">n</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">()</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">entries</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">mean_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_mean</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">get_mean</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@invariant</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">variance_matches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">assert_almost_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_variance</span><span class="p">(),</span>
</span><span class="line">                            <span class="bp">self</span><span class="o">.</span><span class="n">variance_bag</span><span class="o">.</span><span class="n">get_variance</span><span class="p">())</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">BagTest</span> <span class="o">=</span> <span class="n">VarianceBagDriver</span><span class="o">.</span><span class="n">TestCase</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Each call to <code>add_entry</code> will either go to <code>update_entry</code> if
the key already exists, or add an observation to the dictionary
and streaming estimator.  If we have a new key, it is added
to the <code>keys</code> Bundle; calls to <code>del_entry</code> and <code>update_entry</code>
draw keys from this Bundle.  When we remove an entry, it‚Äôs
also consumed from the <code>keys</code> Bundle.</p>

<p>Hypothesis finds no fault with our new implementation of dictionary-with-variance,
but <code>update</code> seems like it could be much faster and numerically stable,
and I intend to mostly use this data structure for calls to <code>update</code>.</p>

<h2 id="faster-multiset-updates">Faster multiset updates</h2>

<p>The key operation for my use-case is to update one observation
by replacing its <code>old</code> value with a <code>new</code> one.
We can maintain the estimator by popping <code>old</code> away and pushing <code>new</code> in,
but this business with updating the number of observation <code>n</code> and
rescaling everything seems like a lot of numerical trouble.</p>

<p>We should be able to do better.</p>

<p>We‚Äôre replacing the multiset of sampled observations \(X\) with
\(X\sp{\prime} = X \setminus \{\textrm{old}\} \cup \{\textrm{new}\}.\)
It‚Äôs easy to maintain the mean after this update: \(\hat{x}\sp{\prime} = \hat{x} + (\textrm{new} - \textrm{old})/n.\)</p>

<p>The update to <code>self.var_sum</code>, the sum of squared differences from the mean, is trickier.
We start with \(v = \sum\sb{x\in X} (x - \hat{x})\sp{2},\)
and we wish to find \(v\sp{\prime} = \sum\sb{x\sp{\prime}\in X\sp{\prime}} (x\sp{\prime} - \hat{x}\sp{\prime})\sp{2}.\)</p>

<p>Let \(\delta = \textrm{new} - \textrm{old}\) and \(\delta\sb{\hat{x}} = \delta/n.\)
We have
\[\sum\sb{x\in X} (x - \hat{x}\sp{\prime})\sp{2} = \sum\sb{x\in X} [(x - \hat{x}) - \delta\sb{\hat{x}}]\sp{2},\]
and
\[[(x - \hat{x}) - \delta\sb{\hat{x}}]\sp{2} = (x - \hat{x})\sp{2} - 2\delta\sb{\hat{x}} (x - \hat{x}) + \delta\sb{\hat{x}}\sp{2}.\]</p>

<p>We can reassociate the sum, and find</p>

<p>\[\sum\sb{x\in X} (x - \hat{x}\sp{\prime})\sp{2} = \sum\sb{x\in X} (x - \hat{x})\sp{2} - 2\delta\sb{\hat{x}} \left(\sum\sb{x \in X} x - \hat{x}\right) + n \delta\sb{\hat{x}}\sp{2}\]</p>

<p>Once we notice that \(\hat{x} = \sum\sb{x\in X} x/n,\)
it‚Äôs clear that the middle term sums to zero, and we find
the very reasonable</p>

<p>\[v\sb{\hat{x}\sp{\prime}} = \sum\sb{x\in X} (x - \hat{x})\sp{2} + n \delta\sb{\hat{x}}\sp{2} = v + \delta \delta\sb{\hat{x}}.\]</p>

<p>This new accumulator \(v\sb{\hat{x}\sp{\prime}}\) corresponds to the sum of the
squared differences between the old observations \(X\) and the new mean \(\hat{x}\sp{\prime}\).
We still have to update one observation from <code>old</code> to <code>new</code>.
The remaining adjustment to \(v\) (<code>self.var_sum</code>) corresponds to
going from \((\textrm{old} - \hat{x}\sp{\prime})\sp{2}\)
to \((\textrm{new} - \hat{x}\sp{\prime})\sp{2},\)
where \(\textrm{new} = \textrm{old} + \delta.\)</p>

<p>After a bit of algebra, we get
\[(\textrm{new} - \hat{x}\sp{\prime})\sp{2} = [(\textrm{old} - \hat{x}\sp{\prime}) + \delta]\sp{2} = (\textrm{old} - \hat{x}\sp{\prime})\sp{2} + \delta (\textrm{old} - \hat{x} + \textrm{new} - \hat{x}\sp{\prime}).\]</p>

<p>The adjusted \(v\sb{\hat{x}\sp{\prime}}\) already includes
\((\textrm{old} - \hat{x}\sp{\prime})\sp{2}\)
in its sum, so we only have to add the last term
to obtain the final updated <code>self.var_sum</code></p>

<p>\[v\sp{\prime} = v\sb{\hat{x}\sp{\prime}} + \delta (\textrm{old} - \hat{x} + \textrm{new} - \hat{x}\sp{\prime}) = v + \delta [2 (\textrm{old} - \hat{x}) + \textrm{new} - \hat{x}\sp{\prime}].\]</p>

<p>That‚Äôs our final implementation for <code>VarianceBag.update</code>,
for which Hypothesis also fails to find failures.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>VarianceBag.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">class</span> <span class="nc">VarianceBag</span><span class="p">(</span><span class="n">VarianceStack</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">new</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">            <span class="k">return</span>
</span><span class="line">        <span class="n">delta</span> <span class="o">=</span> <span class="n">new</span> <span class="o">-</span> <span class="n">old</span>
</span><span class="line">        <span class="n">old_mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span>
</span><span class="line">        <span class="n">delta_mean</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">+=</span> <span class="n">delta_mean</span>
</span><span class="line">
</span><span class="line">        <span class="n">adjustment</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">old</span> <span class="o">-</span> <span class="n">old_mean</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta</span> <span class="o">-</span> <span class="n">delta_mean</span><span class="p">))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_sum</span> <span class="o">+</span> <span class="n">adjustment</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="how-much-do-you-trust-testing">How much do you trust testing?</h2>

<p>We have automated property-based tests and some human-checked proofs.
Ship it?</p>

<p>I was initially going to ask a <a href="https://en.wikipedia.org/wiki/Computer_algebra_system">CAS</a>
to check my reformulations,
but the implicit \(\forall\) looked messy.
Instead, I decided to check the induction hypothesis implicit in
<code>VarianceBag.update</code>, and enumerate all cases up to a certain number
of values with <a href="https://github.com/Z3Prover/z3/wiki">Z3</a> in IPython.</p>

<pre><code>In [1]: from z3 import *
In [2]: x, y, z, new_x = Reals("x y z new_x")
In [3]: mean = (x + y + z) / 3
In [4]: var_sum = sum((v - mean) * (v - mean) for v in (x, y, z))
In [5]: delta = new_x - x
In [6]: new_mean = mean + delta / 3
In [7]: delta_mean = delta / 3
In [8]: adjustment = delta * (2 * (x - mean) + (delta - delta_mean))
In [9]: new_var_sum = var_sum + adjustment

# We have our expressions. Let's check equivalence for mean, then var_sum
In [10]: s = Solver() 
In [11]: s.push()
In [12]: s.add(new_mean != (new_x + y + z) / 3)
In [13]: s.check()
Out[13]: unsat  # No counter example of size 3 for the updated mean
In [14]: s.pop()

In [15]: s.push()
In [16]: s.add(new_mean == (new_x + y + z) / 3)  # We know the mean matches
In [17]: s.add(new_var_sum != sum((v - new_mean) * (v - new_mean) for v in (new_x, y, z)))
In [18]: s.check()
Out[18]: unsat  # No counter example of size 3 for the updated variance
</code></pre>

<p>Given this script, it‚Äôs a small matter of programming to generalise
from 3 values (<code>x</code>, <code>y</code>, and <code>z</code>) to any fixed number of values, and
generate all small cases up to, e.g., 10 values.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>z3-check.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span class="k">def</span> <span class="nf">updated_expressions</span><span class="p">(</span><span class="nb">vars</span><span class="p">,</span> <span class="n">new_x</span><span class="p">):</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">num_var</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">vars</span><span class="p">)</span>
</span><span class="line">    <span class="n">mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span>
</span><span class="line">    <span class="n">var_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">v</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">vars</span><span class="p">)</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">new_x</span> <span class="o">-</span> <span class="n">x</span>
</span><span class="line">    <span class="n">delta_mean</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">/</span> <span class="n">num_var</span>
</span><span class="line">    <span class="n">new_mean</span> <span class="o">=</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">delta_mean</span>
</span><span class="line">    <span class="n">adjustment</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">delta</span> <span class="o">-</span> <span class="n">delta_mean</span><span class="p">))</span>
</span><span class="line">    <span class="n">new_var_sum</span> <span class="o">=</span> <span class="n">var_sum</span> <span class="o">+</span> <span class="n">adjustment</span>
</span><span class="line">    <span class="k">return</span> <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var_sum</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">test_num_var</span><span class="p">(</span><span class="n">num_var</span><span class="p">):</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">num_var</span> <span class="o">&gt;</span> <span class="mi">0</span>
</span><span class="line">    <span class="nb">vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">Real</span><span class="p">(</span><span class="s">&#39;x_</span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_var</span><span class="p">)]</span>
</span><span class="line">    <span class="n">new_x</span> <span class="o">=</span> <span class="n">Real</span><span class="p">(</span><span class="s">&#39;new_x&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">new_mean</span><span class="p">,</span> <span class="n">new_var_sum</span> <span class="o">=</span> <span class="n">updated_expressions</span><span class="p">(</span><span class="nb">vars</span><span class="p">,</span> <span class="n">new_x</span><span class="p">)</span>
</span><span class="line">    <span class="n">new_vars</span> <span class="o">=</span> <span class="p">[</span><span class="n">new_x</span><span class="p">]</span> <span class="o">+</span> <span class="nb">vars</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</span><span class="line">    <span class="n">s</span> <span class="o">=</span> <span class="n">Solver</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">push</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_mean</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">new_vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span><span class="p">)</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">check</span><span class="p">()</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;updated mean </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">result</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="n">unsat</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">model</span><span class="p">())</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">False</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">push</span><span class="p">()</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_mean</span> <span class="o">==</span> <span class="nb">sum</span><span class="p">(</span><span class="n">new_vars</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_var</span><span class="p">)</span>
</span><span class="line">    <span class="n">s</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">new_var_sum</span> <span class="o">!=</span> <span class="nb">sum</span><span class="p">(</span>
</span><span class="line">        <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">new_mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">-</span> <span class="n">new_mean</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">new_vars</span><span class="p">))</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">check</span><span class="p">()</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;updated variance </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">result</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">result</span> <span class="o">!=</span> <span class="n">unsat</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">model</span><span class="p">())</span>
</span><span class="line">        <span class="k">return</span> <span class="bp">False</span>
</span><span class="line">    <span class="k">return</span> <span class="bp">True</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
</span><span class="line">    <span class="k">print</span><span class="p">(</span><span class="s">&#39;testing n=</span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">test_num_var</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="s">&#39;OK&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">else</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span><span class="p">(</span><span class="s">&#39;FAIL </span><span class="si">%i</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
</span><span class="line">        <span class="k">break</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I find the most important thing when it comes to using automated proofs
is to insert errors and confirm we can find the bugs we‚Äôre looking for.</p>

<p>I did that by manually mutating the expressions for <code>new_mean</code> and <code>new_var_sum</code>
in <code>updated_expressions</code>.  This let me find a simple bug in the initial
implementation of <code>test_num_var</code>: I used <code>if not result</code> instead of <code>result != unsat</code>,
and both <code>sat</code> and <code>unsat</code> are truthy.  The code initially failed to flag a failure
when <code>z3</code> found a counter-example for our correctness condition!</p>

<h2 id="and-now-im-satisfied">And now I‚Äôm satisfied</h2>

<p>I have code to augment an arbitrary multiset or dictionary with
a running estimate of the mean and variance;
that code is based on a classic recurrence,
with some new math checked by hand,
with automated tests,
and with some exhaustive checking of small inputs (to which I claim most bugs can be reduced).</p>

<p>I‚Äôm now pretty sure the code works, but there‚Äôs another more obviously correct way to solve that update problem.
This <a href="https://prod-ng.sandia.gov/techlib-noauth/access-control.cgi/2008/086212.pdf">2008 report by Philippe P√©bay</a><sup id="fnref:pebay2016"><a href="#fn:pebay2016" class="footnote">1</a></sup>
presents formulas to compute the mean, variance, and arbitrary moments
in one pass,
and shows how to combine accumulators,
a useful operation in parallel computing.</p>

<p>We could use these formulas to <a href="http://blog.sigfpe.com/2010/11/statistical-fingertrees.html">augment an arbitrary \(k\)-ary tree</a>
and re-combine the merged accumulator as we go back up the (search)
tree from the modified leaf to the root.
The update would be much more stable (we only add and merge observations),
and incur logarithmic time overhead (with linear space overhead).
However, given the same time budget, and a <em>logarithmic</em> space overhead,
we could also implement the constant-time update with arbitrary precision
software floats, and probably guarantee even better precision.</p>

<p>The constant-time update I described in this post demanded more effort to convince myself
of its correctness, but I think it‚Äôs always a better option than
an augmented tree for serial code, especially if initial values
are available to populate the accumulators with batch-computed
mean and variance.
I‚Äôm pretty sure the code works, and <a href="https://gist.github.com/pkhuong/549106fc8194c0d1fce85b00c9e192d5">it‚Äôs up in this gist</a>.
I‚Äôll be re-implementing it in C++
because that‚Äôs the language used by the project that lead me to this problem;
feel free to steal that gist.</p>
<div class="footnotes">
  <ol>
    <li id="fn:pebay2016">
      <p>There‚Äôs also a <a href="https://www.osti.gov/biblio/1426900">2016 journal article by P√©bay and others</a> with numerical experiments, but I failed to implement their simpler-looking scalar update‚Ä¶¬†<a href="#fnref:pebay2016" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A couple of (probabilistic) worst-case bounds for Robin Hood linear probing]]></title>
    <link href="https://www.pvk.ca/Blog/2019/09/29/a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/"/>
    <updated>2019-09-29T15:44:08-04:00</updated>
    <id>https://www.pvk.ca/Blog/2019/09/29/a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing</id>
    <content type="html"><![CDATA[<p>I like to think of Robin Hood hash tables with linear probing as
arrays sorted on uniformly distributed keys, with gaps.  That makes it
clearer that we can use these tables to implement algorithms based on
merging sorted streams in bulk, as well as ones that rely on fast
point lookups.  A key question with randomised data structures is how
badly we can expect them to perform.</p>

<h2 id="a-bound-on-the-length-of-contiguous-runs-of-entries">A bound on the length of contiguous runs of entries</h2>

<p>There‚Äôs a lot of work on the <em>expected</em> time complexity of operations
on linear probing Robin Hood hash tables.  <a href="https://dblp.uni-trier.de/pers/hd/v/Viola:Alfredo">Alfredo Viola</a>,
along with a
<a href="http://algo.inria.fr/flajolet/Publications/FlPoVi98.pdf">few collaborators</a>,
has <a href="https://cs.uwaterloo.ca/research/tr/1995/50/CS95-50.pdf">long</a>
been exploring the <a href="http://www2.math.uu.se/~svante/papers/sj297-aofa.pdf">distribution of displacements</a> (i.e., search times)
for random elements.  The <a href="https://www3.cs.stonybrook.edu/~bender/newpub/BenderHu07-TODS.pdf">packed memory array</a> angle has also been around
for <a href="https://www.cs.cmu.edu/~jcreed/p251-willard.pdf">a while</a>.<sup id="fnref:waterloo"><a href="#fn:waterloo" class="footnote">1</a></sup></p>

<p>I‚Äôm a bit wary of the ‚Äúrandom element‚Äù aspect of the linear probing
bounds: while I‚Äôm comfortable with an expectation over the hash
function (i.e., over the uniformly distributed hash values), a program
could repeatedly ask for the same key, and consistently experience
worse-than-expected performance.  I‚Äôm more interested in bounding the
worst-case displacement (the distance between the ideal location for
an element, and where it is actually located) across all values in a
randomly generated<sup id="fnref:memory-less"><a href="#fn:memory-less" class="footnote">2</a></sup> Robin Hood table, with high enough
probability.  That probability doesn‚Äôt have to be extremely high:
\(p = 0.9\) or even \(p = 0.5\) is good enough, as long as we can
either rehash with an independent hash function, or the probability of
failure drops exponentially enough as the displacement leeway grows.</p>

<p>The people who study hashing with buckets, or hashing as load
balancing, seem more interested in these probable worst-case bounds:
as soon as one bucket overflows, it‚Äôs game over for that hash table!
In that context, we wish to determine how much headroom we must
reserve in each bucket, on top of the expected occupancy, in order to
make sure failures are rare enough.  That‚Äôs a <a href="https://en.wikipedia.org/wiki/Balls_into_bins_problem">balls into bins problem</a>, where
the \(m\) balls are entries in the hash table, and the \(n\) bins
its hash buckets.</p>

<p><a href="http://www.dblab.ntua.gr/~gtsat/collection/scheduling/Balls%20into%20Bins%20A%20Simple%20and%20Tight%20Anal.pdf">Raab and Steger‚Äôs Simple and Tight Analysis</a>
of the ‚Äúballs into bins‚Äù problem shows that the case where the average occupancy grows
with \((\log m)\sp{k}\) and \(k &gt; 1\) has potential, when it comes
to worst-case bounds that shrink quickly enough: we only need headroom
that grows with \(\sqrt{\log\sp{k+1} m} = \log\sp{(k+1)/2} m\), 
slightly more than the square root of the average occupancy.</p>

<p>The only issue is that the balls-into-bins analysis is asymptotic,
and, more importantly, doesn‚Äôt apply at all to linear probing!</p>

<p>One could propose a form of packed memory array, where the sorted set
is subdivided in chunks such that the expected load per chunk is in
\(\Theta(\log\sp{k} m)\), and the size of each chunk
multiplicatively larger (more than \(\log\sp{(k+1)/2}m\))‚Ä¶</p>

<p>Can we instead derive similar bounds with regular linear probing?  It
turns out that Raab and Steger‚Äôs bounds are indeed simple: they
find the probability of overflow for one bucket, and derive a
<a href="https://en.wikipedia.org/wiki/Boole%27s_inequality">union bound</a> for the probability of overflow for any bucket by
multiplying the single-bucket failure probability by the number of
buckets.  Moreover, the single-bucket case itself is a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a> confidence interval.</p>

<p>We can use the same approach for linear probing; I don‚Äôt expect a tight
result, but it might be useful.</p>

<p>Let‚Äôs say we want to determine how unlikely it is to observe a clump
of \(\log\sp{k}n\) entries, where \(n\) is the capacity of the hash
table.  We can bound the probability of observing such a clump
starting at index 0 in the backing array, and multiply by the size of
the array for our union bound (the clump could start anywhere in the array).</p>

<p>Given density \(d = m / n\), where \(m\) is the number of
entries and \(n\) is the size of the array that backs the hash
table, the probability that any given element falls in a range of size
\(\log\sp{k}n\) is \(p = d/n \log\sp{k}n\).  The number of entries
in such a range follows a Binomial distribution \(B(dn, p)\), with
expected value \(d \log\sp{k}n\).  We want to determine the maximum
density \(d\) such that 
\(\mathrm{Pr}[B(dn, p) &gt; \log\sp{k}n] &lt; \frac{\alpha}{dn}\), where
\(\alpha\) is our overall failure rate. If rehashing is acceptable,
we can let \(\alpha = 0.5\), and expect to find a
suitably uniform hash function after half a rehash on average.</p>

<p>We know we want \(k &gt; 1\) for the tail to shrink rapidly enough as
\(n\) grows, but even \(\log\sp{2}n\) doesn‚Äôt shrink very rapidly.
After some trial and error, I settled on a chunk size
\(s(n) = 5 \log\sb{2}\sp{3/2} n\).  That‚Äôs not great for small or medium sized
tables (e.g., \(s(1024) = 158.1\)), but grows slowly, and reflects
extreme worst cases; in practice, we can expect the worst case for
any table to be more reasonable.</p>

<p>Assuming we have a quantile function for the Binomial distribution, we
can find the occupancy of our chunk, at \(q = 1 - \frac{\alpha}{n}\). 
The occupancy is a monotonic function of the density, so we can use,
e.g., bisection search to find the maximum density such that the
probability that we saturate our chunk is \(\frac{\alpha}{n}\),
and thus the probability that any continuous run of entries has size
at least \(s(n) =  5 \log\sb{2}\sp{3/2} n\) is less than \(\alpha\).</p>

<p>For \(\alpha = 0.5\), the plot of densities looks like the following.</p>

<p><img class="center" src="/images/2019-09-29-a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/alpha-half.png" /></p>

<!-- sizer <- function(x) { 5 * (log(x, 2)^1.5) };
     max_overflow <- function(sizer, density, n, p) { n <- ceiling(n); run <- floor(sizer(n)); num_starts = max(1, 1 + n - run); qbinom(1 - (p / num_starts), n, density * run / n) / run }
     density <- sapply(n, function (n) uniroot(function (density) max_overflow(sizer, density, n, 0.5) - 1, c(0, 1))$root)
     ggplot(data=data.frame(n, density), aes(x = n, y = density)) + geom_line()
     -->

<p>This curve roughly matches the shape of some
<a href="https://www.pvk.ca/Blog/more_numerical_experiments_in_hashing.html">my older purely numerical experiments with Robin Hood hashing</a>.
When the table is small (less than \(\approx 1000\)), \(\log n\) is
a large fraction of \(n\), so the probability of finding a run of
size \(s(n) = 5 \log\sb{2}\sp{3/2} n\) is low.  When the table is
much larger, the asymptotic result kicks in, and the probabiliy slowly
shrinks.  However, even around the worst case \(n \approx 4500\),
we can exceed \(77\%\) density and only observe a run of length
\(s(n)\) half the time.</p>

<p>If we really don‚Äôt want to rehash, we can let \(\alpha = 10\sp{-10}\),
which compresses the curve and shifts it down: the minimum value is
now slightly above \(50\%\) density, and we can clearly see the
growth in permissible density as the size of the table grows.</p>

<!-- density <- sapply(n, function (n) uniroot(function (density) max_overflow(sizer, density, n, 1e-10) - 1, c(0, 1))$root)
     ggplot(data=data.frame(n, density), aes(x = n, y = density)) + geom_line()
     -->

<p><img class="center" src="/images/2019-09-29-a-couple-of-probabilistic-worst-case-bounds-for-robin-hood-linear-probing/alpha-marginal.png" /></p>

<p>In practice, we can dynamically compute the worst-case displacement,
which is always less than the longest run (i.e., less than \(s(n) = 5
\log\sb{2}\sp{3/2} n\)).  However, having non-asymptotic bounds lets
us write size-specialised code and know that its assumptions are
likely to be satisfied in real life.</p>

<h2 id="bounding-buffer-sizes-for-operations-on-sorted-hashed-streams">Bounding buffer sizes for operations on sorted hashed streams</h2>

<p>I mentioned at the beginning of this post that we can also manipulate
Robin Hood hash tables as sorted sets, where the sort keys are
uniformly distributed hash values.</p>

<p>Let‚Äôs say we wished to merge the immutable source table <code>S</code> into the
larger destination table <code>D</code> in-place, without copying all of <code>D</code>.
For example, from</p>

<pre><code>S = [2, 3, 4]
D = [1, 6, 7, 9, 10];
</code></pre>

<p>we want the merged result</p>

<pre><code>D' = [1, 2, 3, 4, 6, 7, 9, 10].
</code></pre>

<p>The issue here is that, even with gaps, we might have to overwrite
elements of <code>D</code>, and buffer them in some scratch space until we get to
their final position.  In this case, all three elements of <code>S</code> must be
inserted between the first and second elements of <code>D</code>, so we could
need to buffer <code>D[1:4]</code>.</p>

<p>How large of a merge buffer should we reasonably plan for?</p>

<p>In general, we might have to buffer as many elements as there are in
the smaller table of <code>S</code> and <code>D</code>.  However, we‚Äôre working with hash
values, so we can expect them to be distributed uniformly.  That
should give us some grip on the problem.</p>

<p>We can do even better and only assume that both sorted sets were
sampled from the same underlying distribution.  The key idea
is that the rank of an element in <code>S</code> is equal to the
value of <code>S</code>‚Äôs
<a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical distribution function</a>
for that element, multiplied by the size of <code>S</code> (similarly for
<code>D</code>).</p>

<p>The amount of buffering we might need is simply a measure of the
worst-case difference between the two empirical DFs: the more <code>S</code> get
ahead of <code>D</code>, the more we need to buffer values of <code>D</code> before
overwriting them (if we‚Äôre very unlucky, we might need a buffer the
same size as <code>S</code>).  That‚Äôs the
two-sample <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov%E2%80%93Smirnov_statistic">Kolmogorov-Smirnov statistic</a>, and we have 
<a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Two-sample_Kolmogorov%E2%80%93Smirnov_test">simple bounds for that distance</a>.</p>

<p>With probability \(1 - \alpha\), we‚Äôll consume from <code>S</code> and <code>D</code>
at the same rate \(\pm \sqrt{-\frac{(|S| + |D|) \ln \alpha}{2 |S| |D|}}\).
We can let \(\alpha = 10\sp{-10}\) and pre-allocate a buffer of size</p>

<table>
  <tbody>
    <tr>
      <td>\[</td>
      <td>S</td>
      <td>\sqrt{-\frac{(</td>
      <td>S</td>
      <td>+</td>
      <td>D</td>
      <td>) \ln \alpha}{2</td>
      <td>S</td>
      <td>¬†</td>
      <td>D</td>
      <td>}} &lt; \sqrt{\frac{23.03</td>
      <td>S</td>
      <td>(</td>
      <td>S</td>
      <td>+</td>
      <td>D</td>
      <td>)}{2</td>
      <td>D</td>
      <td>}}.\]</td>
    </tr>
  </tbody>
</table>

<p>In the worst case, \(|S| = |D|\), and we can preallocate a buffer of
size \(\sqrt{23.03 |D|} &lt; 4.8 \sqrt{|D|}\) and only need to grow
the buffer every ten billion (\(\alpha\sp{-1}\)) merge.</p>

<p>The same bound applies in a stream processing setting; I assume this
is closer to what <a href="https://twitter.com/frankmcsherry">Frank</a> had in
mind when he brought up this question.</p>

<p>Let‚Äôs assume a ‚Äúpush‚Äù dataflow model, where we still work on sorted
sets of uniformly distributed hash values (and the data tuples
associated with them), but now in streams that generate values every
tick.  The buffer size problem now sounds as follows.  We wish to
implement a sorted merge operator for two input streams that generate
one value every tick, and we can‚Äôt tell our sources to cease producing
values; how much buffer space might we need in order to merge them
correctly?</p>

<p>Again, we can go back to the Kolmogorov-Smirnov statistic.  In this
case however, we could buffer each stream independently, so we‚Äôre
looking for critical values for the one-sided one-sample
Kolmogorov-Smirnov test (how much one stream might get ahead of the
hypothetical exactly uniform stream).  We have recent (<a href="https://projecteuclid.org/euclid.aop/1176990746">1990</a>)
<a href="https://en.wikipedia.org/wiki/Dvoretzky%E2%80%93Kiefer%E2%80%93Wolfowitz_inequality">simple and tight bounds</a>
for this case as well.</p>

<p>The critical values for the one-sided case are stronger than the
two-sided two-sample critical values we used earlier: given an
overflow probability of \(1 - \alpha\), we need to buffer at most
\(\sqrt{-\frac{n \ln \alpha}{2}},\) elements.  For 
\(\alpha = 10\sp{-20}\) that‚Äôs less than \(4.8 \sqrt{n}\).<sup id="fnref:three-centuries"><a href="#fn:three-centuries" class="footnote">3</a></sup>
This square root scaling is pretty good news in practice: shrinking
\(n\) to \(\sqrt{n}\) tends to correspond to going down a rung or two in
the storage hierarchy.  For example, \(10\sp{15}\) elements is
clearly in the range of distributed storage; however, such a humongous
stream calls for a buffer of fewer than \(1.5 \cdot 10\sp{8}\)
elements, which, at a couple gigabytes, should fit in RAM on one large
machine.  Similarly, \(10\sp{10}\) elements might fill the RAM on
one machine, but the corresponding buffer of less than half a
million elements could fit in L3 cache, while one million elements
could fill the L3, and 4800 elements fit in L1 or L2.</p>

<p>What I find neat about this (probabilistic) bound on the buffer size
is its independence from the size of the other inputs to the merge
operator.  We can have a shared \(\Theta(\sqrt{n})\)-size buffer in
front of each stream, and do all our operations without worrying about
getting stuck (unless we‚Äôre extremely unlucky, in which case we can
grow the buffer a bit and resume or restart the computation).</p>

<p>Probably of more theoretical interest is the fact that these bounds do
not assume a uniform distribution, only that all the input streams are
identically and independently sampled from the same underlying
distribution. That‚Äôs the beauty of working in terms of the (inverse)
distribution functions.</p>

<h2 id="i-dont-think-theres-anything-deeper">I don‚Äôt think there‚Äôs anything deeper</h2>

<p>That‚Äôs it. Two cute tricks that use well-understood statistical
distributions in hashed data structure and algorithm design.  I doubt
there‚Äôs anything to generalise from either bounding approach.</p>

<p>However, I definitely believe they‚Äôre useful in practice. I like knowing
that I can expect the maximum displacement for a table of \(n\)
elements with Robin Hood linear probing to be less than \(5
\log\sb{2}^{3/2} n\), because that lets me select an appropriate
option for each table, as a function of that table‚Äôs maximum
displacement, while knowing the range of displacements I might have to
handle.  Having a strong bound on how much I might have to buffer for
stream join operators feels even more useful: I can pre-allocate a
single buffer per stream and not think about efficiently growing the
buffer, or signaling that a consumer is falling behind.  The
probability that I‚Äôll need a larger buffer is so low that I just need
to handle it, however inefficiently.  In a replicated system, where
each node picks an independent hash function, I would even consider
crashing when the buffer is too small!</p>
<div class="footnotes">
  <ol>
    <li id="fn:waterloo">
      <p>I swear the Waterloo theme isn‚Äôt some <a href="https://en.wikipedia.org/wiki/Canadian_content">CanCon</a> thing.¬†<a href="#fnref:waterloo" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:memory-less">
      <p>With consistent tie-breaking, the layout of entries in a Robin hood hash table is a function of the set of entries, independently of the sequence of add and remove operations.¬†<a href="#fnref:memory-less" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:three-centuries">
      <p>Assuming we consume ten streams per nanosecond, we expect to experience underbuffering once every 316 years.¬†<a href="#fnref:three-centuries" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fractional set covering with experts]]></title>
    <link href="https://www.pvk.ca/Blog/2019/04/23/fractional-set-covering-with-experts/"/>
    <updated>2019-04-23T18:05:09-04:00</updated>
    <id>https://www.pvk.ca/Blog/2019/04/23/fractional-set-covering-with-experts</id>
    <content type="html"><![CDATA[<p>Last winter break, I played with one of the annual
<a href="https://en.wikipedia.org/wiki/Vehicle_routing_problem">capacitated vehicle routing problem</a>
(CVRP) ‚ÄúSanta Claus‚Äù contests.  Real world family stuff
took precedence,  so, after the
obvious <a href="http://webhotel4.ruc.dk/~keld/research/LKH-3/">LKH</a>
with <a href="http://www.math.uwaterloo.ca/tsp/concorde.html">Concorde</a>
polishing for individual tours, I only had enough time for
one diversification moonshot.  I decided to treat the
high level problem of assembling prefabricated routes as
a <a href="https://en.wikipedia.org/wiki/Set_cover_problem">set covering problem</a>:
I would solve the linear programming (LP) relaxation for the
min-cost set cover, and use randomised rounding to feed new starting
points to LKH.  Add a lot of luck, and that might
just strike the right balance between solution quality and diversity.</p>

<p>Unsurprisingly, luck failed to show up, but I had ulterior motives:
I‚Äôm much more interested in exploring first order methods for
relaxations of combinatorial problems than in solving CVRPs.  The
routes I had accumulated after a couple days turned into a
<a href="https://archive.org/details/santa-cvrp-set-cover-instance">set covering LP with 1.1M decision variables, 10K constraints, and 20M nonzeros</a>.
That‚Äôs maybe denser than most combinatorial LPs (the aspect ratio
is definitely atypical), but 0.2% non-zeros is in the right ballpark.</p>

<p>As soon as I had that fractional set cover instance, I tried to solve
it with a simplex solver.  Like any good Googler, I used <a href="https://developers.google.com/optimization/lp/glop">Glop</a>‚Ä¶ and stared at a blank terminal for more than one hour.</p>

<p>Having observed that lack of progress, I implemented the toy I really
wanted to try out: first order online ‚Äúlearning with experts‚Äù
(specifically, <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a>) applied to
LP <em>optimisation</em>.  I let this <a href="https://gist.github.com/pkhuong/c508849180c6cf612f7335933a88ffa6">not-particularly-optimised serial CL code</a>
run on my 1.6 GHz laptop for 21 hours, at which point the first
order method had found a 4.5% infeasible solution (i.e., all the
constraints were satisfied with \(\ldots \geq 0.955\) instead of
\(\ldots \geq 1\)).  I left Glop running long after the contest was
over, and finally stopped it with no solution after more than 40 <em>days</em>
on my 2.9 GHz E5.</p>

<p>Given the shape of the constraint matrix, I would have loved to try an
interior point method, but all my licenses had expired, and I didn‚Äôt
want to risk OOMing my workstation.  <a href="https://twitter.com/e_d_andersen">Erling Andersen</a>
was later kind enough to test Mosek‚Äôs interior point solver on it.
The runtime was much more reasonable: 
<a href="https://twitter.com/e_d_andersen/status/1120579664806842368">10 minutes on 1 core, and 4 on 12 cores</a>, with the sublinear speed-up mostly caused by the serial
crossover to a simplex basis.</p>

<p>At 21 hours for a na√Øve implementation, the ‚Äúlearning with experts‚Äù
first order method isn‚Äôt practical yet, but also not obviously
uninteresting, so I‚Äôll write it up here.</p>

<p>Using online learning algorithms for the ‚Äúexperts problem‚Äù (e.g.,
<a href="https://cseweb.ucsd.edu/~yfreund/papers/adaboost.pdf">Freund and Schapire‚Äôs Hedge algorithm</a>)
to solve linear programming <em>feasibility</em> is now a classic result;
<a href="https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/">Jeremy Kun has a good explanation on his blog</a>.  What‚Äôs
new here is:</p>

<ol>
  <li>Directly solving the optimisation problem.</li>
  <li>Confirming that the parameter-free nature of <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a> helps.</li>
</ol>

<p>The first item is particularly important to me because it‚Äôs a simple
modification to the LP feasibility meta-algorithm, and might make the
difference between a tool that‚Äôs only suitable for theoretical
analysis and a practical approach.</p>

<p>I‚Äôll start by reviewing the experts problem, and how LP feasibility is
usually reduced to the former problem.  After that, I‚Äôll
cast the reduction as a <a href="https://smartech.gatech.edu/bitstream/handle/1853/24230/karwan_mark_h_197612_phd_154133.pdf">surrogate relaxation</a>
method, rather than a <a href="https://en.wikipedia.org/wiki/Lagrangian_relaxation">Lagrangian relaxation</a>;
optimisation should flow naturally from that
point of view.  Finally, I‚Äôll guess why I had more success
with <a href="https://arxiv.org/abs/1301.0534">AdaHedge</a> this time than with 
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">Multiplicative Weight Update</a>
eight years ago.<sup id="fnref:wall"><a href="#fn:wall" class="footnote">1</a></sup></p>

<h2 id="the-experts-problem-and-lp-feasibility">The experts problem and LP feasibility</h2>

<p>I first heard about the experts problem while researching
dynamic sorted set data structures:
<a href="https://dspace.mit.edu/handle/1721.1/10639">Igal Galperin‚Äôs PhD dissertation</a>
describes <a href="http://user.it.uu.se/~arnea/abs/partb.html">scapegoat trees</a>, but is really about online learning with
experts.
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">Arora, Hazan, and Kale‚Äôs 2012 survey of multiplicative weight update methods</a>.
is probably a better introduction to the topic ;)</p>

<p>The experts problem comes in many variations.  The simplest form
sounds like the following.  Assume you‚Äôre playing a binary prediction
game over a predetermined number of turns, and have access to a fixed
finite set of experts at each turn.  At the beginning of every turn,
each expert offers their binary prediction (e.g., yes it will rain
today, or it will not rain today).  You then have to make a prediction
yourself, with no additional input.  The actual result (e.g., it
didn‚Äôt rain today) is revealed at the end of the turn.  In general,
you can‚Äôt expect to be right more often than the best expert at the
end of the game.  Is there a strategy that bounds the ‚Äúregret,‚Äù how
many more wrong prediction you‚Äôll make compared to the expert(s) with
the highest number of correct predictions, and in what circumstances?</p>

<p>Amazingly enough, even with an omniscient adversary that has access to
your strategy and determines both the experts‚Äô predictions and the
actual result at the end of each turn, a stream of random bits (hidden
from the adversary) suffice to bound our expected regret in 
\(\mathcal{O}(\sqrt{T}\,\lg n)\), where \(T\) is the number of 
turns and \(n\) the number of experts.</p>

<p>I long had trouble with that claim: it just seems too good of a magic
trick to be true.  The key realisation for me was that we‚Äôre only
comparing against invidivual experts.  If each expert is a move in a
<a href="https://www.encyclopediaofmath.org/index.php/Matrix_game">matrix game</a>,
that‚Äôs the same as claiming you‚Äôll never do much worse than any pure
strategy.  One example of a pure strategy is always playing rock in 
Rock-Paper-Scissors; pure strategies are really bad!  The trick is
actually in making that regret bound useful.</p>

<p>We need a more continuous version of the experts problem for LP
feasibility.  We‚Äôre still playing a turn-based game, but, this time,
instead of outputting a prediction, we get to ‚Äúplay‚Äù a mixture of the
experts (with non-negative weights that sum to 1).  At the beginning
of each turn, we describe what weight we‚Äôd like to give to each
experts (e.g., 60% rock, 40% paper, 0% scissors).  The cost
(equivalently, payoff) for each expert is then revealed (e.g.,
\(\mathrm{rock} = -0.5\), \(\mathrm{paper} = 0.5\), 
\(\mathrm{scissors} = 0\)), and we incur the weighted average
from our play (e.g., \(60\% \cdot -0.5 + 40\% \cdot 0.5 = -0.1\))
before playing the next round.<sup id="fnref:equivalent"><a href="#fn:equivalent" class="footnote">2</a></sup>  The goal is to minimise
our worst-case regret, the additive difference between the total cost
incurred by our mixtures of experts and that of the a posteriori best single
expert.  In this case as well, online learning
algorithms guarantee regret in \(\mathcal{O}(\sqrt{T} \, \lg n)\)</p>

<p>This line of research is interesting because simple algorithms achieve
that bound, with explicit constant factors on the order of 1,<sup id="fnref:which-log"><a href="#fn:which-log" class="footnote">3</a></sup>
and <a href="http://drops.dagstuhl.de/opus/volltexte/2017/7499/pdf/LIPIcs-ICALP-2017-48.pdf">those bounds are known to be non-asymptotically tight for a large class of algorithms</a>.
Like dense linear algebra or fast Fourier transforms, where algorithms
are often compared by counting individual floating point operations,
online learning has matured into such tight bounds that worst-case
regret is routinely presented without Landau notation.  Advances improve
constant factors in the worst case, or adapt to easier inputs in order
to achieve ‚Äúbetter than worst case‚Äù performance.</p>

<p>The <a href="https://jeremykun.com/2017/02/27/the-reasonable-effectiveness-of-the-multiplicative-weights-update-algorithm/">reduction below</a>
lets us take any learning algorithm with an additive regret bound,
and convert it to an algorithm with a corresponding worst-case
iteration complexity bound for \(\varepsilon\)-approximate LP feasibility.
An algorithm that promises low worst-case regret in \(\mathcal{O}(\sqrt{T})\)
gives us an algorithm that needs at most \(\mathcal{O}(1/\varepsilon\sp{2})\)
iterations to return a solution that almost satisfies every constraint in the
linear program, where each constraint is violated by \(\varepsilon\) or less (e.g.,
\(x \leq 1\) is actually \(x \leq 1 + \varepsilon\)).</p>

<p>We first split the linear program in two components, a simple domain
(e.g., the non-negative orthant or the \([0, 1]\sp{d}\) box) and the
actual linear constraints.  We then map each of the latter constraints
to an expert, and use an arbitrary algorithm that solves our
continuous version of the experts problem as a black box.  At each
turn, the black box will output a set of non-negative weights for the
constraints (experts).  We will average the constraints using these
weights, and attempt to find a solution in the intersection of our
simple domain and the weighted average of the linear constraints.  We
can do so in the ‚Äúexperts problem‚Äù setting by consider each linear
constraint‚Äôs violation as a <em>payoff</em>, or, equivalently, satisfaction
as a loss.</p>

<p>Let‚Äôs use Stigler‚Äôs <a href="https://neos-guide.org/content/diet-problem">Diet Problem with three foods and two constraints</a>
as a small example, and further simplify it by disregarding the
minimum value for calories, and the maximum value for vitamin A.  Our
simple domain here is at least the non-negative orthant: we can‚Äôt
ingest negative food.  We‚Äôll make things more interesting by also
making sure we don‚Äôt eat more than 10 servings of any food per day.</p>

<p>The first constraint says we mustn‚Äôt get too many calories</p>

<p>\[72 x\sb{\mathrm{corn}} + 121 x\sb{\mathrm{milk}} + 65 x\sb{\mathrm{bread}} \leq 2250,\]</p>

<p>and the second constraint (tweaked to improve this example) ensures
we ge enough vitamin A</p>

<p>\[107 x\sb{\mathrm{corn}} + 400 x\sb{\mathrm{milk}} \geq 5000,\]</p>

<p>or, equivalently,</p>

<p>\[-107 x\sb{\mathrm{corn}} - 400 x\sb{\mathrm{milk}} \leq -5000,\]</p>

<p>Given weights \([3/4, 1/4]\), the weighted average of the two constraints is</p>

<p>\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]</p>

<p>where the coefficients for each variable and for the right-hand side
were averaged independently.</p>

<p>The subproblem asks us to find a feasible point in the intersection
of these two constraints:
\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Classically, we claim that this is just Lagrangian relaxation, and
find a solution to</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>In the next section, I‚Äôll explain why I think this analogy is wrong
and worse than useless.  For now, we can easily find the minimum one
variable at a time, and find the solution 
\(x\sb{\mathrm{corn}} = 0\), \(x\sb{\mathrm{milk}} = 10\),
\(x\sb{\mathrm{bread}} = 0\), with objective value \(-92.5\) (which
is \(530\) less than \(437.5\)).</p>

<p>In general, three things can happen at this point.  We could discover
that the subproblem is infeasible.  In that case, the original
non-relaxed linear program itself is infeasible: any solution to the
original LP satisfies all of its constraints, and thus would also
satisfy any weighted average of the same constraints.  We could also
be extremely lucky and find that our optimal solution to the relaxation is
(\(\varepsilon\)-)feasible for the original linear program; we can stop
with a solution.  More commonly, we have a solution that‚Äôs feasible for the
relaxation, but not for the original linear program.</p>

<p>Since that solution satisfies the weighted average constraint and
payoffs track constraint violation, the black box‚Äôs payoff for this
turn (and for every other turn) is non-positive.  In the current case,
the first constraint (on calories) is satisfied by \(1040\), while
the second (on vitamin A) is violated by \(1000\).  On weighted
average, the constraints are satisfied by \(\frac{1}{4}(3 \cdot
1040 - 1000) = 530.\) Equivalently, they‚Äôre violated by \(-530\) on
average.</p>

<p>We‚Äôll add that solution to an accumulator vector that will come in
handy later.</p>

<p>The next step is the key to the reduction: we‚Äôll derive payoffs
(negative costs) for the black box from the solution to the last
relaxation.  Each constraint (expert) has a payoff equal to its level
of violation in the relaxation‚Äôs solution.  If a constraint is
strictly satisfied, the payoff is negative; for example, the constraint
on calories is satisfied by \(1040\), so its payoff this turn is
\(-1040\).  The constraint on vitamin A is violated by \(1000\),
so its payoff this turn is \(1000\).  Next turn, we expect the
black box to decrease the weight of the constraint on calories,
and to increase the weight of the one on vitamin A.</p>

<p>After \(T\) turns, the total payoff for each constraint is equal to
the sum of violations by all solutions in the accumulator.  Once we
divide both sides by \(T\), we find that the divided payoff for each
constraint is equal to its violation by the average of the solutions
in the accumulator.  For example, if we have two solutions, one that
violates the calories constraint by \(500\) and another that
satisfies it by \(1000\) (violates it by \(-1000\)), the total
payoff for the calories constraint is \(-500\), and the average
of the two solutions does strictly satisfy the linear constraint by
\(\frac{500}{2} = 250\)!</p>

<p>We also know that we only generated feasible solutions to the relaxed
subproblem (otherwise, we‚Äôd have stopped and marked the original LP as
infeasible), so the black box‚Äôs total payoff is \(0\) or negative.</p>

<p>Finally, we assumed that the black box algorithm guarantees an additive
regret in \(\mathcal{O}(\sqrt{T}\, \lg n)\), so the black box‚Äôs payoff
of (at most) \(0\) means that any constraint‚Äôs payoff is at most
\(\mathcal{O}(\sqrt{T}\, \lg n)\).  After dividing by \(T\), we obtain
a bound on the violation by the arithmetic mean of all solutions in
the accumulator: for all constraint, that violation is in 
\(\mathcal{O}\left(\frac{\lg n}{\sqrt{T}}\right)\).  In other words, the number
of iteration \(T\) must scale with
\(\mathcal{O}\left(\frac{\lg n}{\varepsilon\sp{2}}\right)\), 
which isn‚Äôt bad when \(n\) is in the millions but
\(\varepsilon \approx 0.01\).</p>

<p>Theoreticians find this reduction interesting because there are
concrete implementations of the black box, e.g., the
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">multiplicative weight update (MWU) method</a>
with non-asymptotic bounds.  For many problems, this makes it
possible to derive the exact number of iterations necessary
to find an \(\varepsilon-\)feasible fractional solution, given
\(\varepsilon\) and the instance‚Äôs size (but not the instance
itself).</p>

<p>That‚Äôs why algorithms like MWU are theoretically useful tools for
fractional approximations, when we already have subgradient methods
that only need \(\mathcal{O}\left(\frac{1}{\varepsilon}\right)\) iterations:
state-of-the-art algorithms for learning with experts explicit
non-asymptotic regret bounds that yield, for many problems, iteration
bounds that only depend on the instance‚Äôs size, but not its data.
While the iteration count when solving LP feasibility with MWU scales
with \(\frac{1}{\varepsilon\sp{2}}\), it is merely proportional to
\(\lg n\), the log of the the number of linear constraints.  That‚Äôs
attractive, compared to subgradient methods for which the iteration
count scales with \(\frac{1}{\varepsilon}\), but also scales
linearly with respect to instance-dependent values like the distance
between the initial dual solution and the optimum, or the Lipschitz
constant of the Lagrangian dual function; these values are hard to
bound, and are often proportional to the square root of the number
of constraints.  Given the choice between
\(\mathcal{O}\left(\frac{\lg n}{\varepsilon\sp{2}}\right)\) 
iterations with explicit constants, and a looser
\(\mathcal{O}\left(\frac{\sqrt{n}}{\varepsilon}\right)\), it‚Äôs 
obvious why MWU and online learning are powerful additions to 
the theory toolbox.</p>

<p>Theoreticians are otherwise not concerned with efficiency, so the
usual answer to someone asking about optimisation is to tell them they
can always reduce linear optimisation to feasibility with a binary
search on the objective value.  I once made the mistake of
implementing that binary search last strategy.  Unsurprisingly, it
wasn‚Äôt useful.  I also tried another theoretical reduction, where I
looked for a pair of primal and dual -feasible solutions that happened
to have the same objective value.  That also failed, in a more
interesting manner: since the two solution had to have almost the same
value, the universe spited me by sending back solutions that were
primal and dual infeasible in the worst possible way.  In the end, the
second reduction generated fractional solutions that were neither
feasible nor superoptimal, which really isn‚Äôt helpful.</p>

<h2 id="direct-linear-optimisation-with-experts">Direct linear optimisation with experts</h2>

<p>The reduction above works for any ‚Äúsimple‚Äù domain, as long as it‚Äôs
convex and we can solve the subproblems, i.e., find a point in the
intersection of the simple domain and a single linear constraint or
determine that the intersection is empty.</p>

<p>The set of (super)optimal points in some initial simple domain is
still convex, so we could restrict our search to the search of the
domain that is superoptimal for the linear program we wish to
optimise, and directly reduce optimisation to the feasibility problem
solved in the last section, without binary search.</p>

<p>That sounds silly at first: how can we find solutions that are
superoptimal when we don‚Äôt even know the optimal value?</p>

<p>Remember that the subproblems are always relaxations of the original
linear program.  We can port the objective function from the original
LP over to the subproblems, and optimise the relaxations.  Any
solution that‚Äôs optimal for a realxation must have an optimal or
superoptimal value for the original LP.</p>

<p>Rather than treating the black box online solver as a generator of 
<a href="https://en.wikipedia.org/wiki/Duality_\(optimization\)#The_strong_Lagrangian_principle:_Lagrange_duality">Lagrangian dual</a>
vectors, we‚Äôre using its weights as solutions to the
<a href="https://smartech.gatech.edu/bitstream/handle/1853/24230/karwan_mark_h_197612_phd_154133.pdf"><em>surrogate</em> relaxation dual</a>.
The latter interpretation isn‚Äôt just more powerful by handling
objective functions.  It also makes more sense: the weights generated
by algorithms for the experts problem are probabilities, i.e., they‚Äôre
non-negative and sum to \(1\).  That‚Äôs also what‚Äôs expected for surrogate
dual vectors, but definitely not the case for Lagrangian dual vectors,
even when restricted to \(\leq\) constraints.</p>

<p>We can do even better!</p>

<p>Unlike Lagrangian dual solvers, which only converge when fed
(approximate) subgradients and thus make us (nearly) optimal solutions
to the relaxed subproblems, our reduction to the experts problem only
needs feasible solutions to the subproblems.  That‚Äôs all we need to
guarantee an \(\varepsilon-\)feasible solution to the initial problem
in a bounded number of iterations.  We also know exactly how that
\(\varepsilon-\)feasible solution is generated: it‚Äôs the arithmetic
mean of the solutions for relaxed subproblems.</p>

<p>This lets us decouple finding lower bounds from generating feasible
solutions that will, on average, \(\varepsilon-\)satisfy the
original LP.  In practice, the search for an \(\varepsilon-\)feasible
solution that is also superoptimal will tend to improve the lower
bound.  However, nothing forces us to evaluate lower bounds
synchronously, or to only use the experts problem solver to improve
our bounds.</p>

<p>We can find a new bound from any vector of non-negative constraint
weights: they always yield a valid surrogate relaxation.  We can solve
that relaxation, and update our best bound when it‚Äôs improved.  The
Diet subproblem earlier had</p>

<p>\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Adding the original objective function back yields the linear program</p>

<p>\[\min 0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\]
subject to
\[27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}} \leq 437.5,\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10,\]</p>

<p>which has a trivial optimal solution at \([0, 0, 0]\).</p>

<p>When we generate a feasible solution for the same subproblem, we can
use any valid bound on the objective value to find the most feasible
solution that is also assuredly (super)optimal.  For example, if some
oracle has given us a lower bound of \(2\) for the original Diet
problem, we can solve for</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\leq 2\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>We can relax the objective value constraint further, since we know
that the final \(\varepsilon-\)feasible solution is a simple
arithmetic mean.  Given the same best bound of \(2\), and, e.g., a
current average of \(3\) solutions with a value of \(1.9\), a new
solution with an objective value of \(2.3\) (more than our best
bound, so not necessarily optimal!) would yield a new average solution
with a value of \(2\), which is still (super)optimal.  This means
we can solve the more relaxed subproblem</p>

<p>\[\min 27.25 x\sb{\mathrm{corn}} - 9.25 x\sb{\mathrm{milk}} + 48.75 x\sb{\mathrm{bread}}\]
subject to
\[0.18 x\sb{\mathrm{corn}} + 0.23 x\sb{\mathrm{milk}} + 0.05 x\sb{\mathrm{bread}}\leq 2.3\]
\[0 \leq x\sb{\mathrm{corn}},\, x\sb{\mathrm{milk}},\, x\sb{\mathrm{bread}} \leq 10.\]</p>

<p>Given a bound on the objective value, we swapped the constraint and
the objective; the goal is to maximise feasibility, while generating a
solution that‚Äôs ‚Äúgood enough‚Äù to guarantee that the average solution
is still (super)optimal.</p>

<p>For box-constrained linear programs where the box is the convex
domain, subproblems are bounded linear knapsacks, so we can simply
stop the greedy algorithm as soon as the objective value constraint is
satisfied, or when the knapsack constraint becomes active (we found a
better bound).</p>

<p>This last tweak doesn‚Äôt just accelerate convergence to
\(\varepsilon-\)feasible solutions.  More importantly for me, it
pretty much guarantees that out \(\varepsilon-\)feasible solution
matches the best known lower bound, even if that bound was provided by
an outside oracle.  <a href="http://www.inrialpes.fr/bipop/people/malick/Docs/05-frangioni.pdf">Bundle methods</a>
and the <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.8194&amp;rep=rep1&amp;type=pdf">Volume algorithm</a>
can also mix solutions to relaxed subproblems in order to generate
\(\varepsilon-\)feasible solutions, but the result lacks the last
guarantee: their fractional solutions are even more superoptimal
than the best bound, and that can make bounding and variable fixing
difficult.</p>

<h2 id="the-secret-sauce-adahedge">The secret sauce: AdaHedge</h2>

<p>Before last Christmas‚Äôs CVRP set covering LP, I had always used the 
<a href="https://www.satyenkale.com/papers/mw-survey.pdf">multiplicative weight update (MWU) algorithm</a>
as my black box online learning algorithm:  it wasn‚Äôt great, but I
couldn‚Äôt find anything better. The two main downsides for me
were that I had to know a ‚Äúwidth‚Äù parameter ahead of time, as well
as the number of iterations I wanted to run.</p>

<p>The width is essentially the range of the payoffs; in our case, the
potential level of violation or satisfaction of each constraints by
any solution to the relaxed subproblems.  The dependence isn‚Äôt
surprising: folklore in Lagrangian relaxation also says that‚Äôs a big
factor there.  The problem is that the most extreme violations and
satisfactions are initialisation parameters for the MWU algorithm,
and the iteration count for a given \(\varepsilon\) is quadratic
in the width (\(\mathrm{max}\sb{violation} \cdot \mathrm{max}\sb{satisfaction}\)).</p>

<p>What‚Äôs even worse is that the MWU is explicitly tuned for a specific
iteration count.  If I estimate that, give my worst-case width estimate,
one million iterations will be necessary to achieve \(\varepsilon-\)feasibility,
MWU tuned for 1M iterations will need 1M iterations, even if the actual
width is narrower.</p>

<p><a href="https://arxiv.org/abs/1301.0534">de Rooij and others published AdaHedge in 2013</a>,
an algorithm that addresses both these issues by smoothly estimating
its parameter over time, without using the doubling trick.<sup id="fnref:doubling"><a href="#fn:doubling" class="footnote">4</a></sup>
AdaHedge‚Äôs loss (convergence rate to an \(\varepsilon-\)solution)
still depends on the relaxation‚Äôs width.  However, it depends on the
maximum width actually observed during the solution process, and not
on any explicit worst-case bound.  It‚Äôs also not explicily tuned for a
specific iteration count, and simply keeps improving at a rate that
roughly matches MWU.  If the instance happens to be easy, we will find
an \(\varepsilon-\)feasible solution more quickly.  In the worst
case, the iteration count is never much worse than that of an
optimally tuned MWU.</p>

<p>These <a href="https://gist.github.com/pkhuong/c508849180c6cf612f7335933a88ffa6">400 lines of Common Lisp</a>
implement AdaHedge and use it to optimise the set covering LP.  AdaHedge acts
as the online black box solver for the surrogate dual problem, the relaxed
set covering LP is a linear knapsack, and each subproblem attempts
to improve the lower bound before maximising feasibility.</p>

<p>When I ran the code, I had no idea how long it would take to find a
feasible enough solution: covering constraints can never be violated
by more than \(1\), but some points could be covered by hundreds of
tours, so the worst case satisfaction width is high. I had to rely on
the way AdaHedge adapts to the actual hardness of the problem.  In the
end, \(34492\) iterations sufficed to find a solution that was \(4.5\%\)
infeasible.<sup id="fnref:wrong-log"><a href="#fn:wrong-log" class="footnote">5</a></sup>  This corresponds to a worst case with a width
of less than \(2\), which is probably not what happened.  It seems
more likely that the surrogate dual isn‚Äôt actually an omniscient
adversary, and AdaHedge was able to exploit some of that ‚Äúeasiness.‚Äù</p>

<p>The iterations themselves are also reasonable: one sparse matrix /
dense vector multiplication to convert surrogate dual weights to an
average constraint, one solve of the relaxed LP, and another sparse
matrix / dense vector multiplication to compute violations for each
constraint.  The relaxed LP is a fractional \([0, 1]\) knapsack, so
the bottleneck is sorting double floats.  Each iteration took 1.8
seconds on my old laptop; I‚Äôm guessing that could easily be 10-20
times faster with vectorisation and parallelisation.</p>

<p>In another post, I‚Äôll show how using the same surrogate dual optimisation
algorithm to mimick <a href="https://link.springer.com/article/10.1007/BF02592954">Lagrangian decomposition</a>
<a href="https://perso.ensta-paristech.fr/~diam/ro/online/Monique.Guignard-top11201.pdf">instead of Lagrangian relaxation</a>
guarantees an iteration count in \(\mathcal{O}\left(\frac{\lg \#\mathrm{nonzero}}{\varepsilon\sp{2}}\right)\) independently of luck or the specific linear constraints.</p>
<div class="footnotes">
  <ol>
    <li id="fn:wall">
      <p>Yes, I have been banging my head against that wall for a while.¬†<a href="#fnref:wall" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:equivalent">
      <p>This is equivalent to minimising expected loss with random bits, but cleans up the reduction.¬†<a href="#fnref:equivalent" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:which-log">
      <p>When was the last time you had to worry whether that log was natural or base-2?¬†<a href="#fnref:which-log" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:doubling">
      <p>The doubling trick essentially says to start with an estimate for some parameters (e.g., width), then adjust it to at least double the expected iteration count when the parameter‚Äôs actual value exceeds the estimate. The sum telescopes and we only pay a constant multiplicative overhead for the dynamic update.¬†<a href="#fnref:doubling" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:wrong-log">
      <p>I think I computed the \(\log\) of the number of decision variables instead of the number of constraints, so maybe this could have gone a bit better.¬†<a href="#fnref:wrong-log" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The unscalable, deadlock-prone, thread pool]]></title>
    <link href="https://www.pvk.ca/Blog/2019/02/25/the-unscalable-thread-pool/"/>
    <updated>2019-02-25T21:27:06-05:00</updated>
    <id>https://www.pvk.ca/Blog/2019/02/25/the-unscalable-thread-pool</id>
    <content type="html"><![CDATA[<p><small>Epistemic Status: I‚Äôve seen thread pools fail this way multiple
times, am confident the pool-per-state approach is an improvement, and
have confirmed with others they‚Äôve also successfully used it in anger.
While I‚Äôve thought about this issue several times over ~4 years and
pool-per-state seems like a good fix, I‚Äôm not convinced it‚Äôs
undominated and hope to hear about better approaches.</small></p>

<p>Thread pools tend to only offer a sparse interface: 
<a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.Executor.submit">pass a closure or a function and its arguments to the pool</a>,
<a href="https://github.com/silentbicycle/loom/blob/master/loom.h#L46">and that function</a>
<a href="https://github.com/lmj/lparallel/blob/9c11f40018155a472c540b63684049acc9b36e15/src/kernel/core.lisp#L374">will be called, eventually</a>.<sup id="fnref:convenience"><a href="#fn:convenience" class="footnote">1</a></sup>  Functions can do 
anything, so this interface should offer all the expressive power one
could need. Experience tells me otherwise.</p>

<p>The standard pool interface is so impoverished that it is nearly
impossible to use correctly in complex programs, and leads us down
design dead-ends. I would actually argue it‚Äôs better to work with raw
threads than to even have <del>generic</del> amorphous thread pools: the former force us
to stop and think about resource requirements (and lets the OS‚Äôs real
scheduler help us along), instead of making us pretend we only care
about CPU usage. I claim thread pools aren‚Äôt scalable because, with
the exception of CPU time, they actively hinder the development of
programs that achieve high resource utilisation.</p>

<p>This post comes in two parts. First, the story of a simple program
that‚Äôs parallelised with a thread pool, then hits a wall as a wider set
of resources becomes scarce. Second, a solution I like for that kind of
program: an explicit state machine, where each state gets a dedicated
queue that is aware of the state‚Äôs resource requirements.</p>

<h2 id="stages-of-parallelisation">Stages of parallelisation</h2>

<p>We start with a simple program that processes independent work units,
a serial loop that pulls in work (e.g., files in a directory), or wait
for requests on a socket, one work unit at a time.</p>

<p>At some point, there‚Äôs enough work to think about parallelisation, and
we choose threads.<sup id="fnref:processes"><a href="#fn:processes" class="footnote">2</a></sup> To keep things simple, we simply spawn
a thread per work unit. Load increases further, and we observe that
we spend more time switching between threads or contending on shared
data than doing actual work. We could use a semaphore to limit the
number of work units we process concurrently, but we might as well
just push work units to a thread pool and recycle threads instead of
wasting resources on a thread-per-request model.  We can even start
thinking about queueing disciplines, admission control, backpressure,
etc.  Experienced developers will often jump directly to this stage
after the serial loop.</p>

<p>The 80s saw a lot of research on generalising this ‚Äúflat‚Äù parallelism
model to nested parallelism, where work units can spawn additional
requests and wait for the results (e.g., to recursively explore
sub-branches of a search tree).  Nested parallelism seems like a good
fit for contemporary network services: we often respond to a request
by sending simpler requests downstream, before merging and munging the
responses and sending the result back to the original requestor. That
may be why futures and promises are so popular these days.</p>

<p>I believe that, for most programs, the futures model is an excellent
answer to the wrong question. The moment we perform I/O (be it
network, disk, or even with hardware accelerators) in order to
generate a result, running at scale will have to mean controlling
more resources than just CPU, and both the futures and the generic
thread pool models fall short.</p>

<p>The issue is that futures only work well when a waiter can help along
the value it needs, with task stealing, while thread pools implement a
trivial scheduler (dedicate a thread to a function until that function
returns) that must be oblivious to resource requirements, since it
handles opaque functions.</p>

<p>Once we have futures that might be blocked on I/O, we can‚Äôt
guarantee a waiter will achieve anything by lending CPU time to its
children.  We could help sibling tasks, but that way stack overflows
lie.</p>

<p>The deficiency of flat generic thread pools is more subtle. Obviously, one
doesn‚Äôt want to take a tight thread pool, with one thread per core,
and waste it on synchronous I/O. We‚Äôll simply kick off I/O
asynchronously, and re-enqueue the continuation on the pool upon
completion!</p>

<p>Instead of doing</p>

<pre><code>A, I/O, B
</code></pre>

<p>in one function, we‚Äôll split the work in two functions and a callback</p>

<pre><code>A, initiate asynchronous I/O
On I/O completion: enqueue B in thread pool
B
</code></pre>

<p>The problem here is that it‚Äôs easy to create too many asynchronous
requests, and run out of memory, DOS the target, or delay the rest of
the computation for too long.  As soon as the I/O requests has been
initiated in <code>A</code>, the function returns to the thread pool, which will
just execute more instances of <code>A</code> and initiate even more I/O.</p>

<p>At first, when the program doesn‚Äôt heavily utilise any resource in
particular, there‚Äôs an easy solution: limit the total number of
in-flight work units with a semaphore. Note that I wrote work unit,
not function calls. We want to track logical requests that we started
processing, but for which there is still work to do (e.g., the
response hasn‚Äôt been sent back yet).</p>

<p>I‚Äôve seen two ways to cap in-flight work units. One‚Äôs buggy, the other
doesn‚Äôt generalise.</p>

<p>The buggy implementation acquires a semaphore in the first stage of
request handling (<code>A</code>) and releases it in the last stage (<code>B</code>). The
bug is that, by the time we‚Äôre executing <code>A</code>, we‚Äôre already using up a
slot in the thread pool, so we might be preventing <code>B</code>s from
executing.  We have a lock ordering problem: <code>A</code> acquires a thread
pool slot before acquiring the in-flight semaphore, but <code>B</code> needs to
acquire a slot before releasing the same semaphore. If you‚Äôve seen
code that deadlocks when the thread pool is too small, this was
probably part of the problem.</p>

<p>The correct implementation acquires the semaphore before enqueueing a
new work unit, before shipping a call to <code>A</code> to the thread pool (and
releases it at the end of processing, in <code>B</code>). This only works because
we can assume that the first thing <code>A</code> does is to acquire the
semaphore. As our code becomes more efficient, we‚Äôll want to more
finely track the utilisation of multiple resources, and
pre-acquisition won‚Äôt suffice. For example, we might want to limit
network requests going to individual hosts, independently from disk
reads or writes, or from database transactions.</p>

<h2 id="resource-aware-thread-pools">Resource-aware thread pools</h2>

<p>The core issue with thread pools is that the only thing they can do is
run opaque functions in a dedicated thread, so the only way to reserve
resources is to already be running in a dedicated thread. However, the
one resource that every function needs is a thread on which to run, thus
any correct lock order must acquire the thread last.</p>

<p>We care about reserving resources because, as our code becomes more
efficient and scales up, it will start saturating resources that used
to be virtually infinite.  Unfortunately, classical thread pools can
only control CPU usage, and actively hinder correct resource
throttling.  If we can‚Äôt guarantee we won‚Äôt overwhelm the supply of a
given resource (e.g., read IOPS), we must accept wasteful
overprovisioning.</p>

<p>Once the problem has been identified, the solution becomes obvious:
make sure the work we push to thread pools describes the resources
to acquire before running the code in a dedicated thread.</p>

<p>My favourite approach assigns one global thread pool (queue) to each
function or processing step. The arguments to the functions will
change, but the code is always the same, so the resource requirements
are also well understood. This does mean that we incur complexity to
decide how many threads or cores each pool is allowed to use. However,
I find that the resulting programs are better understandable at a high
level: it‚Äôs much easier to write code that traverses and describes the
work waiting at different stages when each stage has a dedicated
thread pool queue.  They‚Äôre also easier to model as queueing systems,
which helps answer ‚Äúwhat if?‚Äù questions without actually implementing
the hypothesis.</p>

<p>In increasing order of annoyingness, I‚Äôd divide resources to acquire in
four classes.</p>

<ol>
  <li>Resources that may be seamlessly<sup id="fnref:thrashing"><a href="#fn:thrashing" class="footnote">3</a></sup> shared or timesliced, like CPU.</li>
  <li>Resources that are acquired for the duration of a single function
call or processing step, like DB connections.</li>
  <li>Resources that are acquired in one function call, then released in
another thread pool invocation, like DB transactions, or asynchronous
I/O semaphores.</li>
  <li>Resources that may only be released after temporarily using more of
it, or by cancelling work: memory.</li>
</ol>

<p>We don‚Äôt really have to think about the first class of resources, at
least when it comes to correctness. However, repeatedly running the
same code on a given core tends to improve performance, compared to
running all sorts of code on all cores.</p>

<p>The second class of resources may be acquired once our code is running
in a thread pool, so one could pretend it doesn‚Äôt exist. However, it
is more efficient to batch acquisition, and execute a bunch of calls
that all need a given resource (e.g., a DB connection from a
connection pool) before releasing it, instead of repetitively
acquiring and releasing the same resource in back-to-back function
calls, or blocking multiple workers on the same
bottleneck.<sup id="fnref:SEDA"><a href="#fn:SEDA" class="footnote">4</a></sup> More importantly, the property of always being
acquired and released in the same function invocation, is a global
one: as soon as even one piece of code acquires a given resource and
releases in another thread pool call (e.g., acquires a DB connection,
initiates an asynchronous network call, writes the result of the call
to the DB, and releases the connection), we must always treat that
resource as being in the third, more annoying, class.  Having explicit
stages with fixed resource requirements helps us confirm resources
are classified correctly.</p>

<p>The third class of resources <em>must</em> be acquired in a way that
preserves forward progress in the rest of the system. In particular,
we must never have all workers waiting for resources of this third
class. In most cases, it suffices to make sure there at least as many
workers as there are queues or stages, and to only let each stage run
the initial resource acquisition code in one worker at a time.
However, it can pay off to be smart when different queued items
require different resources, instead of always trying to satisfy
resource requirements in FIFO order.</p>

<p>The fourth class of resources is essentially heap memory. Memory is
special because the only way to release it is often to complete the
computation. However, moving the computation forward will use even
more heap. In general, my only solution is to impose a hard cap on the
total number of in-flight work units, and to make sure it‚Äôs easy to
tweak that limit at runtime, in disaster scenarios.  If we still run
close to the memory capacity with that limit, the code can either
crash (and perhaps restart with a lower in-flight cap), or try to
cancel work that‚Äôs already in progress. Neither option is very
appealing.</p>

<p>There are some easier cases. For example, I find that temporary bumps
in heap usage can be caused by parsing large responses from
idempotent (<code>GET</code>) requests.  It would be nice if networking subsystems
tracked memory usage to dynamically throttle requests, or
even cancel and retry idempotent ones.</p>

<p>Once we‚Äôve done the work of explicitly writing out the processing
steps in our program as well as their individual resource
requirements, it makes sense to let that topology drive the structure
of the code.</p>

<p>Over time, we‚Äôll gain more confidence in that topology and bake it in
our program to improve performance.  For example, rather than limiting
the number of in-flight requests with a semaphore, we can have a
fixed-size allocation pool of request objects.  We can also
selectively use bounded ring buffers once we know we wish to impose a
limit on queue size.  Similarly, when a sequence (or subgraph) of
processing steps is fully synchronous or retires in order, we can
control both the queue size and the number of in-flight work units
with a <a href="https://lmax-exchange.github.io/disruptor/">disruptor</a>, which
should also improve locality and throughput under load.  These
transformations are easy to apply once we know what the movement of
data and resource looks like.  However, they also ossify the structure
of the program, so I only think about such improvements if they
provide a system property I know I need (e.g., a limit on the number
of in-flight requests), or once the code is functional and we have
load-testing data.</p>

<p>Complex programs are often best understood as state machines. These
state machines can be implicit, or explicit. I prefer the latter.  I
claim that it‚Äôs also preferable to have one thread pool<sup id="fnref:queue"><a href="#fn:queue" class="footnote">5</a></sup> per
explicit state than to dump all sorts of state transition logic
in a shared pool.  If writing functions that process flat tables is
data-oriented programming, I suppose I‚Äôm arguing for data-oriented
state machines.</p>

<div class="footnotes">
  <ol>
    <li id="fn:convenience">
      <p>Convenience wrappers, like parallel map, or ‚Äúrun after this time,‚Äù still rely on the flexibility of opaque functions.¬†<a href="#fnref:convenience" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:processes">
      <p>Maybe we decided to use threads because there‚Äôs a lot of shared, read-mostly, data on the heap. It doesn‚Äôt really matter, process pools have similar problems.¬†<a href="#fnref:processes" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:thrashing">
      <p>Up to a point, of course. No model is perfect, etc. etc.¬†<a href="#fnref:thrashing" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:SEDA">
      <p>Explicit resource requirements combined with one queue per stage lets us steal ideas from <a href="https://en.wikipedia.org/wiki/Staged_event-driven_architecture">SEDA</a>.¬†<a href="#fnref:SEDA" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:queue">
      <p>One thread pool per state in the sense that no state can fully starve out another of CPU time. The concrete implementation may definitely let a shared set of workers pull from all the queues.¬†<a href="#fnref:queue" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
