<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2020-11-15T13:30:08-05:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  
  <entry>
    <title type="html"><![CDATA[1.5x the PH bits for one more CLMUL]]></title>
    <link href="https://www.pvk.ca/Blog/2020/10/31/nearly-double-the-ph-bits-with-one-more-clmul/"/>
    <updated>2020-10-31T18:30:19-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/10/31/nearly-double-the-ph-bits-with-one-more-clmul</id>
    <content type="html"><![CDATA[<p><small>Turns out the part where I simply asserted a property was slightly off üòâ. I had to go back to an older proof with weaker bounds, but that‚Äôs not catastrophic: we still collide way more rarely than \(2^{-70}\).</small></p>

<p>The core of <a href="https://github.com/backtrace-labs/umash">UMASH</a> is a
hybrid <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.9929&amp;rep=rep1&amp;type=pdf#page=3">PH</a>/<a href="https://eprint.iacr.org/2004/319.pdf#page=4">(E)NH</a>
block compression function.
That function is fast
(it needs one multiplication for each 16-byte ‚Äúchunk‚Äù in a block),
but relatively weak:
despite a 128-bit output, the worst-case probability of collision is
\(2^{-64}\).</p>

<p>For a <a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprinting</a>
application, we want collision probability less than \(\approx 2^{-70},\)
so that‚Äôs already too weak,
before we even consider merging a variable-length string of compressed
block values.</p>

<p>The <a href="https://pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting/">initial UMASH proposal</a>
compresses each block with two independent compression functions.
Krovetz showed that we could do so while reusing most of the key material
(random parameters), with a <a href="http://krovetz.net/csus/papers/thesis.pdf#page=50">Toeplitz extension</a>,
and I simply recycled the proof for UMASH‚Äôs hybrid compressor.</p>

<p>That‚Äôs good for the memory footprint of the random parameters, but
doesn‚Äôt help performance: we still have to do double the work to get
double the hash bits.</p>

<p>Earlier this month, <a href="https://github.com/jbapple">Jim Apple</a> pointed me at
a <a href="https://link.springer.com/chapter/10.1007/978-3-662-46706-0_25">promising alternative that doubles the hash bit with only one more multiplication</a>.
The construction adds finite field operations that aren‚Äôt particularly
efficient in software, on top of the additional 64x64 -&gt; 128
(carryless) multiplication, so isn‚Äôt a slam dunk win over a
straightforward Toeplitz extension.
However, Jim felt like we could ‚Äúspend‚Äù some of the bits we don‚Äôt need
for fingerprinting (\(2^{-128}\) collision probability is overkill
when we only need \(2^{-70}\)) in order to make do with faster operations.</p>

<p>Turns out he was right! We can use carryless multiplications by sparse
constants (concretely, xor-shift and one more shift) without any
reducing polynomial, <em>on independent 64-bit halves</em>‚Ä¶ and still
collide with probability at most <s>2^{-126}</s> \(2^{-98}\).</p>

<p>The proof is fairly simple, but relies on a bit of notation for clarity.
Let‚Äôs start by re-stating UMASH‚Äôs hybrid PH/ENH block compressor in
that notation.</p>

<h2 id="how-does-umash-currently-work">How does UMASH currently work?</h2>

<p>The current block compressor in UMASH splits a 256-byte block \(m\)
in 16 chunks \(m_i,\, i\in [0, 15]\) of 128 bits each, and processes
all but the last chunk with a PH loop,</p>

<p>\[ \bigoplus_{i=0}^{14} \mathtt{PH}(k_i, m_i), \]</p>

<p>where</p>

<p>\[ \mathtt{PH}(k_i, m_i) = ((k_i \bmod 2^{64}) \oplus (m_i \bmod 2^{64})) \odot (\lfloor k_i / 2^{64} \rfloor \oplus \lfloor m_i / 2^{64} \rfloor) \]</p>

<p>and each \(k_i\) is a randomly generated 128-bit parameter.</p>

<p>The compression loop in UMASH handles the last chunk, along with a
size tag (to protect against extension attacks), with
<a href="https://eprint.iacr.org/2004/319.pdf#page=4">ENH</a>:</p>

<p>\[ \mathtt{ENH}(k, x, y) = ((k + x) \bmod 2^{64}) \cdot (\lfloor k / 2^{64}\rfloor + \lfloor x / 2^{64} \rfloor \bmod 2^{64}) + y \mod 2^{128}. \]</p>

<p>The core operation in ENH is a full (64x64 -&gt; 128) integer
multiplication, which has lower latency than PH‚Äôs carryless
multiplication on x86-64.  That‚Äôs why UMASH switches to ENH for the
last chunk.  We use ENH for only one chunk because combining multiple
NH values calls for 128-bit additions, and that‚Äôs slower than PH‚Äôs
xors.  Once we have mixed the last chunk and the size tag with ENH,
the result is simply xored in with the previous chunks‚Äô PH values:</p>

<p>\[ \left(\bigoplus_{i=0}^{14} \mathtt{PH}(k_i, m_i)\right)  \oplus \mathtt{ENH}(m_{15}, k_{15}, \mathit{tag}). \]</p>

<p>This function is annoying to analyse directly, because we end up
having to manipulate different proofs of almost-universality.  Let‚Äôs
abstract things a bit, and reduce the ENH/PH to the bare minimum we need
to find our collision bounds.</p>

<p>Let‚Äôs split our message blocks in \(n\) (\(n = 16\) for
UMASH) ‚Äúchunks‚Äù, and apply an independently sampled mixing function
to each chunk.  Let‚Äôs say we have two messages \(m\) and
\(m^\prime\) with chunks \(m_i\) and \(m^\prime_i\), for \(i\in
[0, n)\), and let \(h_i\) be the result of mixing chunk \(m_i,\)
and \(h^\prime_i\) that of mixing \(m^\prime_i.\)</p>

<p>We‚Äôll assume that the first chunk is mixed with a
\(2^{-w}\)-almost-universal (\(2^{-64}\) for UMASH) hash function:
if \(m_0 \neq m^\prime_0,\) \(\mathrm{P}[h_0 = h^\prime_0] \leq 2^{-w},\)
(where the probability is taken over the set of randomly chosen
parameters for the mixer).
Otherwise, \(m_0 = m^\prime_0 \Rightarrow h_i = h^\prime_i\).</p>

<p>This first chunks stands for the ENH iteration in UMASH.</p>

<p>Every remaining chunk will instead be mixed with a
\(2^{-w}\)-XOR-almost-universal hash function:
if \(m_i \neq m^\prime_i\) (\(0 &lt; i &lt; n\)),
\(\mathrm{P}[h_i \oplus h^\prime_i = y] \leq 2^{-w}\)
for any \(y,\)
where the probability is taken over the randomly generated
parameter for the mixer.</p>

<p>This stronger condition represents the PH iterations in UMASH.</p>

<p>We hash a full block by xoring all the mixed chunks together:</p>

<p>\[ H = \bigoplus_{i = 0}^{n - 1} h_i, \]</p>

<p>and</p>

<p>\[ H^\prime = \bigoplus_{i = 0}^{n - 1} h^\prime_i. \]</p>

<p>We want to bound the probability that \(H = H^\prime \Leftrightarrow
H \oplus H^\prime = 0,\) assuming that the messages differ
(i.e., there is at least one index \(i\) such that
\(m_i \neq m^\prime_i\)).</p>

<p>If the two messages only differ in \(m_0 \neq n^\prime_0\) (and thus
\(m_i = m^\prime_i,\,\forall i \in [1, n)\)),</p>

<p>\[ \bigoplus_{i = 1}^{n - 1} h_i = \bigoplus_{i = 1}^{n - 1} h^\prime_i, \]</p>

<p>and thus \(H = H^\prime \Leftrightarrow h_0 = h^\prime_0\).</p>

<p>By hypothesis, the 0th chunks are mixed with a
\(2^{-w}\)-almost-universal hash, so this happens with probability
at most \(2^{-w}\).</p>

<p>Otherwise, assume that \(m_j \neq m^\prime_j\), for some \(j \in
[1, n)\).
We will rearrange the expression</p>

<p>\[ H \oplus H^\prime = h_j \oplus h^\prime_j \oplus \left(\bigoplus_{i\in [0, n) \setminus \{ j \}} h_i \oplus h^\prime_i\right). \]</p>

<p>Let‚Äôs conservatively replace that unwieldly sum with an adversarially
chosen value \(y\):</p>

<p>\[ H \oplus H^\prime = h_j \oplus h^\prime_j \oplus y, \]</p>

<p>and thus \(H = H^\prime\) iff \(h_j \oplus h^\prime_j = y.\)
By hypothesis, the \(j\)th chunk (every chunk but the 0th),
is mixed with a \(2^{-w}\)-almost-XOR-universal hash, and
this thus happens with probability at most \(2^{-w}\).</p>

<p>In both cases, we find a collision probability at most \(2^{-w}\)
with a simple analysis, despite combining mixing functions from
different families over different rings.</p>

<h2 id="wringing-more-bits-out-of-the-same-mixers">Wringing more bits out of the same mixers</h2>

<p>We combined strong mixers (each is \(2^{-w}\)-almost-universal),
and only got a \(2^{-w}\)-almost-universal output.
It seems like we should be able to do better when two or
more chunks differ.</p>

<p>As <a href="https://link.springer.com/chapter/10.1007/978-3-662-46706-0_25">Nandi</a>
points outs, we can apply erasure codes to derive additional
chunks from the original messages‚Äô contents.  We only need
one more chunk, so we can simply xor together all the original
chunks:</p>

<p>\[m_n = \bigoplus_{i=0}^{n - 1} m_i,\]</p>

<p>and similarly for \(m^\prime_n\).
If \(m\) and \(m^\prime\) differ in only one chunk, \(m_n \neq
m^\prime_n\).  It‚Äôs definitely possible for \(m_n = m^\prime_n\)
when \(m \neq m^\prime\), but only if two or more chunks differ.</p>

<p>We will again mix \(m_n\) and \(m^\prime_n\) with a fresh
\(2^{-w}\)-almost-XOR-universal hash function to yield \(h_n\) and
\(h^\prime_n\).</p>

<p>We want to xor the result \(h_n\) and \(h^\prime_n\) with the second
(still undefined) hash values \(H_2\) and \(H^\prime_2\); if
\(m_n \neq m^\prime_n\), the final xored values are equal with
probability at most \(2^{-w}\), regardless of \(H_2\) and
\(H^\prime_2\ldots\) and, crucially, independently of \(H \neq
H^\prime\).</p>

<p>When the two messages \(m\) and \(m^\prime\) only differ in a
single (initial) chunk, mixing a <a href="https://en.wikipedia.org/wiki/Longitudinal_redundancy_check">LRC checksum</a>
gives us an independent hash function, which
squares the collision probability to \(2^{-2w}\).</p>

<p>Now to the interesting bit: we must define a second hash function
that combines \(h_0,h_1,\ldots, h_{n - 1}\) and \(h^\prime_0, h^\prime_1, \ldots, h^\prime_{n - 1}\)
such that the resulting hash values \(H_2\) and \(H^\prime_2\) collide
independently enough of \(H\) and \(H^\prime\).
That‚Äôs a tall order, but we do have one additional assumption to work
with: we only care about collisions in this second hash function if the
additional checksum chunks are equal, which means that the two messages
differ in two or more chunks (or they‚Äôre identical).</p>

<p>For each index \(0 &lt; i &lt; n\), we‚Äôll fix a <em>public</em> linear (with xor
as the addition) function \(\overline{xs}_i(x)\).  This family of
function must have two properties:</p>

<ol>
  <li>\(f(x) = x \oplus \overline{xs}_i(x)\) is invertible for all \(0 &lt; i &lt; n\).</li>
  <li>For any two distinct \(0 &lt; i &lt; j &lt; n\), xoring the two functions
together into \(g(x) = \overline{xs}_i(x) \oplus
\overline{xs}_j(x)\) yields a function with a small (low rank)
null space.  In other words, while \(g\) may not be invertible,
it must be ‚Äúpretty close.‚Äù</li>
</ol>

<p>For regularity, we will also define \(\overline{xs}_0(x) = x\).</p>

<p>Concretely, let \(\overline{xs}_1(x) = x \mathtt{¬´} 1\), where the
bitshift is computed for the two 64-bit halves independently, and
\(\overline{xs}_i(x) = (x  \mathtt{¬´} 1) \oplus (x \mathtt{¬´} i)\)
for \(i &gt; 1\), again with all the bitshifts computed independently
over the two 64-bit halves.</p>

<p>To see that these satisfy our requirements, we can represent the
functions as carryless multiplication by distinct ‚Äúeven‚Äù constants
(the least significant bit is 0) on each 64-bit half:</p>

<ol>
  <li>Once we xor in \(x\), we get a multiplication by an odd constant,
and that‚Äôs invertible.</li>
  <li>Combining \(\overline{xs}_i\) and \(\overline{xs}_j\) with xor
yields either \(x \mathtt{¬´} j\) or \((x \mathtt{¬´} i) \oplus (x \mathtt{¬´} j)\).  In the worst case, we lose \(j\) bits in each 64-bit half,
and there are thus \(2^{2j}\) values in \(g^{-1}(0)\).</li>
</ol>

<p>To recapitulate, we defined the first hash function as</p>

<p>\[ H = \bigoplus_{i = 0}^{n - 1} h_i, \]</p>

<p>the (xor) sum of the mixed value \(h_i\) for each chunk \(m_i\) in
the message block \(m\), and similarly for \(H^\prime\) and
\(h^\prime_i\).</p>

<p>We‚Äôll let the second hash function be</p>

<p>\[ H_2 \oplus h_n = \left(\bigoplus_{i = 0}^{n - 1} \overline{xs}_i(h_i)\right) \oplus h_n, \]</p>

<p>and</p>

<p>\[ H^\prime_2 \oplus h^\prime_n = \left(\bigoplus_{i = 0}^{n - 1} \overline{xs}_i(h^\prime_i)\right) \oplus h^\prime_n. \]</p>

<p>We can finally get down to business and find some collision bounds.
We‚Äôve already shown that both \(H = H^\prime\) <em>and</em> \(H_2 \oplus h_n =
H^\prime_2 \oplus h^\prime_n\) collide simultaneously with probability at most \(2^{-2w}\)
when the checksum chunks differ, i.e., when \(m_n \neq m^\prime_n\).</p>

<p>Let‚Äôs now focus on the case when \(m \neq m^\prime\), but \(m_n =
m^\prime_n\).
In that case, we know that at least two chunks \(0 \leq i &lt; j &lt; n\)
differ: \(m_i \neq m^\prime_i\) and \(m_j \neq m^\prime_j\).</p>

<p>If only two chunks \(i\) and \(j\) differ, and one of them is the
\(i = 0\)th chunk, we want to bound the probability that</p>

<p>\[ h_0 \oplus h_j = h^\prime_0 \oplus h^\prime_j \]</p>

<p>and</p>

<p>\[ h_0 \oplus \overline{xs}_j(h_j) = h^\prime_0 \oplus \overline{xs}_j(h^\prime_j), \]</p>

<p>both at the same time.</p>

<p>Letting \(\Delta_i = h_i \oplus h^\prime_i\), we can reformulate the
two conditions as</p>

<p>\[ \Delta_0 = \Delta_j \]
and
\[ \Delta_0 = \overline{xs}_j(\Delta_j). \]</p>

<p>Taking the xor of the two conditions yields</p>

<p>\[ \Delta_j \oplus \overline{xs}_j(\Delta_j) = 0, \]</p>

<p>which is only satisfied for \(\Delta_j = 0\), since \(f(x) = x
\oplus \overline{xs}_j(x)\) is an invertible linear function.
This also forces \(\Delta_0 = 0\).</p>

<p>By hypothesis, \(\mathrm{P}[\Delta_j = 0] \leq 2^{-w}\), and
\(\mathrm{P}[\Delta_0 = 0] \leq 2^{-w}\) as well.  These two
probabilities are independent, so we get a probability that both
hash collide less than or equal to \(2^{-2w}\) (\(2^{-128}\)).</p>

<p>In the other case, we have messages that differ in at least two chunks
\(0 &lt; i &lt; j &lt; n\): \(m_i \neq m^\prime_i\) and \(m_j \neq
m^\prime_j\).</p>

<p>We can simplify the collision conditions to</p>

<p>\[ h_i \oplus h_j = h^\prime_i \oplus h^\prime_j \oplus y \]</p>

<p>and</p>

<p>\[ \overline{xs}_i(h_i) \oplus \overline{xs}_j(h_j) = \overline{xs}_i(h^\prime_i) \oplus \overline{xs}_j(h^\prime_j) \oplus z, \]</p>

<p>for \(y\) and \(z\) generated arbitrarily (adversarially), but
without knowledge of the parameters that generated \(h_i, h_j, h^\prime_i,
h^\prime_j\).</p>

<p>Again, let \(\Delta_i = h_i \oplus h^\prime_i\) and \(\Delta_j = h_j \oplus h^\prime_j\), and reformulate the conditions into</p>

<p>\[ \Delta_i \oplus \Delta_j = y \]
and
\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_j(\Delta_j) = z. \]</p>

<p>Let‚Äôs apply the linear function \(\overline{xs}_i\) to the first
condition</p>

<p>\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_i(\Delta_j) = \overline{xs}_i(y); \]</p>

<p>since \(\overline{xs}_i\) isn‚Äôt invertible, the result
isn‚Äôt equivalent, but is a weaker (necessary, not sufficient)
version of the initial condiion.</p>

<p>After xoring that with the second condition</p>

<p>\[ \overline{xs}_i(\Delta_i) \oplus \overline{xs}_j(\Delta_j) = z, \]</p>

<p>we find</p>

<p>\[ \overline{xs}_i(\Delta_j) \oplus \overline{xs}_j(\Delta_j) = \overline{xs}_i(y) \oplus z. \]</p>

<p>By hypothesis, the null space of
\(g(x) = \overline{xs}_i(x) \oplus \overline{xs}_j(x)\)
is ‚Äúsmall.‚Äù
For our concrete definition of \(\overline{xs}\), there
are \(2^{2j}\) values in that null space, which means that \(\Delta_j\)
can only satisfy the combined xored condition by taking one of at most
\(2^{2j}\) values; otherwise, the two hashes definitely can‚Äôt both
collide.</p>

<p>Since \(j &lt; n\), this happens with probability at most \(2^{2(n -
1) - w} \leq 2^{-34}\) for UMASH with \(w = 64\) and \(n =
16\).</p>

<p>Finally, for any given \(\Delta_j\), there is at most one
\(\Delta_i\) that satisfies</p>

<p>\[ \Delta_i \oplus \Delta_j = y,\]</p>

<p>and so <em>both</em> hashes collide with probability at most \(2^{-98}\),
for \(w = 64\) and \(n = 16\).</p>

<p>Astute readers will notice that we could let
\(\overline{xs}_i(x) = x \mathtt{¬´} i\),
and find the same combined collision probability.
However, this results in a much weaker secondary hash, since a chunk
could lose up to \(2n - 2\) bits (\(n - 1\) in each 64-bit half)
of hash information to a plain shift.
The shifted xor-shifts might be a bit slower to compute, but
guarantees that we only lose at most 2 bits<sup id="fnref:one-bit-with-ph" role="doc-noteref"><a href="#fn:one-bit-with-ph" class="footnote">1</a></sup> of
information per chunk.  This feels like an interface that‚Äôs harder to
misuse.</p>

<p>If one were to change the \(\overline{xs}_i\) family of functions, I
think it would make more sense to look at a more diverse form of
(still sparse) multipliers, which would likely let us preserve a
couple more bits of independence.  Jim has constructed such a family
of multipliers, in arithmetic modulo \(2^{64}\); I‚Äôm sure we could
find something similar in carryless multiplication.  The hard part is
implementing these multipliers: in order to exploit the multipliers‚Äô
sparsity, we‚Äôd probably have to fully unroll the block hashing loop,
and that‚Äôs not something I like to force on implementations.</p>

<h2 id="what-does-this-look-like-in-code">What does this look like in code?</h2>

<p>The base <a href="https://github.com/backtrace-labs/umash/blob/8fd6287617f41e236bfb679e8d29a8b32f82c0e9/umash.c#L336">UMASH block compressor</a>
mixes all but the last of the message block‚Äôs 16-byte chunks with
PH: xor the chunk with the corresponding bytes in the parameter
array, computes a carryless multiplication of the xored chunks‚Äô half
with the other half.  The last chunk goes through a variant of
<a href="https://github.com/backtrace-labs/umash/blob/8fd6287617f41e236bfb679e8d29a8b32f82c0e9/umash.c#L358">ENH</a>
with an invertible finaliser (safe because we only rely on
\(\varepsilon\)-almost-universality),
and everything is xored in the accumulator.</p>

<p>The collision proofs above preserved the same structure for the first hash.</p>

<p>The second hash reuses so much work from the first that it mostly
makes sense to consider a combined loop that computes both (regular
UMASH and this new xor-shifted variant) block compression functions at
the same time.</p>

<p>The first change for this combined loop is that we need to xor
together all 16-bytes chunk in the message, and mix the resulting
checksum with a fresh PH function.  That‚Äôs equivalent to xoring
everything in a new accumulator (or two accumulators when working with
256-bit vectors) initialised with the PH parameters, and <code>CLMUL</code>ing
together the accumulator‚Äôs two 64-bit halves at the end.</p>

<p>We also have to apply the \(\overline{xs}_i\) quasi-xor-shift
functions to each \(h_i\).  The trick is to accumulate the shifted
values in two variables: one is the regular UMASH accumulator without
\(h_0\) (i.e., \(h_1 \oplus h_2 \ldots\)), and the other shifts
the current accumulator before xoring in a new value, i.e.,
\(\mathtt{acc}^\prime = (\mathtt{acc} \mathtt{¬´} 1) \oplus h_i\),
where the left shift on parallel 64-bit halves simply adds <code>acc</code>
to itself.</p>

<p>This additional shifted accumulator includes another special case to
skip \(\overline{xs}_1(x) = x \mathtt{¬´} 1\); that‚Äôs not a
big deal for the code, since we already have to special case the last
iteration for the ENH mixer.</p>

<p>Armed with \(\mathtt{UMASH} = \bigoplus_{i=1}^{n - 1} h_i\) and
\(\mathtt{acc} = \bigoplus_{i=2}^{n - 1} h_i \mathtt{¬´} (i - 1),\)
we have
\[\bigoplus_{i=1}^{n - 1} \overline{xs}_i(h_i) = (\mathtt{UMASH} \oplus \mathtt{acc}) \mathtt{¬´} 1.\]</p>

<p>We just have to xor in the <code>PH</code>-mixed checksum \(h_n\), and finally
\(h_0\) (which naturally goes in GPRs, so can be computed while we
extract values out of vector registers).</p>

<p>We added two vector xors and one addition for each chunk in a block,
and, at the end, one <code>CLMUL</code> plus a couple more xors and adds again.</p>

<p>This should most definitely be faster than computing two UMASH at the
same time, which incurred two vector xors and a <code>CLMUL</code> (or full
integer multiplication) for each chunk: even when <code>CLMUL</code> can pipeline
one instruction per cycle, vector additions can dispatch to more
execution units, so the combined throughput is still higher.</p>

<h2 id="one-last-thing-what-if-we-have-blocks-of-different-length">One last thing: what if we have blocks of different length?</h2>

<p>It‚Äôs easy to show that UMASH is relatively safe when one block is
shorter than the other, and we simply xor together fewer mixed chunks.
Without loss of generality, we can assume the longer block has \(n\)
chunks; that block‚Äôs final ENH is independent of the shorter block‚Äôs
UMASH, and any specific value occurs with probability at most
\(2^{-63}\) (the probability of a multiplication by zero).</p>

<p>A similar argument seems more complex to defend for the shifted UMASH.</p>

<p>Luckily, we can tweak the <a href="https://en.wikipedia.org/wiki/Longitudinal_redundancy_check">LRC checksum</a>
we use to generate an additional chunk in the block: rather than xoring
together the raw message chunks, we‚Äôll xor them <em>after</em> xoring them
with the PH key, i.e.,</p>

<p>\[m_n = \bigoplus_{i=0}^{n - 1} m_i \oplus k_i, \]</p>

<p>where \(k_i\) are the PH parameters for each chunk.</p>

<p>When checksumming blocks of the same size, this is a no-op with respect
to collision probabilities.  Implementations might however benefit
from the ability to use a fused <code>xor</code> with load from memory<sup id="fnref:latency" role="doc-noteref"><a href="#fn:latency" class="footnote">2</a></sup>
to compute \(m_i \oplus k_i\), and feed that both into the checksum
and into <code>CLMUL</code> for PH.</p>

<p>Unless we‚Äôre extremely unlucky (\(m_{n - 1} = k_{n - 1}\), with
probability \(2^{-2w}\)), the long block‚Äôs LRC will differ from the
shorter block‚Äôs.  As long as we always xor in the same PH parameters
when mixing the artificial LRC, the secondary hashes collide with
probability at most \(2^{-64}\).</p>

<p>With a small tweak to the checksum function, we can easily guarantee
that blocks with a different number of chunks collide with probability
less than \(2^{-126}\).<sup id="fnref:tag" role="doc-noteref"><a href="#fn:tag" class="footnote">3</a></sup></p>

<p><small>Thank you Joonas for helping me rubber duck the presentation, and Jim for pointing me in the right direction, and for the fruitful discussion!</small></p>

<p><hr style="width: 50%" /></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:one-bit-with-ph" role="doc-endnote">
      <p>It‚Äôs even better for UMASH, since we obtained these shifted chunks by mixing with PH.  The result of PH is the carryless product of two 64-bit values, so the most significant bit is always 0.  The shifted-xorshift doesn‚Äôt erase any information in the high 64-bit half!¬†<a href="#fnref:one-bit-with-ph" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:latency" role="doc-endnote">
      <p>This might also come with a small latency hit, which is unfortunate since PH-ing \(m_n\) is likely to be on the critical path‚Ä¶ but one cycle doesn‚Äôt seem that bad.¬†<a href="#fnref:latency" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tag" role="doc-endnote">
      <p>The algorithm to expand any input message to a sequence of full 16-byte chunks is fixed.  That‚Äôs why we incorporate a size tag in ENH; that makes it impossible for two messages of different lengths to collide when they are otherwise identical after expansion.¬†<a href="#fnref:tag" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UMASH: a fast and universal enough hash]]></title>
    <link href="https://www.pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting/"/>
    <updated>2020-08-24T00:00:15-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting</id>
    <content type="html"><![CDATA[<p><small>Originally posted on the <a href="https://engineering.backtrace.io/2020-08-24-umash-fast-enough-almost-universal-fingerprinting/">Backtrace I/O tech blog</a>.</small></p>

<p>We accidentally a whole hash function‚Ä¶ but we had a good reason!
Our
<a href="https://github.com/backtrace-labs/umash">MIT-licensed UMASH hash function</a>
is a decently fast non-cryptographic hash function that guarantees
a worst-case bound on the  probability of collision
<a href="https://en.wikipedia.org/wiki/Universal_hashing#Mathematical_guarantees">between any two inputs generated independently of the UMASH parameters</a>.</p>

<p>On the
<a href="https://en.wikichip.org/wiki/intel/xeon_platinum/8175m">2.5 GHz Intel 8175M</a>
servers that power <a href="https://backtrace.io/">Backtrace</a>‚Äôs hosted
offering, UMASH computes a 64-bit hash for short cached inputs of up
to 64 bytes in 9-22 ns, and for longer ones at up to 22 GB/s, while
guaranteeing that two distinct inputs of at most \(s\) bytes collide
with probability less than \(\lceil s / 2048 \rceil \cdot 2^{-56}\).
If that‚Äôs not good enough, we can also reuse most of the parameters to
compute two independent UMASH values. The resulting 128-bit
<a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprint function</a>
offers a short-input latency of 9-26 ns, a peak throughput of 11.2
GB/s, and a collision probability of \(\lceil s / 2048 \rceil^2 \cdot
2^{-112}\) (better than \(2^{-70}\) for input size up to 7.5 GB).
These collision bounds hold for all inputs constructed without any
feedback about the randomly chosen UMASH parameters.</p>

<p>The latency on short cached inputs (9-22 ns for 64 bits, 9-26 ns for
128) is somewhat worse than the state of the art for non-cryptographic
hashes‚Äî
<a href="https://github.com/wangyi-fudan/wyhash">wyhash</a> achieves
8-15 ns and <a href="http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html">xxh3</a>
 8-12 ns‚Äîbut still in the same ballpark.  It also
compares well with latency-optimised hash functions like
<a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function#FNV-1a_hash">FNV-1a</a>
(5-86 ns) and
<a href="https://en.wikipedia.org/wiki/MurmurHash#MurmurHash2">MurmurHash64A</a>
(7-23 ns).</p>

<p>Similarly, UMASH‚Äôs peak throughput (22 GB/s) does not match
the current best hash throughput (37 GB/s with
<a href="https://github.com/Cyan4973/xxHash">xxh3</a>
and <a href="https://github.com/gamozolabs/falkhash">falkhash</a>, apparently
10% higher with <a href="https://github.com/cmuratori/meow_hash">Meow hash</a>),
but does comes within a factor of two; it‚Äôs actually higher than that of
some performance-optimised hashes, like
<a href="https://github.com/wangyi-fudan/wyhash">wyhash</a> (16 GB/s) and
<a href="https://github.com/google/farmhash">farmhash32</a>
(19 GB/s).  In fact, even the 128-bit fingerprint (11.2 GB/s) is
comparable to respectable options like
<a href="https://github.com/aappleby/smhasher/blob/master/src/MurmurHash2.cpp#L89">MurmurHash64A</a>
(5.8 GB/s) and
<a href="https://burtleburtle.net/bob/hash/spooky.html">SpookyHash</a> (11.6 GB/s).</p>

<p>What sets UMASH apart from these other non-cryptographic hash
functions is its proof of a collision probability bound.  In the
absence of an adversary that adaptively constructs pathological inputs
as it infers more information about the randomly chosen parameters, we
know that two distinct inputs of \(s\) or fewer bytes will have the
same 64-bit hash with probability at most \(\lceil s / 2048 \rceil
\cdot 2^{-56},\) where the expectation is taken over the random
‚Äúkey‚Äù parameters.</p>

<p>Only one non-cryptographic hash function in
<a href="https://github.com/rurban/smhasher">Reini Urban‚Äôs fork of SMHasher</a>
provides this sort of bound: <a href="https://github.com/lemire/clhash">CLHash</a>
<a href="https://arxiv.org/abs/1503.03465">guarantees a collision probability \(\approx 2^{-63}\)</a>
in the same
<a href="https://en.wikipedia.org/wiki/Universal_hashing#Mathematical_guarantees">universal hashing</a>
model as UMASH.  While CLHash‚Äôs peak throughput (22 GB/s) is
equal to UMASH‚Äôs, its latency on short inputs is worse (23-25 ns
instead of 9-22ns).  We will also see that its stronger collision
bound remains too weak for many practical applications.  In order to
compute a <a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprint</a>
with CLHash, one would have to combine multiple hashes, exactly like
we did for the 128-bit UMASH fingerprint.</p>

<p>Actual cryptographic hash functions provide stronger bounds in a much
more pessimistic model; however they‚Äôre also markedly slower than
non-cryptographic hashes.  <a href="https://github.com/BLAKE3-team/BLAKE3">BLAKE3</a>
needs at least 66 ns to hash short inputs, and achieves a peak throughput
of 5.5 GB/s.  Even the <a href="https://github.com/rust-lang/rust/issues/29754">reduced-round SipHash-1-3</a>
hashes short inputs in 18-40 ns and longer ones at a peak throughput
of 2.8 GB/s.  That‚Äôs the price of their pessimistically adversarial
security model.  Depending on the application, it can make sense to
consider a more restricted adversary that must prepare its dirty deed
before the hash function‚Äôs parameters are generated at random, and
still ask for provable bounds on the probability of collisions.
That‚Äôs the niche we‚Äôre targeting with UMASH.</p>

<p>Clearly, the industry is comfortable with no bound at all.
However, even in the absence of
<a href="https://www.131002.net/siphash/#at">seed-independent collisions</a>,
timing side-channels in a data structure implementation could
theoretically leak information about colliding inputs, and iterating
over a hash table‚Äôs entries to print its contents can divulge even more
bits.  A sufficiently motivated adversary could use something like
that to learn more about the key and deploy an algorithmic denial of
service attack.  For example, the linear structure of UMASH (and of
other polynomial hashes like CLHash) makes it easy to combine known
collisions to create exponentially more colliding inputs.  There is no
universal answer; UMASH is simply another point in the solution space.</p>

<p>If reasonable performance coupled with an actual bound on collision
probability <em>for data that does not adaptively break the hash</em> sounds
useful to you,
<a href="https://github.com/backtrace-labs/umash">take a look at UMASH on GitHub</a>!</p>

<p>The <a href="#but-why">next section</a> will explain why we found it useful to
design another hash function.  The rest of the post
<a href="#umash-high-level">sketches how UMASH works</a> and
<a href="#implementation-tricks">how it balances short-input latency and strength</a>,
before <a href="#usage">describing a few interesting usage patterns.</a></p>

<p><small>The latency and throughput results above were all measured on
the same unloaded 2.5 GHz Xeon 8175M.  While we did not disable
frequency scaling (#cloud), the clock rate seemed stable at 3.1
GHz during our run.</small></p>

<h2 id="how-did-we-even-get-here"><a id="but-why"></a>How did we even get here?</h2>

<p>Engineering is the discipline of satisficisation: crisply defined
problems with perfect solutions rarely exist in reality, so we must
resign ourselves to satisfying approximate constraint sets ‚Äúwell
enough.‚Äù  However, there are times when all options are not only
imperfect, but downright sucky.  That‚Äôs when one has to put on a
different hat, and question the problem itself: are our constraints
irremediably at odds, or are we looking at an under-explored
solution space?</p>

<p>In the former case, we simply have to want something else.  In the
latter, it might make sense to spend time to really understand the
current set of options and hand-roll a specialised approach.</p>

<p>That‚Äôs the choice we faced when we started caching intermediate
results in
<a href="https://help.backtrace.io/en/articles/2428859-web-console-overview">Backtrace‚Äôs database</a>
and found a dearth of acceptable hash functions.  Our in-memory
columnar database is a core component of the backend, and, like most
analytics databases, it tends to process streams of similar queries.
However, a na√Øve query cache would be ineffective: our more heavily
loaded servers handle a constant write load of more than 100 events
per second with dozens of indexed attributes (populated column values)
each.  Moreover, queries invariably select a large number of data
points with a time windowing predicate that excludes old data‚Ä¶ and
the endpoints of these time windows advance with each wall-clock
second.  The queries evolve over time, and must usually consider newly
ingested data points.</p>

<p><a href="https://www.gsd.inesc-id.pt/~rodrigo/slider_middleware14.pdf">Bhatotia et al‚Äôs Slider</a>
show how we can specialise the idea of
<a href="http://adapton.org/">self-adjusting or incremental computation</a>
for repeated MapReduce-style queries over a sliding window.
The key idea is to split the data set at stable boundaries (e.g., on
date change boundaries rather than 24 hours from the beginning of the
current time window) in order to expose memoisation opportunities, and
to do so recursively to repair around point mutations to older data.</p>

<p>Caching fully aggregated partial results works well for static
queries, like scheduled reports‚Ä¶ but the first step towards creating
a great report is interactive data exploration, and that‚Äôs an activity
we strive to support well, even when drilling down tens of millions of
rich data points.  That‚Äôs why we want to also cache intermediate
results, in order to improve response times when tweaking a saved
report, or when crafting ad hoc queries to better understand how and
when an application fails.</p>

<p>We must go back to a
<a href="http://www.umut-acar.org/self-adjusting-computation">more general incremental computation strategy</a>:
rather than only splitting up inputs, we want to stably partition the
data dependency graph of each query, in order to identify shared
subcomponents whose results can be reused.  This finer grained
strategy surfaces opportunities to ‚Äúresynchronise‚Äù computations, to
recognize when different expressions end up generating a subset of
identical results, enabling reuse in later steps.  For example, when
someone updates a query by adding a selection predicate that only
rejects a small fraction of the data, we can expect to reuse some of
the post-selection work executed for earlier incarnations of the
query, if we remember to key on the selected data points rather than
the predicates.</p>

<p>The complication here is that these intermediate results tend to be
large.  Useful analytical queries start small (a reasonable query
coupled with cache/transaction invalidation metadata to stand in for
the full data set), grow larger as we select data points, arrange them
in groups, and materialise their attributes, and shrink again at the
end, as we summarise data and throw out less interesting groups.</p>

<p>When caching the latter shrinking steps, where resynchronised reuse
opportunities abound and can save a lot of CPU time, we often
find that storing a fully materialised representation of the cache key
would take up more space than the cached result.</p>

<p>A classic approach in this situation is to fingerprint cache keys with
a cryptographic hash function like
<a href="https://en.wikipedia.org/wiki/BLAKE_(hash_function)">BLAKE</a>
or <a href="https://en.wikipedia.org/wiki/SHA-3">SHA-3</a>, and store a
compact (128 or 256 bits) fingerprint instead of the cache key: the
probability of a collision is then so low that we might as well assume
any false positive will have been caused by a bug in the code or a
hardware failure.  For example,
<a href="https://users.ece.cmu.edu/~omutlu/pub/memory-errors-at-facebook_dsn15.pdf#page=3">a study of memory errors at Facebook</a>
found that uncorrectable memory errors affect 0.03% of servers each
month.  Assuming a generous clock rate of 5 GHz, this means each
clock cycle may be afflicted by such a memory error with probability
\(\approx 2.2\cdot 10^{-20} &gt; 2^{-66}.\) If we can guarantee that
distinct inputs collide with probability significantly less than
\(2^{-66}\), e.g., \(&lt; 2^{-70},\) any collision is far
more likely to have been caused by a bug in our code or by
hardware failure than by the fingerprinting algorithm itself.</p>

<p>Using cryptographic hashes is certainly safe enough, but requires a lot of
CPU time, and, more importantly, worsens latency on smaller keys (for
which caching may not be that beneficial, such that our goal should be
to minimise overhead).  It‚Äôs not that state-of-the-art cryptographic
hash functions are wasteful, but that they defend against attacks like key
recovery or collision amplification that we may not care to consider
in our design.</p>

<p>At the other extreme of the hash spectrum, there is a plethora of fast
hash functions with no proof of collision probability.  However, most
of them are keyed on just a 64-bit ‚Äúseed‚Äù integer, and that‚Äôs already
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=4">enough for a pigeonhole argument</a>
to show we can construct sets of strings of length \(64m\) bits
where any two members collide with probability at least \(m/
2^{64}\). In practice, <a href="https://131002.net/siphash/#at">security researchers seem to find key-independent collisions wherever they look</a>
(i.e., the collision probability is on the order of 1 for some
particularly pathological sets of inputs), so it‚Äôs safe to assume that
lacking a proof of collision probability implies a horrible worst
case.  I personally wouldn‚Äôt put too much faith in ‚Äúsecurity claims‚Äù
taking the form of failed attempts at breaking a proposal.</p>

<p><a href="https://arxiv.org/abs/1503.03465">Lemire and Kaser‚Äôs CLHash</a> is one
of the few exceptions we found: it achieves a high throughput of 22
GB/s and comes with a proof of \(2^{-63}\)-almost-universality.
However, its finalisation step is slow (23 ns for one-byte inputs), due
to a <a href="https://en.wikipedia.org/wiki/Barrett_reduction">Barrett reduction</a>
followed by
<a href="https://github.com/lemire/clhash/blob/742f81a66c8e2ae7889d1bc4c4b4d8734bdcd5af/src/clhash.c#L243">three rounds of <code>xorshift</code>/multiply mixing</a>.
<a href="https://eprint.iacr.org/2007/338">Dai and Krovetz‚Äôs VHASH</a>,
which inspired CLHash, offers similar guarantees, with worse
performance.</p>

<p>Unfortunately, \(2^{-63}\) is also not quite good enough for our
purposes: we estimate that the probability of uncorrectable memory
errors is on the order of \(2^{-66}\) per clock cycle, so we want
the collision probability for any two distinct inputs to be
comfortably less than that, around \(2^{-70}\) (i.e., \(10^{-21}\)) or
less.  This also tells us that any acceptable fingerprint must consist
of more than 64 bits, so we will have to either work in slower
multi-word domains, or combine independent hashes.</p>

<p>Interestingly, we also don‚Äôt need much more than that for
(non-adversarial) fingerprinting: at some point, the theoretical
probability of a collision is dominated by the practical possibility of
a hardware or networking issue making our program execute the
fingerprinting function incorrectly, or pass the wrong data to
that function.</p>

<p>While CLHash and VHASH aren‚Äôt quite what we want, they‚Äôre pretty
close, so we felt it made sense to come up with a specialised solution
for our fingerprinting use case.</p>

<p><a href="https://tools.ietf.org/html/rfc4418">Krovetz et al‚Äôs RFC 4418</a> brings
an interesting idea: we can come up with a fast 64-bit hash function
structured to make it easy to compute a second independent hash value,
and concatenate two independent 64-bit outputs.  The hash function can
heavily favour computational efficiency and let each 64-bit half
collide with probability \(\varepsilon\) significantly worse than
\(2^{-64}\), as long as the collision probability for the
concatenated fingerprint, \(\varepsilon^2\), is small enough, i.e.,
as long as
\(\varepsilon^2 &lt; 2^{-70} \Longleftrightarrow \varepsilon &lt; 2^{-35}\).
We get a more general purpose hash function out of the deal, and the
fingerprint comparison logic is now free to only compute and look at
half the fingerprint when it makes sense (e.g., in a prepass that
tolerates spurious matches).</p>

<h2 id="umash-at-a-high-level"><a id="umash-high-level"></a>UMASH, at a high level</h2>

<p>The design of UMASH is driven by two observations:</p>

<ol>
  <li>
    <p>CLHash achieves a high throughput, but introduces a lot of
latency to finalise its 127-bit state into a 64 bits result.</p>
  </li>
  <li>
    <p>We can get away with a significantly weaker hash, since we plan to
combine two of them when we need a strong fingerprint.</p>
  </li>
</ol>

<p>That‚Äôs why we started with the high-level structure diagrammed below,
the same as
<a href="https://web.cs.ucdavis.edu/~rogaway/papers/umac.html">UMAC</a>,
<a href="https://eprint.iacr.org/2007/338">VHASH</a>, and
<a href="https://arxiv.org/abs/1503.03465">CLHash</a>:
a fast first-level block compression function based on
<a href="https://dl.acm.org/doi/10.1109/TC.1968.227420">Winograd‚Äôs pseudo dot-product</a>,
and a second-level <a href="https://link.springer.com/chapter/10.1007/3-540-55719-9_77">Carter-Wegman polynomial hash function</a>
to accumulate the compressed outputs in a fixed-size state.</p>

<iframe frameborder="0" style="width:100%;height:523px;" src="https://viewer.diagrams.net/?highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;title=UMASH%20overview#R7VzLcqM4FP0aL5MCIR5ZxulMuxepTpWre5ZTipGNJgJRIGJ7vn4kA7ZB6jSpsgUN2SRwAQHnHMR94ZnzEO%2B%2BZiiNnliI6QxY4W7mfJkBEHie%2BCsN%2B9Lg%2BVZp2GQkLE32ybAk%2F%2BHKWO9WkBDnjR05Y5STtGlcsSTBK96woSxj2%2BZua0abZ03RBiuG5QpR1fo3CXlU3ZZrnewLTDZRfWbbqrbEqN65MuQRCtn2zOQ8zpyHjDFeLsW7B0wldjUu5XF%2F%2FWLr8cIynPAuBxTwpxMvof9j8f0nCZ4eg%2BTVugmqYd4QLao7rq6W72sIxDACbbEyF7eQSuOKskKMOt9GhONlilbSuBX8C1vEYyrWbLFYDY0zjne%2FvGj7CIWQEGYx5tle7FId4MMKvX0ti2p9eyLDrrUSnRFxV9lQxf%2FmOPQJIrFQofQBxODvActYkYRYDmIZAckOmiBBTwXJczQgedcCyVVA%2BpakBT%2FAlJFkIxbYWvyRA73suXjA2xgKNHgTKHEke8UPjLJMWBKWSEWuCaUtE6Jkk0iRCvSwsM8ltkQ80ffVhpiEoTyNlpkmdxcgB7gtcgKVnKPtnBxwLXKC4SvYgZrHHJhU8J0CEnC9G6lVKVnKVq%2Fjlax95zbZ0MwndyYVW0%2FwQ5KsYw9Nsrb6Lp%2BOZh0wNM2C4WkWwsFp1pmwZqE7NM0O0Ll1%2FcFpVvVup6NZNxiaZr3hada3OmhWF7VeT7O%2BgtIzEjpC8kichOMXrt%2F213oXrhpiKOifw16njyQeIcqjIzjdga7zN%2FFuI5N1t2WCDJT%2F5bASG%2BvW8eSyxA2IFShXSIZXnLDkQG8m8ZonjK%2Biy%2FEDbP%2B2ObVAW2UIuipD8FoMgQ7JsQkx5MDhMdTByZ4QQ9AfHkOqgz9lhjxreAx1cPonxJDvgKEx5HTwFE7%2Bkm3ExQVNbwrq6jJGo7Ia%2FTOQliklsuZQpPLgpPZxR1xsaOV3dMrVlRquplzYwYPqWblAU48xrFzViXlezGRJO04znOcyQkNiiht9iNZS7zEc6029n77L%2BxGajiGjb0b46bu8H6H1z5CaLJ0yQ2qE1j9DHRKoE2LIs9r%2Bf%2F8MqcnbKTOkRmj9MzTEnppWFwdwVJB0CW%2F3aiCpYax9LJsdPF5W8EOX2FgdXIWSWqR91SBcNWjuXbftTo7edeuqQezEdKtQ0rtuO0Rmxptr3KHpVg2OJqZbhZLeddshGjLeYNPqve1ft2pAMjHdKpT0rtsOEYjxJht7aLpVg4CJ6VahpHfdDr9yZvuqbM3WHzw1CHhmVF5xhHJZdghqEUeIvo35W532x2aa0pDRAoT3ofKZITfXaYEEetcvUFDy4M3LofSLaMxyflMkRNx0fuh2LEU9WgnDpstra1pPjU7BXodQrcS6%2Fr4WtIhopSLXlG1XEcr4LUoSxpFMNv4DzpigeC25pOgF02eWkyobmZU3O08ZkUw9vglE88s9F3Z78tDM60a%2FwfQ61MbGCLyuFcUs8B2CvDECr%2BukMAt8hyy8oTrJv0Wcfkvuz6sltlvXSpxAnkNuW2Akz%2Bm6F3It23OQ7t3sayg5foxwcU581bn8gzhZU5IuLvfEtL9l0vKj8zCvyM%2Bf4GJ29DCvFtn773iYP57ul1OI7WHrdwyAGtob%2FRUD%2F7M5q0GP124s0fifRmvi%2Fody3b3kXjSfgJkNXX011f1EdsIg5hbZ95lhGbaSF7of8cTS5EQjW6M5F79DqnuM3rvmWTDqvAeqo7gTqgUWxcmGyzTkoYOfoly%2BdaeRTvd%2Bn8vxLvPOFaunXyI7bDv7OTfn8X8%3D"></iframe>

<p>The inner loop in this two-level strategy is the block compressor,
which divides each 256-byte block \(m\) into 32 64-bit values
\(m_i\), combines them with randomly generated parameters \(k_i\),
and converts the resulting sequence of machine words to a 16-byte
output.  The performance of that component will largely determine the
hash function‚Äôs global peak throughput.  After playing around with
<a href="https://web.cs.ucdavis.edu/~rogaway/umac/umac_thesis.pdf#page=41">the <code>NH</code> inner loop</a>,</p>

<div>
\[ \mathtt{NH}_k(m) = \sum_{i=0}^{|m| / 2 - 1} (m_{2i} + k_{2i} \bmod 2^{64})\cdot(m_{2i + 1} + k_{2i + 1} \bmod 2^{64})\mod 2^{128},\]
</div>

<p>we came to the
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=13">same conclusion as Lemire and Kaser</a>:
the scalar operations, the outer 128-bit ones in particular, map to too
many ¬µops.  We thus focused on the
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.9929&amp;rep=rep1&amp;type=pdf#page=3">same <code>PH</code> inner loop</a>
as CLHash,</p>

<div>
\[ \mathtt{PH}_k(m) = \bigoplus_{i=0}^{|m| / 2 - 1} (m_{2i} \oplus k_{2i})\odot (m_{2i + 1} \oplus k_{2i + 1}).\]
</div>

<p>While the similarity to <code>NH</code> is striking, analysing <code>PH</code> is actually
much simpler: we can see the <code>xor</code> and carry-less multiplications as
working in the same ring of polynomials over \(\mathrm{GF}(2)\), unlike
<code>NH</code>‚Äôs mixing of \(\bmod 2^{64}\) for the innermost additions with
\(\bmod 2^{128}\) for the outer multiplications and sum.  In
fact, as
<a href="http://cr.yp.to/antiforgery/pema-20071022.pdf#page=6">Bernstein points out</a>,
<code>PH</code> is a direct application of
<a href="https://dl.acm.org/doi/10.1109/TC.1968.227420">Winograd‚Äôs pseudo dot-product</a>
to compute a multiplicative vector hash in half the multiplications.</p>

<p>CLHash uses an aggressively throughput-optimised block size of 1024
bytes.  We found diminishing returns after 256 bytes, and stopped
there.</p>

<p>With modular or polynomial ring arithmetic, the collision probability
is \(2^{-64}\) for any pair of blocks.  Given this fast compression
function, the rest of the hashing algorithm must chop the input in
blocks, accumulate compressed outputs in a constant-size state, and
handle the potentially shorter final block while avoiding
<a href="https://en.wikipedia.org/wiki/Length_extension_attack">length extension issues</a>.</p>

<p>Both VHASH and CLHash accumulate compressed outputs in a
polynomial string hash over a large field
(\(\mathbb{Z}/M_{127}\mathbb{Z}\) for VHASH, and
\(\mathrm{GF}(2^{127})\) with irreducible polynomial \(x^{127} + x + 1\)
for CLHash): the collision probability for polynomial string hashes is
inversely proportional to the field size and grows with the string
length (number of compressed blocks), so working in fields much larger
than \(2^{64}\) lets the <code>NH</code>/<code>PH</code> term dominate.</p>

<p>Arithmetic in such large fields is slow, and reducing the 127-bit
state to 64 bits is also not fast.  CLHash and VHASH make the
situation worse by zero-padding the final block, and CLHash defends
against length extension attacks with
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=10">a more complex mechanism</a>
than <a href="https://eprint.iacr.org/2007/338">the one in VHASH</a>.</p>

<p>Similarly to VHASH, UMASH uses a polynomial hash over the (much
smaller) prime field \(\mathbb{F} = \mathbb{Z}/M_{61}\mathbb{Z},\)</p>

<div>
\[ CW_f(y) = \left(\sum_{j=0}^{d - 1} y_j \cdot f^{d - j}\right) \mod 2^{61} - 1, \]
</div>

<p>where \(f\in\mathbb{F}\) is the randomly chosen point at which we
evaluate the polynomial, and \(y\), the polynomial‚Äôs coefficients, is
the stream of 64-bit values obtained by splitting in half the <code>PH</code>
output for each block.  This choice saves 20-30 cycles of latency in
the final block, compared to CLHash: modular multiplications have
lower latency than carry-less multiplications for judiciously picked
machine-integer-sized moduli, and integer multiplications seem to mix
better, so we need less work in the finaliser.</p>

<p>Of course, UMASH sacrifices a lot of strength by working in
\(\mathbb{F} =\, \bmod 2^{61} - 1:\) the resulting field is much
smaller than \(2^{127}\), and we now have to update the polynomial
twice for the same number of blocks.  This means the collision
probability starts worse \((\approx 2^{-61}\) instead of \(\approx
2^{-127})\), and grows twice as fast with the number of blocks
\(n\) \((\approx 2n\cdot 2^{-61}\) instead of \(\approx n\cdot
2^{-61})\).  But remember, we‚Äôre only aiming for collision
probability \(&lt; 2^{-35}\) and each block represents 256 bytes of
input data, so this is acceptable, assuming that multi-gigabyte inputs
are out of scope.</p>

<p>We protect against <a href="https://en.wikipedia.org/wiki/Length_extension_attack">length extension collisions</a>
by <code>xor</code>ing (adding, in the polynomial ring) the original byte size of
the final block to its compressed <code>PH</code> output.  This <code>xor</code> is simpler
than CLHash‚Äôs finalisation step with a carry-less multiplication, but
still sufficient: we can adapt
<a href="http://krovetz.net/csus/papers/vmac.pdf#page=11">Krovetz‚Äôs proof for VHASH</a>
by replacing <code>NH</code>‚Äôs almost-\(\Delta\)-universality with <code>PH</code>‚Äôs
almost-XOR-universality.</p>

<p>Having this protection means we can extend short final blocks however
we want.  Rather than conceptually zero-padding our inputs (which adds
complexity and thus latency on short inputs), we allow redundant
reads.  We bifurcate inputs shorter than 16 bytes to a completely
different latency-optimised code path, and let the final <code>PH</code>
iteration read the last 16 bytes of the input, regardless of how
redundant that might be.</p>

<p>The
<a href="https://github.com/backtrace-labs/umash/blob/master/umash_reference.py">semi-literate Python reference implementation</a>
has the full code and includes more detailed analysis and rationale
for the design decisions.</p>

<h2 id="internal-implementation-tricks"><a id="implementation-tricks"></a>Internal implementation tricks</h2>

<p>The previous section already showed how we let micro-optimisation
inform the high-level structure of UMASH.  The use of <code>PH</code> over
<code>NH</code>, our choice of a polynomial hash in a small modular field, and
the way we handle short blocks all aim to improve the performance of
production implementations.  We also made sure to enable a couple more
implementation tricks with lower level design decisions.</p>

<p>The block size is set to 256 bytes because we observed diminishing
returns for larger blocks‚Ä¶ but also because it‚Äôs reasonable to cache
the <code>PH</code> loop‚Äôs parameters in 8 AVX registers, if we need to shave load
¬µops.</p>

<p>More importantly, it‚Äôs easy to implement a
<a href="https://en.wikipedia.org/wiki/Horner%27s_method">Horner update</a>
with the prime modulus \(2^{61} - 1\).  Better, that‚Äôs also true for a
‚Äúdouble-pumped‚Äù Horner update, \(h^\prime = H_f(h, a, b) = af + (b +
h)f^2.\)</p>

<p>The trick is to work in \(\bmod 2^{64} - 8 = \bmod 8\cdot(2^{-61} -
1),\) which lets us implement modular multiplication of an arbitrary
64-bit integer \(a\) by a multiplier \(0 &lt; f &lt; 2^{61} - 1\)
without worrying too much about overflow. \(2^{64} \equiv 8 \mod
2^{64} - 8,\) so we can reduce a value \(x\) to a smaller
representative with</p>

<div>
\[af = x \equiv 8\lfloor x / 2^{64}\rfloor + (x\bmod 2^{64}) \mod 2^{64} - 8;\]
</div>

<p>this equivalence is particularly useful when \(x &lt; 2^{125}\): in that
case, \(x / 2^{64} &lt; 2^{61},\) and the intermediate product
\(8\lfloor x / 2^{64}\rfloor &lt; 2^{64}\) never overflows 64 bits.
That‚Äôs exactly what happens when \(x = af\) is the product of
\(0\leq a &lt; 2^{64}\) and \(0 &lt; f &lt; 2^{61} - 1\).  This also
holds when we square the multiplier \(f\): it‚Äôs sampled from the
field \(\mathbb{Z}/(2^{61} - 1)\mathbb{Z},\) so its square also
satisfies \(f^2 &lt; 2^{61}\) once fully reduced.</p>

<p>Integer multiplication instructions for 64-bit values will naturally
split the product \(x = af\) in its high and low 64-bit half; we get
\(\lfloor x / 2^{64}\rfloor\) and \(x\bmod 2^{64}\) for free.  The
rest of the double-pumped Horner update is a pair of modular
additions, where only the final sum must be reduced to fit in \(\bmod
2^{64} - 8\).  The resulting instruction-parallel double Horner
update is only a few cycles slower than a single Horner update.</p>

<p>We also never fully reduce to \(\bmod 2^{61} - 1\). While the
collision bound assumes that prime field, we simply work in its \(\bmod
2^{64} - 8\) extension.  This does not affect the collision bound,
and the resulting expression is still amenable to algebraic
manipulation: modular arithmetic is a well defined ring even for
composite moduli.</p>

<h2 id="smhasher-tricks">SMHasher tricks</h2>

<p>A proof of almost-universality doesn‚Äôt mean a hash passes the SMHasher
test suite.  It should definitely guarantee collisions are (probably)
rare enough, but SMHasher also looks at bit avalanching and bias, and
universality is oblivious to these issues.  Even XOR- or
\(\Delta\)-universality doesn‚Äôt suffice: the hash values for a
given string are well distributed when parameters are chosen
uniformly at random, but this does not imply that hashes are always (or
usually) well distributed for fixed parameters.</p>

<p>The most stringent SMHasher tests focus on short inputs: mostly up to
128 or 256 bits, unless ‚ÄúExtra‚Äù torture testing is enabled.  In a way,
this makes sense, given that arbitrary-length string hashing is
provably harder than the bounded-length vector case.  Moreover, a
specialised code path for these inputs is beneficial, since they‚Äôre
relatively common and deserve strong and low-latency hashes.  That‚Äôs
why UMASH uses a completely different code path for inputs of at
most 8 bytes, and a specialised <code>PH</code> iteration for inputs of 9 to 16
bytes.</p>

<p>However, this means that SMHasher‚Äôs best avalanche and
bias tests often tell us very little about the general case.  For
UMASH, the medium length (9 to 16 bytes) code path at least
shares the same structure and finalisation logic as the code for
longer inputs.</p>

<p>There may also be a bit of co-evolution between the test harness and
the design of hash functions: the sort of <code>xorshift</code>/multiply mixers
favoured by Appleby in the various versions of MurmurHash tends to do
well on SMHasher.  These mixers are also invertible, so we can take
any hash function with good collision properties, mix its output with
someone else‚Äôs series of <code>xorshift</code> and multiplications (in UMASH‚Äôs
case, the
<a href="http://prng.di.unimi.it/splitmix64.c">SplitMix64 update function</a>
or a subset thereof), and usually find that the result satisfies
SMHasher‚Äôs bias and avalanche tests.</p>

<p>It definitely looks like interleaving rightward bitwise operations and
integer multiplications is a good mixing strategy.  However, I find it
interesting that the hash evaluation harness created by the author of
MurmurHash steers implementations towards MurmurHash-syle mixing code.</p>

<h2 id="fun-things-to-do-with-umash"><a id="usage"></a>Fun things to do with UMASH</h2>

<p>The structure of UMASH lets us support more sophisticated usage
patterns than merely hashing or fingerprinting an array of bytes.</p>

<p>The <code>PH</code> loop needs less than 17 bytes of state for its 16-byte
accumulator and an iteration count, and the polynomial hash also needs
17 bytes, for its own 8-byte accumulator, the 8-byte ‚Äúseed,‚Äù and a
counter for the final block size (up to 256 bytes).  The total comes up to
34 bytes of state, plus a 16-byte input buffer, since the <code>PH</code> loop
consumes 16-byte chunks at a time.  Coupled with the way we only
consider the input size at the end of UMASH, this makes it easy to
implement incremental hashing.</p>

<p>In fact, the state is small enough that our implementation stashes some
parameter data inline in the state struct, and uses the same
layout for hashing and fingerprinting with a pair of hashes (and thus
double the state): most of the work happens in <code>PH</code>, which only
accesses the constant parameter array, the shared input buffer and iteration
counter, and its private 16-byte accumulator.</p>

<p>Incremental fingerprinting is a crucial capability for our caching
system: cache keys may be large, so we want to avoid serialising them
to an array of contiguous bytes just to compute a fingerprint.
Efficient incrementality also means we can hash NUL-terminated C
strings with a fused UMASH / <code>strlen</code> loop, a nice speed-up when the
data is in cache.</p>

<p>The outer polynomial hash in UMASH is so simple to analyse that we
can easily process blocks out of order.  In my experience, such a
‚Äúparallel hashing‚Äù capability is more important than peak
throughput when checksumming large amounts of data coming over the
wire.  We usually maximise transfer throughput by asking for several
ranges of data in parallel.  Having to checksum these ranges in
order introduces a serial bottleneck and the usual head-of-line
blocking challenges; more importantly, checksumming in order adds
complexity to code that should be as obviously correct as possible.
The polynomial hash lets us hash an arbitrary subsequence of 256-byte
aligned blocks and use modular exponentiation to figure out its impact
on the final hash value, given the subsequence‚Äôs position in the
checksummed data.  Parallel hashing can exploit multiple cores (more
cores, more bandwidth!) with simpler code.</p>

<p>The <a href="https://tools.ietf.org/html/rfc4418">UMAC RFC</a> uses a Toeplitz
extension scheme to compute independent <code>NH</code> values while recycling
most of the parameters.  We do the same with <code>PH</code>, by adapting
<a href="https://web.cs.ucdavis.edu/~rogaway/umac/umac_thesis.pdf#page=51">Krovetz‚Äôs proof</a>
to exploit <code>PH</code>‚Äôs almost-XOR-universality instead of <code>NH</code>‚Äôs
almost-\(\Delta\)-universality.  Our fingerprinting code reuses all
but the first 32 bytes of <code>PH</code> parameters for the second hash: that‚Äôs
the size of an AVX register, which makes is trivial to avoid loading
parameters twice in a fused <code>PH</code> loop.</p>

<p>The same RFC also points out that concatenating the output of fast
hashes lets validation code decide which speed-security trade-off
makes sense for each situation: some applications may be willing to
only compute and compare half the hashes.</p>

<p>We use that freedom when reading from large hash tables keyed on
the UMASH fingerprint of strings.  We compute a single UMASH
hash value to probe the hash tables, and only hash the second half
of the fingerprint when we find a probable hit.  The idea is that
hashing the search key (now hot in cache) a second time will be faster
than comparing it against the hash entry‚Äôs string key in cold storage.</p>

<p>When we add this sort of trickery to our code base, it‚Äôs important to
make sure the interfaces are hard to misuse.  For example, it would be
unfortunate if only one half of the 128-bit fingerprint were well
distributed and protected against collisions: this would make it far
too easy to implement the two-step lookup-by-fingerprint above
<em>correctly but inefficiently</em>.  That‚Äôs why we maximise the symmetry in
the fingerprint: the two 64-bit halves are computed with the same
algorithm to guarantee the same worst-case collision probability and
distribution quality.  This choice leaves fingerprinting throughput on
the table when a weaker secondary hash would suffice.  However, I
prefer a safer if slightly slower interface to one ripe for silent
performance bugs.</p>

<h2 id="caveat-programmator">Caveat programmator</h2>

<p>While we intend for UMASH to become our default hash and fingerprint
function, it can‚Äôt be the right choice for every application.</p>

<p>First, it shouldn‚Äôt be used for authentication or similar
cryptographic purposes: the implementation is probably riddled with
side-channels, the function has no protection against parameter
extraction or adaptive attacks, and collisions are too frequent
anyway.</p>

<p>Obviously, this rules out using UMASH in a
<a href="https://en.wikipedia.org/wiki/Message_authentication_code">MAC</a>, but
might also be an issue for, e.g., hash tables where attackers control
the keys and can extrapolate the hash values.  A timing side-channel
may let attackers determine when keys collide; once a set of colliding
keys is known, the linear structure of UMASH makes it trivial to
create more collisions by combining keys from that set.  Worse,
iterating over the hash table‚Äôs entries can leak the hash values,
which would let an attacker slowly extract the parameters.  We
conservatively avoid non-cryptographic hashes and even hashed data
structures for sections of the <a href="https://backtrace.io">Backtrace</a> code
base where such attacks are in scope.</p>

<p>Second, the performance numbers reported by
<a href="https://github.com/rurban/smhasher">SMHasher</a> (up to 22 ns when
hashing 64 bytes or less, and 22 GB/s peak throughput) are probably a
lie for real applications, even when running on the exact same 2.5 GHz
Xeon 8175M hardware.  These are best-case values, when the code and
the parameters are all hot in cache‚Ä¶ and that‚Äôs a fair amount of
bytes for UMASH.  The instruction footprint for a 64-bit hash is 1435
bytes (comparable to heavier high-throughput hashes, like the
1600-byte <a href="https://github.com/Cyan4973/xxHash">xxh3_64</a> or 1350-byte
<a href="https://github.com/google/farmhash">farmhash64</a>), and the parameters
span 288 bytes (320 for a fingerprint).</p>

<p>There is a saving grace for UMASH and other complex hash functions:
the amount of bytes executed is proportional to the input size (e.g.,
the code for 8 or fewer byte only needs 141 bytes, and would inline to
around 100 bytes), and the number of parameters read is bounded by the
input length.  Although UMASH can need a lot of instruction and
parameter bytes, the worst case only happens for larger inputs, where
the cache misses can hopefully be absorbed by the work of loading and
hashing the data.</p>

<p>The numbers are also only representative of powerful CPUs with
carry-less multiplication in hardware.  The <code>PH</code> inner loop has 50%
higher throughput than <code>NH</code> (22 vs 14 GB/s) on contemporary Intel
servers.  The carry-less approach still has an edge over 128-bit
modular arithmetic on <a href="https://en.wikichip.org/wiki/amd/cores/naples">AMD‚Äôs Naples</a>,
but less so, around 20-30%. We did not test on ARM (the
<a href="https://help.backtrace.io/en/articles/2428859-web-console-overview">Backtrace database</a>
only runs on x86-64), but I would assume the situation there is closer
to AMD‚Äôs than Intel‚Äôs.</p>

<p>However, I also believe we‚Äôre more likely to observe improved
performance for <code>PH</code> than <code>NH</code> in future micro-architectures: the core
of <code>NH</code>, full-width integer multiplication, has been aggressively
optimised by now, while the gap between Intel and AMD shows there
may still be low-hanging fruits for the carry-less multiplications
at the heart of <code>PH</code>.  So, <code>NH</code> is probably already as good as
it‚Äôs going to be, but we can hope that <code>PH</code> will continue to benefit from
hardware optimisations, as chip designers improve the performance of
cryptographic algorithms like
<a href="https://en.wikipedia.org/wiki/Galois/Counter_Mode">AES-GCM</a>.</p>

<p>Third and last, UMASH isn‚Äôt fully stabilised yet.  We do not plan to
modify the high level structure of UMASH, a <code>PH</code> block compressor
that feeds into a polynomial string hash.  However, we are looking for
suggestions to improve its latency on short inputs, and to simplify
the finaliser while satisfying SMHasher‚Äôs distribution tests.</p>

<h2 id="help-us-improve-umash">Help us improve UMASH</h2>

<p>We believe UMASH is ready for non-persistent usage: we‚Äôre confident in
its quality, but the algorithm isn‚Äôt set in stone yet, so hash or
fingerprint values should not reach long-term storage.  We do not
plan to change anything that will affect the proof of collision
bound, but improvements to the rest of the code are more than
welcome.</p>

<p>In particular:</p>

<ol>
  <li>The short (8 or fewer bytes) input code can hopefully be simpler.</li>
  <li>The medium-length (9-15 bytes) input code path is a micro-optimised
version of the general case, but does not actually share any machine
code; can we improve the latency and maintain the collision bound
by replacing it with something completely different?</li>
  <li>It‚Äôs already nice that we can get away with a single round of
<code>xorshift</code> / multiply in the finaliser, but can we shave even
more latency there?</li>
  <li>We only looked at straightforward x86-64 implementations; we will
consider tweaks that improve performance on x86-64, or on
other platforms without penalising x86-64.</li>
  <li>We currently only use incremental and one-shot hashing interfaces.
If someone needs parallel hashing, we can collaborate to find out
what that interface could look like.</li>
</ol>

<p>A hash function is a perfect target for automated correctness and
performance testing.  I hope to use UMASH as a test bed for the
automatic evaluation (and approval?!) of pull requests.</p>

<p>Of course, you‚Äôre also welcome to just use
<a href="https://github.com/backtrace-labs/umash/blob/master/umash.c">UMASH as a single-file C library</a>
or re-implement it to fit your requirements.
<a href="https://github.com/backtrace-labs/umash">The MIT-licensed C code is on GitHub</a>,
and we can definitely discuss validation strategies for alternative
implementations.</p>

<p>Finally, our fingerprinting use case shows collision rates are
probably not something to minimise, but closer to soft constraints.
We estimate that, once the probability reaches \(2^{-70}\),
collisions are rare enough to only compare fingerprints instead of the
fingerprinted values.  However, going lower than \(2^{-70}\)
doesn‚Äôt do anything for us.</p>

<p>It would be useful to document other back-of-the-envelope requirements
for a hash function‚Äôs output size or collision rate.  Now that most
developers work on powerful 64-bit machines, it seems far too easy to
add complexity and waste resources for improved collision bounds that
may not unlock any additional application.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Any error in the analysis or the code is mine, but a few people
helped improve UMASH and its presentation.</p>

<p><a href="https://www.daemonology.net/blog/">Colin Percival</a> scanned an earlier
version of the reference implementation for obvious issues, encouraged
me to simplify the parameter generation process, and prodded us to
think about side channels, even in data structures.</p>

<p>Joonas Pihlaja helped streamline my initial attempt while making the
reference implementation easier to understand.</p>

<p><a href="https://github.com/jshufro">Jacob Shufro</a> independently confirmed
that he too found the reference implementation understandable, and
tightened the natural language.</p>

<p>Phil Vachon helped me gain more confidence in the implementation
tricks borrowed from VHASH after replacing the <code>NH</code> compression
function with <code>PH</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flatter wait-free hazard pointers]]></title>
    <link href="https://www.pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers/"/>
    <updated>2020-07-07T14:30:26-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers</id>
    <content type="html"><![CDATA[<p><small>2020-07-09: we can fix <a href="#hp_read_swf-relaxed">time-travel in <code>hp_read_swf</code></a> without changing the fast path. See <a href="#addendum-2020-07-09">the addendum</a>.</small></p>

<p>Back in February 2020, Blelloch and Wei submitted this cool preprint: <a href="https://arxiv.org/abs/2002.07053">Concurrent Reference Counting and Resource Management in Wait-free Constant Time</a>.
Their work mostly caught my attention because they propose a wait-free implementation of hazard pointers for safe memory reclamation.<sup id="fnref:but-also" role="doc-noteref"><a href="#fn:but-also" class="footnote">1</a></sup>
<a href="http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf">Safe memory reclamation (PDF)</a> is a key component in lock-free algorithms when garbage collection isn‚Äôt an option,<sup id="fnref:it-is-gc" role="doc-noteref"><a href="#fn:it-is-gc" class="footnote">2</a></sup>
and <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">hazard pointers (PDF)</a> let us bound the amount of resources stranded by delayed cleanups much more tightly than, e.g., <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation (PDF)</a>.
However the usual implementation has a <em>loop</em> in its <a href="https://www.iecc.com/gclist/GC-algorithms.html">read barriers (in the garbage collection sense)</a>,
which can be annoying for code generation and bad for worst-case time bounds.</p>

<p>Blelloch and Wei‚Äôs wait-free algorithm eliminates that loop‚Ä¶ with a construction that stacks <a href="https://arxiv.org/abs/1911.09671">two emulated primitives‚Äîstrong LL/SC, and atomic copy, implemented with the former‚Äî</a>on top of what real hardware offers.
I see the real value of the construction in proving that wait-freedom is achievable,<sup id="fnref:not-obvious" role="doc-noteref"><a href="#fn:not-obvious" class="footnote">3</a></sup>
and that the key is atomic memory-memory copies.</p>

<p>In this post, I‚Äôll show how to flatten down that abstraction tower into something practical with a bit of engineering elbow grease,
and come up with wait-free alternatives to the usual lock-free hazard pointers
that are competitive in the best case.
Blelloch and Wei‚Äôs insight that hazard pointers can use any wait-free atomic memory-memory copy lets us improve the worst case
without impacting the common case!</p>

<p>But first, what are hazard pointers?</p>

<h2 id="hazard-pointers-and-the-safe-memory-reclamation-problem">Hazard pointers and the safe memory reclamation problem</h2>

<p>Hazard pointers were introduced by <a href="https://dblp.uni-trier.de/pers/m/Michael:Maged_M=.html">Maged Michael</a>
in <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">Hazard Pointers: Safe Memory Reclamation for Lock-Free Objects (2005, PDF)</a>,
as the first solution to reclamation races in lock-free code.
The introduction includes a concise explanation of the safe memory reclamation (SMR) problem.</p>

<blockquote>
  <p>When a thread removes a node, it is possible that some other contending thread‚Äîin the course of its lock-free operation‚Äîhas earlier read a reference to that node, and is about to access its contents. If the removing thread were to reclaim the removed node for arbitrary reuse, the contending thread might corrupt the object or some other object that happens to occupy the space of the freed node, return the wrong result, or suffer an access error by dereferencing an invalid pointer value. [‚Ä¶] Simply put, the memory reclamation problem is how to allow the memory of removed nodes to be freed (i.e., reused arbitrarily or returned to the OS), while guaranteeing that no thread accesses free memory, and how to do so in a lock-free manner.</p>
</blockquote>

<p>In other words, a solution to the SMR problem lets us know when it‚Äôs safe to
<em>physically release</em> resources that used to be owned by a linked data structure,
once all links to these resources have been removed from that data structure (<a href="http://concurrencykit.org/presentations/ebr.pdf#page=8">after ‚Äúlogical deletion‚Äù</a>).
The problem makes intuitive sense for dynamically managed memory,
but it applies equally well to any resource (e.g., file descriptors),
and its solutions can even be seen as extremely read-optimised reader/writer locks.<sup id="fnref:RCU" role="doc-noteref"><a href="#fn:RCU" class="footnote">4</a></sup></p>

<p>The basic idea behind Hazard Pointers is to have
each thread publish to permanently allocated<sup id="fnref:stable-alloc" role="doc-noteref"><a href="#fn:stable-alloc" class="footnote">5</a></sup> hazard pointer records (HP records) the set of resources (pointers) it‚Äôs temporarily borrowing from a lock-free data structure.
That‚Äôs enough information for a background thread to snapshot the current list of resources that have been logically deleted but not yet physically released (the limbo list),
scan all records for all threads,
and physically release all resources in the snapshot that aren‚Äôt in any HP record.</p>

<p>With just enough batching of the limbo list, this scheme can be practical:
in practice, lock-free algorithms only need to pin a few (usually one or two) nodes at a time to ensure memory safety.  As long as we avoid running arbitrary code while holding hazardous references, we can bound the number of records each thread may need at any one time.
Scanning the records thus takes time roughly linear in the number of active threads, and we can amortise that to constant time per deleted item by waiting until the size of the limbo list is greater than a multiple of the number of active threads.<sup id="fnref:even-with-pinned-nodes" role="doc-noteref"><a href="#fn:even-with-pinned-nodes" class="footnote">6</a></sup></p>

<p>The tricky bit is figuring out how to reliably publish to a HP record without locking.
Hazard pointers simplify that challenge with three observations:</p>
<ol>
  <li>It‚Äôs OK to have arbitrary garbage in a record (let‚Äôs disregard language-level<sup id="fnref:or-hw-level" role="doc-noteref"><a href="#fn:or-hw-level" class="footnote">7</a></sup> undefined behaviour), since protected values are only ever subtracted from the limbo list: a garbage record simply doesn‚Äôt protect anything.</li>
  <li>It‚Äôs also OK to leave a false positive in a record: correctness arguments for hazard pointers already assume each record keeps a different node (resource) alive, and that‚Äôs the worst case.</li>
  <li>1 and 2 mean it doesn‚Äôt matter what pinned value we read in a record whose last update was started after we snapshotted the limbo list: resources in the limbo list are unreachable, so freshly pinned resources can‚Äôt refer to anything in the snapshot.</li>
</ol>

<p>This is where the clever bit of hazard pointers comes in:
we must make sure that any resource (pointer to a node, etc.) we borrow from a lock-free data structure is immediately protected by a HP record.
We can‚Äôt make two things happen atomically without locking.
Instead, we‚Äôll <em>guess</em><sup id="fnref:guessing-is-fine" role="doc-noteref"><a href="#fn:guessing-is-fine" class="footnote">8</a></sup> what resource we will borrow,
publish that guess,
and then actually borrow the resource.
If we guessed correctly, we can immediately use the borrowed resource;
if we were wrong, we must try again.</p>

<p>On an ideal <a href="https://en.wikipedia.org/wiki/Sequential_consistency">sequentially consistent</a> machine,
the pseudocode looks like the following.  The <code>cell</code> argument points to the resource we wish to acquire
(e.g., it‚Äôs a reference to the <code>next</code> field in a linked list node), and <code>record</code> is the hazard pointer
record that will protect the value borrowed from <code>cell</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_sc.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_sc</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="s2">&quot;Reads the resource (pointer) in `cell`, and protects it through `record`.&quot;</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span> <span class="o">=</span> <span class="n">guess</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load</span><span class="p">():</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>In practice, we must make sure that our write to <code>record.pin</code> is visible before re-reading the <code>cell</code>‚Äôs value, and we should also make sure the pointer read is ordered with respect to the rest of the calling read-side code.<sup id="fnref:beware-accidental-success" role="doc-noteref"><a href="#fn:beware-accidental-success" class="footnote">9</a></sup></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_explicit.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_explicit</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">        <span class="n">fence_store_load</span><span class="p">()</span>  <span class="c1"># R1</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">():</span> <span class="c1"># R2</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We need a store/load fence in <code>R1</code> to make sure the store to the record (just above <code>R1</code>) is visible by the time the second read (<code>R2</code>) executes.  Under the <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tphols.pdf">TSO memory model implemented by x86 chips (PDF)</a>,
this fence is the only one that isn‚Äôt implicitly satisfied by the hardware.
It also happens that fences are best implemented with atomic operations
on x86oids,
so we can eliminate the fence in <code>R1</code>
by replacing the store just before <code>R1</code> with an atomic exchange (fetch-and-set).</p>

<p>The slow cleanup path has its own fence that matches <code>R1</code> (the one in <code>R2</code>
matches the mutators‚Äô writes to <code>cell</code>).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_explicit.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_explicit</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We must make sure all the values in the limbo list we grab in <code>C1</code>
were added to the list (and thus logically deleted) before we read any
of the records in <code>C2</code>, with the  acquire read in <code>C1</code>
matching the store-load fence in <code>R1</code>.</p>

<p>It‚Äôs important to note that the cleanup loop does not implement
anything like an atomic snapshot of all the records.  The reclamation
logic is correct as long as we scan the correct value for records that
have had the same pinned value since before <code>C1</code>: we assume that a
resource only enters the limbo list once <em>all</em> its persistent
references have been cleared (in particular, this means circular
backreferences must be broken before scheduling a node for
destruction), so any newly pinned value cannot refer to any resource
awaiting destruction in the limbo list.<sup id="fnref:DEBRA-plus" role="doc-noteref"><a href="#fn:DEBRA-plus" class="footnote">10</a></sup></p>

<p>The following sequence diagrams shows how the fencing guarantees that any
iteration of <code>hp_read_explicit</code> will fail if it starts before <code>C1</code>
and observes a stale value.
If the read succeeds, the ordering between <code>R1</code> and <code>C1</code> instead
guarantees that the cleanup loop will observed the pinned value
when it reads the record in <code>C2</code>.</p>

<p><a href="https://sequencediagram.org/index.html#initialData=C4S2BsFMAIDFIHYGNIBNoAsCGAvLAndABwHsQFhJ8Aoaog0JEei6AJUi1SutS2CwAjLAGcYAWUgBbEvgCedBiCYtg0AOr4wVAPQBhKFgQBXIrU3b8+wyaIBaAHySZ8gFzQAqkT6VoKcODQAG5Y4MaQAPzUHFxUADx2zrJy7gDi4SIifpABwaHh0Zzc+I5JbtAACsaC4CAiGNBE5Aho0ADmGVnAJND4kEiyqNQWlFYGnLal0snuXj4w-oEhYZHUZXKOI7rjRqZp+ELQtVKCPbUiwNQAvFfwyDBsAIzQOtB6jzeFsfgJ6+4A8oIxPgggscoFTPMADS9SDAeTQEgIaCoEAAMzRa2m8k2WlG1gme2ggOBoOIzVaywKQA">
    <img src="/images/2020-07-07-flatter-wait-free-hazard-pointers/fenced-hp.png" />
</a></p>

<p>This all works, but it‚Äôs slow:<sup id="fnref:travis-says-its-fine" role="doc-noteref"><a href="#fn:travis-says-its-fine" class="footnote">11</a></sup> we added an <em>atomic</em> write instruction (or worse, a fence) to a read-only operation.  We can do better with a little help from our operating system.</p>

<h2 id="injecting-fences-with-os-support">Injecting fences with OS support</h2>

<p>When we use fences or memory ordering correctly, there should be
an implicit pairing between fences or ordered operations: we use fencing to
enforce an ordering (one must fully execute before or after another,
overlap is forbidden) between pairs of instructions in different
threads.  For example, the pseudocode for hazard pointers with
explicit fencing and memory ordering paired the store-load fence in
<code>R1</code> with the acquisition of the limbo list in <code>C1</code>.</p>

<p>We only need that pairing very rarely, when a thread actually executes the
cleanup function.  The amortisation strategy guarantees we don‚Äôt scan records all the time, and we can always increase the amortisation factor if we‚Äôre
generating tiny amounts of garbage very quickly.</p>

<p>It kind of sucks that we have to incur a full fence on the fast read
path, when it only matches reads in the cleanup loop maybe as rarely
as, e.g., once a second.  If we waited long enough on the slow path, we could
<a href="https://pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering">rely on events like preemption or other interrupts to insert a barrier</a>
in all threads that are executing the read-side.</p>

<p>How long is ‚Äúenough?‚Äù
Linux has the <a href="https://man7.org/linux/man-pages/man2/membarrier.2.html"><code>membarrier</code> syscall</a>
to block the calling thread until (more than) long enough has elapsed,
Windows has <a href="https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-flushprocesswritebuffers">the similar <code>FlushProcessWriteBuffers</code></a>, and
on other operating systems, we can probably <a href="https://github.com/pkhuong/barrierd">do something useful with scheduler statistics</a> or ask for a new syscall.</p>

<p>Armed with these new blocking system calls, we can replace the store-load fence in <code>R1</code> with a compiler barrier, and execute a slow <code>membarrier</code>/<code>FlushProcessWriteBuffers</code> after <code>C1</code>.
The cleanup function will then wait long enough<sup id="fnref:arguably-lock-ful" role="doc-noteref"><a href="#fn:arguably-lock-ful" class="footnote">12</a></sup> to ensure that any
read-side operation that had executed before <code>R1</code> at the time we read the limbo list in <code>C1</code> will be visible (e.g., because the operating system knows a preemption interrupt executed at least once on each core).</p>

<p>The pseudocode for this asymmetric strategy follows.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_membarrier.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_membarrier</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">        <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># R1</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">():</span> <span class="c1"># R2</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_membarrier.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_membarrier</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We‚Äôve replaced a fence on the fast read path with a compiler barrier, at the expense of executing a heavy syscall on the slow path.  That‚Äôs usually an advantageous trade-off, and is the <a href="https://github.com/facebook/folly/blob/master/folly/synchronization/Hazptr.h">preferred implementation strategy for Folly‚Äôs hazard pointers</a>.</p>

<p>The ability to pair mere <em>compiler</em> barriers with <code>membarrier</code>
syscalls opens the door for many more ‚Äúatomic enough‚Äù operations, not
just the fenced stores and loads we used until now:
similarly to the key idea in <a href="https://github.com/concurrencykit/ck/blob/master/include/ck_ec.h">Concurrency Kit‚Äôs atomic-free SPMC event count</a>,
we can use non-interlocked read-modify-write instructions,
since any interrupt (please don‚Äôt mention imprecise interrupts) will happen before or after any such instruction,
and never in the middle of an instruction.</p>

<p>Let‚Äôs use that to simplify wait-free hazard pointers.</p>

<h2 id="wait-free-hazard-pointers-with-interrupt-atomic-instructions">Wait-free hazard pointers with interrupt-atomic instructions</h2>

<p>The key insight that lets <a href="https://arxiv.org/abs/2002.07053">Blelloch and Wei</a>
achieve wait-freedom in hazard pointer is that the combination
of publishing a guess and confirming that the guess is correct in <code>hp_read</code>
emulates an atomic memory-memory copy.  Given such an atomic copy primitive, the read-side becomes trivial.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_blelloch_wei.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_membarrier</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">atomic_copy</span><span class="p">(</span><span class="n">cell</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The ‚Äúonly‚Äù problem is that atomic copies (which would look like locking all other cores out of memory accesses, copying the <code>cell</code>‚Äôs word-sized contents to <code>record.pin</code>, and releasing the lock) don‚Äôt exist in contemporary hardware.</p>

<p>However, we‚Äôve already noted that syscalls like <code>membarrier</code> mean we can weaken our requirements to interrupt atomicity. In other words, individual non-atomic instructions work since we‚Äôre assuming precise interrupts‚Ä¶ and <code>x86</code> and <code>amd64</code> do have an instruction for memory-memory copies!</p>

<p>The <a href="https://www.felixcloutier.com/x86/movs:movsb:movsw:movsd:movsq"><code>MOVS</code> instructions</a> are typically only used with a <code>REP</code> prefix.  However, they can also be executed without any prefix, to execute one iteration of the copy loop.  Executing a <code>REP</code>-free <code>MOVSQ</code> instruction copies one quadword (8 bytes) from the memory address in the source register <code>[RSI]</code> to the address in the destination register <code>[RDI]</code>, and advances both registers by 8 bytes‚Ä¶ and all this stuff happens in one instruction, so will never be split by an interrupt.
That‚Äôs an <em>interrupt</em>-atomic copy, which we can slot in place
of the software atomic copy in Blelloch and Wei‚Äôs proposal!</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_movs</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">x86</span><span class="o">.</span><span class="n">movs</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span>  <span class="c1"># R1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, the <code>MOVS</code> instruction is not atomic, but will be ordered with
respect to the <code>membarrier</code> syscall in <code>hp_cleanup_membarrier</code>: either
the copy fully executes before the <code>membarrier</code> in <code>C1</code>, in which case the
pinned value will be visible to the cleanup loop, or it executes after
the <code>membarrier</code>, which guarantees the copy will not observe a stale value
that‚Äôs waiting in the limbo list.</p>

<p>That‚Äôs just one instruction, but instructions aren‚Äôt all created
equal. <a href="https://uops.info/html-instr/MOVSQ.html"><code>MOVS</code> is on the heavy side</a>: in order to read from memory, write to memory, and increment two registers,
a modern Intel chip has to execute 5 micro-ops in at least ~5 cycles.
That‚Äôs not exactly fast; definitely better than an atomic (<code>LOCK</code>ed)
instruction, but not fast.</p>

<p>We can improve that with a trick from side-channel attacks, and
preserve wait-freedom.  We can usually guess what value we‚Äôll find in
<code>record.pin</code>, simply by reading <code>cell</code> with a regular relaxed load.
Unless we‚Äôre extremely unlucky (realistically, as long as the reader
thread isn‚Äôt interrupted), <code>MOVSQ</code> will copy the same value we just
guessed.  That‚Äôs enough to exploit branch prediction and turn a data
dependency on <code>MOVSQ</code> (a high latency instruction) into a data
dependency on a regular load <code>MOV</code> (low latency), and a highly
predictable control dependency.  In very low level pseudo code, this
‚Äúspeculative‚Äù version of the <code>MOVS</code> read-side might look like:</p>

<div id="hp_read_movs_spec">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs_spec.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_movs_spec</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">    <span class="n">x86</span><span class="o">.</span><span class="n">movs</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span>  <span class="c1"># R1</span>
</span><span class="line">    <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">:</span>  <span class="c1"># Likely</span>
</span><span class="line">        <span class="k">return</span> <span class="n">guess</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

At this point though, we might as well just read assembly.

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs_spec.s </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="s"><span class="line"><span></span>    <span class="c1"># rsi: cell, rdi: record.pin</span>
</span><span class="line">    <span class="c1"># rax: guess</span>
</span><span class="line">    mov    <span class="p">(</span><span class="o">%rsi),%</span>rax            <span class="c1"># guess = cell.load_relaxed()</span>
</span><span class="line">    movsq  <span class="o">%ds:(%</span>rsi<span class="p">),</span><span class="o">%es:(%</span>rdi<span class="p">)</span>  <span class="c1"># MOVS cell -&gt; record.pin</span>
</span><span class="line">    cmp    <span class="o">%rax,-0x8(%</span>rdi<span class="p">)</span>        <span class="c1"># guess == record.pin ?</span>
</span><span class="line">    jne    slow                   <span class="c1"># if !=, goto slow</span>
</span><span class="line">    retq                          <span class="c1"># return guess</span>
</span><span class="line">slow<span class="o">:</span>
</span><span class="line">    mov    <span class="m">-0</span>x8<span class="p">(</span><span class="o">%rdi),%</span>rax        <span class="c1"># ret = record.pin</span>
</span><span class="line">    retq                          <span class="c1"># return ret</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>We‚Äôll see that, in reasonable circumstances, this wait-free
code sequence is faster than the usual membarrier-based lock-free
read side.  But first, let‚Äôs see how we can achieve wait-freedom
when CISCy instructions like <code>MOVSQ</code> aren‚Äôt available, with an asymmetric ‚Äúhelping‚Äù scheme.</p>

<h2 id="interrupt-atomic-copy-with-some-help">Interrupt-atomic copy, with some help</h2>

<p>Blelloch and Wei‚Äôs wait-free atomic copy primitive builds on the usual
trick for wait-free algorithms: when a thread would wait for an
operation to complete, it helps that operation make progress instead
of blocking.  In this specific case, a thread initiates an atomic copy
by acquiring a fresh hazard pointer record, setting that descriptor‚Äôs
<code>pin</code> field to \(\bot\), publishing the address it wants to
copy from, and then performing the actual copy.  When another thread
enters the cleanup loop and wishes to read the record‚Äôs <code>pin</code> field , it may either find a value, or
\(\bot\); in the latter case, the cleanup thread has to help the
hazard pointer descriptor forward, by attempting to update the
descriptor‚Äôs <code>pin</code> field.</p>

<p>This strategy has the marked advantage of working.  However, it‚Äôs
also symmetric between the common case (the thread that initiated
the operation quickly completes it), and the worst case (a cleanup
thread notices the initiating thread got stuck and moves the
operation along).
This forces the common case to use atomic operations, similarly to the way cleanup threads would.
We pessimised the common case in order to eliminate blocking in the worst case, a frequent and unfortunate pattern in wait-free algorithms.</p>

<p>The source of that symmetry is our specification of an atomic copy
from the source field to a single destination <code>pin</code> field, which must
be written exactly once by the thread that initiated the copy
(the hazard pointer reader), or any concurrent helper (the cleanup loop).</p>

<p>We can relax that requirement, since we know that the hazard pointer
scanning loop can handle spurious or garbage pinned values.  Rather
than forcing both the read sequence (fast path) and the cleanup loop (slow path) to write to the same
<code>pin</code> field, we will give each HP record <em>two</em> pin fields: a
single-writer one for the fast path, and a multi-writer one for all
helpers (all threads in cleanup code).</p>

<p>The read-side sequence will have to first write to the HP record to
publish the cell it‚Äôs reading from, read and publish the cell‚Äôs pinned
value, and then check if a cleanup thread helped the record along.  If
the record was helped, the read-side sequence must use the value
written by the helping cleanup thread.  This means cleanup threads can
detect when a HP record is missing its pinned value, and help it along
with the cell‚Äôs current value.  Cleanup threads may later observe two
pinned values (both the reader and a cleanup thread wrote a pinned
value); in that case, both values are conservatively protected from
physical destruction.</p>

<p>Until now a hazard pointer record has only had one field, the ‚Äúpinned‚Äù
value.  We must add some complexity to make this asymmetric helping
scheme work: in order for cleanup threads to be able to help, we must publish
the cell we are reading, and we need somewhere for cleanup threads to write
the pinned value they read.  We also need some sort of <a href="https://en.wikipedia.org/wiki/ABA_problem">ABA protection</a>
to make sure slow cleanup threads don‚Äôt overwrite a fresher pinned value
with a stale one, when the <em>cleanup thread</em> gets stuck (preempted).</p>

<p>Concretely, the HP record still has a <code>pin</code> field, which is only
written by the reader that owns the record, and read by cleanup
threads.  The <code>help</code> subrecord is written by both the owner of the
record and any cleanup thread that might want to move a reader along.  The
reader will first write the address of the pointer it wants to read
and protect in <code>cell</code>, generate a new unique generation id by incrementing
<code>gen_sequence</code>, and write that to <code>pin_or_gen</code>.  We‚Äôll tag
generation ids with their sign: negative
values are generation ids, positive ones are addresses.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_record_wf.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="kt">intptr_t</span> <span class="n">gen_sequence</span> <span class="o">=</span> <span class="n">INTPTR_MIN</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">struct</span> <span class="n">hp_record_wf</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">void</span> <span class="o">*</span><span class="n">pin</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">void</span> <span class="o">**</span><span class="k">volatile</span> <span class="n">cell</span><span class="p">;</span>
</span><span class="line">                <span class="cm">/* -ve are gen, +ve are pinned pointers. */</span>
</span><span class="line">                <span class="k">volatile</span> <span class="kt">intptr_t</span> <span class="n">pin_or_gen</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span> <span class="n">help</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>At this point, any cleanup thread should be able to notice that the
<code>help.pin_or_gen</code> is a generation value, and find a valid cell address
in <code>help.cell</code>.  That‚Äôs all the information a cleanup threads needs to 
attempt to help the record move forward.  It can read the cell‚Äôs value, and publish
the pinned value it just read with an atomic compare-and-swap (CAS) of
<code>pin_or_gen</code>; if the CAS fails, another cleanup thread got there first, or
the reader has already moved on to a new target cell.  In the latter
case, any in-flight hazard pointer read sequence started before we
started reclaiming the limbo list, and it doesn‚Äôt matter what pinned
value we extract from the record.</p>

<p>Having populated the <code>help</code> subrecord, a reader can now publish a
value in <code>pin</code>, and then look for a pinned value in
<code>help.pin_or_gen</code>: if a cleanup thread published a pinned value there, the
reader must use it, and not the potentially stale (already destroyed)
value the reader wrote to <code>pin</code>.</p>

<p>On the read side, we obtain plain wait-freedom, with standard operations.
All we need are two compiler barriers to let
membarriers guarantee writes to the <code>help</code> subrecord are visible
before we start reading from the target cell, and to guarantee
that any cleanup thread‚Äôs write to <code>record.help.pin_or_gen</code> is visible
before we compare <code>record.help.pin_or_gen</code> against <code>gen</code>:</p>

<div id="hp_read_wf">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_wf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_wf</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">gen</span> <span class="o">=</span> <span class="n">gen_sequence</span>
</span><span class="line">    <span class="n">gen_sequence</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">cell</span><span class="p">)</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">store_release</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RA</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>  <span class="c1"># R2</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RB</span>
</span><span class="line">    <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span> <span class="o">!=</span> <span class="n">gen</span><span class="p">:</span>  <span class="c1"># Unlikely</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span>
</span><span class="line">    <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>On the cleanup side, we will consume the limbo list, issue a
membarrier to catch any read-side critical section that wrote to
<code>pin_or_gen</code> before we consumed the list, help these sections
along, issue another membarrier to guarantee that either the readers‚Äô
writes to <code>record.pin</code> are visible, or our writes to
<code>record.help.pin_or_gen</code> are visible to readers, and finally scan the
records while remembering to pin the union of <code>record.pin</code> and
<code>record.help.pin_or_gen</code> if the latter holds a pinned value.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_wf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_wf</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">gen</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="c1"># Help this record by populating its helped value.</span>
</span><span class="line">        <span class="k">if</span> <span class="n">gen</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="c1"># XXX: How do we know this is safe?!</span>
</span><span class="line">            <span class="n">value</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>
</span><span class="line">            <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">helped</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The membarrier in <code>C1</code> matches the compiler barrier in <code>RA</code>: if a
read-side section executed <code>R2</code> before we consumed the limbo list, its
writes to <code>record.help</code> must be visible.  The second membarrier in
<code>C2</code> matches the compiler barrier in <code>RB</code>: if the read-side section
has written to <code>record.pin</code>, that write must be visible, otherwise,
the cleanup threads‚Äôs write to <code>help.pin_or_gen</code> must be visible to the reader.
Finally, when scanning for pinned values, we can‚Äôt determine whether
the reader used its own value or the one we published, so we must
conservatively add both to the pinned set.</p>

<p>That‚Äôs a couple more instructions on the read-side than the
speculative <code>MOVSQ</code> implementation.  However, the instructions are
simpler, and the portable wait-free implementation benefits even more
from speculative execution: the final branch is equally predictable,
and now depends only on a read of <code>record.help.pin_or_gen</code>, which can
be satisfied by forwarding the reader‚Äôs own write to that same field.</p>

<p>The end result is that, in my microbenchmarks, this portable wait-free
implementation does slightly <em>better</em> than the speculative <code>MOVSQ</code> code.
We make this even tighter, by further specialising the code.  The cleanup
path is already slow.  What if we also assumed mutual exclusion; what if,
for each record, only one cleanup at a time could be in flight?</p>

<h2 id="interrupt-atomic-copy-with-at-most-one-helper">Interrupt-atomic copy, with at most one helper</h2>

<p>Once we may assume mutual exclusion between cleanup loops‚Äìmore specifically, the ‚Äúhelp‚Äù loop, the only part that writes to records‚Äìwe don‚Äôt
have to worry about ABA protection anymore.  Hazard pointer records
can lose some weight.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_record_swf.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">hp_record_swf</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">void</span> <span class="o">*</span><span class="n">pin</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="p">{</span>
</span><span class="line">                <span class="k">volatile</span> <span class="kt">intptr_t</span> <span class="n">cell_or_pin</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span> <span class="n">help</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We‚Äôll also use tagging with negative or positive values, this time to
distinguish target cell addresses (positive) from pinned values
(negative).  Now that the read side doesn‚Äôt have to update a
generation counter to obtain unique sequence values, it‚Äôs even simpler.</p>

<div id="hp_read_swf">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_swf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_swf</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">as_int</span><span class="p">())</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RA</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>  <span class="c1"># R2</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RB</span>
</span><span class="line">    <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Unlikely; &lt; cell.as_int() also works.</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">())</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span>
</span><span class="line">    <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>The cleanup function isn‚Äôt particularly different, except for the new
encoding scheme.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_swf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_swf</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="k">with</span> <span class="n">cleanup_lock</span><span class="p">:</span>
</span><span class="line">        <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">            <span class="n">cell</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">            <span class="k">if</span> <span class="n">cell</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="c1"># XXX: How do we know this is safe?!</span>
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">cell</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
</span><span class="line">                <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="o">-</span><span class="n">helped</span><span class="p">)</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><a href="https://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUQHYHMoFoDGUCGCCuAHaAdyzFQDMAnSGACywC8tKATafAe0WEkoCg++ZqHQghCYNABKkLC158WWYFgBGWAM4wAspAC2HSgE9BwkKPGSA6pTC8A9AGFsefAJt3KTlwVQA+XQNjAC5oAFV8JR5odEhwcGgANyxwXEg+GTlef0DDI1CABVxVcBANWmgVSiRISVj4vlzjfw8eL2dZV1CAcUo1aFK9VQ4BsuA+AF4JgDFIBFjpAEFoe2hHAEYpjNl5SgAeVCb86R2YuITk1PTM3Zz9PMLi0vL2RARINku0xvvjA9aHB0cARQgAZDhyM7xAD6hmh+EQfAB7R8+DuQWOjkWsCh4FhlHhiGg5EMJFYkxmcwWUgAQis1gAmLY3XgHI6hMJaXH4wkIaAgcj84AAcg00CwrwQPwxLVsbW8nRB0AA8qotJREh9Je9Pik0ho+EA">
    <img src="/images/2020-07-07-flatter-wait-free-hazard-pointers/swf-hp.png" />
</a></p>

<p>Again, <code>RA</code> matches <code>C1</code>, and <code>RB</code> <code>C2</code>.  This new implementation is simpler than <code>hp_read_wf</code> on the read side, and needs even fewer instructions.</p>

<h2 id="qualitative-differences-between-hp-implementations">Qualitative differences between HP implementations</h2>

<p>A key attribute for hazard pointers is how much they slow down pointer
traversal in the common case.  However, there are other qualitative
factors that should impact our choice of implementation.</p>

<p>The <a href="https://github.com/concurrencykit/ck/blob/master/include/ck_hp.h">classic fenced (<code>hp_read_explicit</code>) implementation</a>
needs one atomic or fence instruction per read, but does not require any
exotic OS operation.</p>

<p>A <a href="https://github.com/facebook/folly/blob/master/folly/synchronization/Hazptr.h">simple membarrier implementation (<code>hp_read_membarrier</code>)</a>
is ABI-compatible with the fenced implementations, but lets the read side
replace the fence with a compiler barrier, as long as the
slow cleanup path can issue <a href="https://man7.org/linux/man-pages/man2/membarrier.2.html"><code>membarrier</code> syscalls</a>
on Linux, or <a href="https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-flushprocesswritebuffers">the similar <code>FlushProcessWriteBuffers</code></a>
on Windows.  All the remaining implementations (we won‚Äôt mention the much more complex wait-free implementation of <a href="https://arxiv.org/abs/2002.07053">Blelloch and Wei</a>)
also rely on the same syscalls to avoid fences or atomic instructions
on the read side, while additionally providing wait-freedom (constant
execution time) for readers, rather than mere lock-freedom.</p>

<p>The simple <code>MOVSQ</code>-based implementations (<code>hp_read_movs</code>) is fully
compatible with <code>hp_read_membarrier</code>, wait-free, and usually compiles
down to fewer instruction bytes, but is slightly slower.  Adding
speculation (<code>hp_read_movs_spec</code>) retains compatibility and closes the
performance gap, with a number of instruction bytes comparable to the
lock-free membarrier implementation.  In both cases, we rely on
<code>MOVSQ</code>, an instruction that only exists on <code>x86</code> and <code>amd64</code>.</p>

<p>However, we can also provide portable wait-freedom, once we modify the
cleanup code to help the read side sections forward.  The basic
implementation <code>hp_read_wf</code> compiles to many more instructions than
the other read-side implementations, but those instructions are mostly
upstream of the protected pointer read; in microbenchmarks, the result
can even be faster than the simple <code>hp_read_membarrier</code> or the
speculative <code>hp_read_movs_spec</code>.  The downside is that instruction
bytes tend to hurt much more in real code than in microbenchmarks.
We also rely on pointer tagging, which could make the code less widely applicable.</p>

<p>We can simplify and shrink the portable wait-free code by assuming
mutual exclusion on the cleanup path (<code>hp_read_swf</code>).  Performance is
improved or roughly the same, and instruction bytes comparable to
<code>hp_read_membarrier</code>.  However, we‚Äôve introduced more opportunities
for reclamation hiccups.</p>

<p>More importantly, achieving wait-freedom with concurrent help suffers
from a fundamental issue: helpers don‚Äôt know that the pointer read they‚Äôre
helping move forward is stale until they (fail to) CAS into
place the value they just read.  This means they must be able to safely read potentially stale
pointers without crashing.  One might think mutual exclusion in the
cleanup function fixes that, but programs often mix and match
different reclamation schemes, as well as lock-free and lock-ful code.
On Linux, we could
abuse <a href="https://man7.org/linux/man-pages/man2/process_vm_readv.2.html">the <code>process_vm_readv</code> syscall</a>;<sup id="fnref:no-guarantees" role="doc-noteref"><a href="#fn:no-guarantees" class="footnote">13</a></sup>
in general I suppose we could install signal handlers to catch <code>SIGSEGV</code> and <code>SIGBUS</code>.</p>

<div id="hp_read_swf-relaxed"></div>

<p>The stale read problem is even worse for the single-cleanup <code>hp_read_swf</code> read sequence:
there‚Äôs no ABA protection, so a cleanup helper can pin an
old value in <code>record.help.cell_or_pin</code>.  This could happen if a
read-side sequence is initiated before <code>hp_cleanup_swf</code>‚Äôs <code>membarrier</code>
in <code>C1</code>, and the associated incomplete record is noticed by the
helper, at which point the helper is preempted.  The read-side
sequence completes, and later uses the same record to read from the
same address‚Ä¶ and that‚Äôs when the helper resumes execution, with a
<code>compare_exchange</code> that succeeds.</p>

<p>The pinned value ‚Äúhelped in‚Äù by <code>hp_cleanup_swf</code> is still valid‚Äîthe
call to <code>hp_cleanup_swf</code> hasn‚Äôt physically destroyed anything yet‚Äîso
the hazard pointer implementation is technically correct.  However,
this scenario shows that <code>hp_read_swf</code> can violate memory ordering and
causality, and even let long-overwritten values time travel into the future.  The
simpler read-side code sequence comes at a cost: its load is extremely
relaxed, much more so than any intuitive mental model might allow.<sup id="fnref:hybrid-swf" role="doc-noteref"><a href="#fn:hybrid-swf" class="footnote">14</a></sup></p>

<p>EDIT 2020-07-09: However, see <a href="#addendum-2020-07-09">this addendum</a> for a way to fix that race
without affecting the fast (read) path.</p>

<p>Having to help readers forward also loses a nice practical property of
hazard pointers: it‚Äôs always safe to spuriously consider arbitrary
(readable) memory as a hazard pointer record, it only costs us
additional conservatism in reclamation.  That‚Äôs not the case anymore,
once the cleanup thread has to help readers, and thus must write to HP
records.  This downside does not impact plain implementations of
hazard pointers, but does make it harder to improve record management
overhead by taking inspiration from managed language runtimes.</p>

<h2 id="some-microbenchmarks">Some microbenchmarks</h2>

<p>The overhead of hazard pointers only matters in code that traverse a
lot of pointers, especially pointer chains.  That‚Äôs why I‚Äôll focus on
microbenchmarking a loop that traverses a pseudo-randomly shuffled
circular linked list (embedded in an array of 1024 nodes, at 16
bytes per node) for a fixed number of pointer chasing hops.  You can
find the <a href="https://gist.github.com/pkhuong/5f3acc53b4f046f2717e645ecd504f7b">code to replicate the results in this gist</a>.</p>

<p>The unprotected (baseline) inner loop follows.  Note the <code>NULL</code>
end-of-list check, for realism; the list is circular, so the loop
never breaks early.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_baseline.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line"><span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>  <span class="err">#</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">=</span> <span class="n">head</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>There‚Äôs clearly a dependency chain between each read of <code>head-&gt;next</code>.
The call to <code>frob_ptr</code> lets us introduce work in the dependency chain,
which more closely represents realistic use cases.  For example, when
using hazard pointers to protect a binary search tree traversal, we
must perform a small amount of work to determine whether we want to go
down the left or the right subtree.</p>

<p>A hazard pointer-ed implementation of this loop would probably unroll
the loop body twice, to more easily implement hand-over-hand locking.
That‚Äôs why I also include an unrolled version of this inner loop in
the microbenchmark: we avoid discovering that hazard pointer
protection improves performance because it‚Äôs also associated with
unrolling, and gives us an idea of how much variation we can expect from
small code generation changes.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_unrolled.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line"><span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">||</span> <span class="o">++</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_hops</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The hazard pointer inner loops are just like the above, except that
<code>head = head-&gt;next</code> is replaced with calls to an inline function.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_hp.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="kr">inline</span> <span class="nf">__attribute__</span><span class="p">((</span><span class="n">__always_inline__</span><span class="p">,</span> <span class="n">__flatten__</span><span class="p">))</span> <span class="kt">uint64_t</span>
</span><span class="line"><span class="n">traverse_hp_generic</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">(</span><span class="o">*</span><span class="n">hp_read</span><span class="p">)(</span><span class="k">struct</span> <span class="n">hp_record</span> <span class="o">*</span><span class="p">,</span> <span class="kt">void</span> <span class="o">**</span><span class="p">),</span>
</span><span class="line">    <span class="kt">size_t</span> <span class="n">num_hops</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">start</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">static</span> <span class="k">struct</span> <span class="n">hp_record</span> <span class="n">backing</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">hp_record</span> <span class="o">*</span><span class="n">records</span> <span class="o">=</span> <span class="n">backing</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Let&#39;s pretend we published these records. */</span>
</span><span class="line">        <span class="k">asm</span> <span class="k">volatile</span><span class="p">(</span><span class="s">&quot;&quot;</span> <span class="o">:</span> <span class="s">&quot;+r&quot;</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">::</span> <span class="s">&quot;memory&quot;</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                        <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">hp_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">records</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">||</span> <span class="o">++</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_hops</span><span class="p">)</span>
</span><span class="line">                        <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">hp_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">records</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The dependency chain is pretty obvious; we can measure the sum of
latencies for 1000 pointer dereferences by running the loop 1000
times.  I‚Äôm using a large iteration count to absorb noise from the
timing harness (roughly 50 cycles per call), as well as any boundary
effect around the first and last few loop iterations.</p>

<p>All the cycle measurements here are from my unloaded E5-4617, running
at 2.9 GHz without Turbo Boost.  First, let‚Äôs see what happens with a
pure traversal, where <code>frob_ptr</code> is an inline no-op function
that simply return its first argument.  This microbenchmark is far
from realistic (if I found an inner loop that <em>only</em> traversed a
singly linked list, I‚Äôd consider a different data structure), but helps
establish an upper bound on the overhead from different hazard pointer
read sides.  I would usually show a faceted graph of the latency
distribution for the various methods‚Ä¶ but the results are so stable<sup id="fnref:did-not-randomize-codegen" role="doc-noteref"><a href="#fn:did-not-randomize-codegen" class="footnote">15</a></sup>
that I doubt there‚Äôs any additional information to be found in the graphs,
compared to the tables below.</p>

<p>The following table shows the cycle counts for following 1000 pointers in a
circular linked list, with various hazard pointer schemes and <em>no work to find the next node</em>, on an unloaded E5-4617 @ 2.9 GHz, without Turbo Boost.</p>

<pre><code>| Method * 1000 iterations  | p00.1 latency | median latency | p99.9 latency |
|---------------------------|---------------|----------------|---------------|
|  noop                     |            52 |             56 |            56 |
|  baseline                 |          4056 |           4060 |          4080 |
|  unrolled                 |          4056 |           4060 |          4080 |
|  hp_read_explicit         |         20136 |          20160 |         24740 |
|  hp_read_membarrier       |          5080 |           5092 |          5164 |
|  hp_read_movs             |         10060 |          10076 |         10348 |
|  hp_read_movs_spec        |          8568 |           8568 |          8572 |
|  hp_read_wf               |          6572 |           7620 |          8140 |
|  hp_read_swf              |          4268 |           4304 |          4368 |
</code></pre>

<p>The table above reports quantiles for the total runtime of 1000
pointer dereferences, after one million repetitions.</p>

<p>We‚Äôre looking at a baseline of 4 cycles/pointer dereference (the L1 cache
latency), regardless of unrolling.  The only implementation with an
actual fence or atomic operation, <code>hp_read_explicit</code> fares pretty
badly, at more than 5x the latency.  Replacing that fence with a
compiler barrier in <code>hp_read_membarrier</code> reduces the overhead to ~1
cycle per pointer dereference.  Our first wait-free implementation,
<code>hp_read_movs</code> (based on a raw <code>MOVSQ</code>) doesn‚Äôt do too great, with a
~6 cycle (150%) overhead for each pointer dereference.  However,
speculation (<code>hp_read_movs_spec</code>) does help shave that to ~4.5 cycles
(110%).  The portable wait-free implementation <code>hp_read_wf</code> does
slightly better, and its single-cleanup version <code>hp_read_swf</code> takes
the crown, by adding around 0.2 cycle/dereference.</p>

<p>These results are stable and repeatable, but still fragile, in a way:
except for <code>hp_read_explicit</code>, which is massively slowed down by its
atomic operation, and for <code>hp_read_movs</code>, which adds a known latency bump on the hot path, the other slowdowns mostly reflect contention for
execution resources.  In real life, such contention usually only occurs in
heavily tuned code, and the actual execution units (ports) in high
demand will vary from one inner loop to another.</p>

<p>Let‚Äôs see what happens when we insert a ~4-cycle latency slowdown
(three for the multiplication, and one more for the increment) in the
hot path, by redefining <code>frob_ptr</code>.  The result of the integer
multiplication by 0 is always 0, but adds a (non speculated) data
dependency on the node value and on the multiplication to the pointer
chasing dependency chain.  Only four cycles of work to decide which
pointer to traverse is on the low end of my practical experience, but
suffices to equalise away most of the difference between the hazard
pointer implementations.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>frob_ptr.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="o">*</span>
</span><span class="line"><span class="nf">frob_ptr</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">ptr</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">uintptr_t</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Make sure we get an actual MUL. */</span>
</span><span class="line">        <span class="k">asm</span><span class="p">(</span><span class="s">&quot;&quot;</span> <span class="o">:</span> <span class="s">&quot;+r&quot;</span><span class="p">(</span><span class="n">y</span><span class="p">));</span>
</span><span class="line">        <span class="k">return</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">y</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let‚Äôs again look at the quantiles for the cycle count for one million
loops of 1000 pointer dereferences, on an unloaded E5-4617 @ 2.9 GHz.</p>

<pre><code>| Method * 1000 iterations  | p00.1 latency | median latency | p99.9 latency |
|---------------------------|---------------|----------------|---------------|
|  noop                     |            52 |             56 |            56 |
|  baseline                 |         10260 |          10320 |         10572 |
|  unrolled                 |          9056 |           9060 |          9180 |
|  hp_read_explicit         |         22124 |          22156 |         26768 |
|  hp_read_membarrier       |         10052 |          10084 |         10264 |
|  hp_read_movs             |         12084 |          12112 |         15896 |
|  hp_read_movs_spec        |          9888 |           9940 |         10152 |
|  hp_read_wf               |          9380 |           9420 |          9672 |
|  hp_read_swf              |         10112 |          10136 |         10360 |
</code></pre>

<p>The difference between <code>unrolled</code> in this table and in the previous
one shows we actually added around 5 cycles of latency per iteration
with the multiplication in <code>frob_ptr</code>.  This dominates the overhead we
estimated earlier for all the hazard pointer schemes except for the
remarkably slow <code>hp_read_explicit</code> and <code>hp_read_movs</code>.  It‚Äôs thus not
surprising that all hazard pointer implementations but the latter
two are on par with the unprotected traversal loops (within 1.1 cycle
per pointer dereference, less than the impact of unrolling the loop
without unlocking any further rewrite).</p>

<p>The relative speed of the methods has changed, compared
to the previous table.  The speculative wait-free implementation
<code>hp_read_movs_spec</code> was slower than <code>hp_read_membarrier</code> and much
slower than <code>hp_read_swf</code>; it‚Äôs now slightly faster than both.
The simple portable wait-free implementation <code>hp_read_wf</code> was slower than
<code>hp_read_membarrier</code> and <code>hp_read_swf</code>; it‚Äôs now the fastest implementation.</p>

<p>I wouldn‚Äôt read too much into the relative rankings of
<code>hp_read_membarrier</code>, <code>hp_read_movs_spec</code>, <code>hp_read_wf</code>, and
<code>hp_read_swf</code>.  They only differ by fractions of a cycle per
dereference (all between 9.5 and 10.1 cycle/deref), and the exact values are a function of the
specific mix of micro-ops in the inner loop, and of the
near-unpredictable impact of instruction ordering on the chip‚Äôs
scheduling logic.  What really matters is that their impact
on traversal latency is negligible once the pointer chasing loop does <em>some</em>
work to find the next node.</p>

<h2 id="whats-the-best-hazard-pointer-implementation">What‚Äôs the best hazard pointer implementation?</h2>

<p>I hope I‚Äôve made a convincing case that hazard pointers can be
<em>wait-free and efficient</em> on the read-side, as long as we have access
to something like <code>membarrier</code> or <code>FlushProcessWriteBuffers</code> on the
slow cleanup (reclamation) path.  If one were to look at the
microbenchmarks alone, one would probably pick <code>hp_read_swf</code>.</p>

<p>However, the real world is more complex than microbenchmarks.  When I
have to extrapolate from microbenchmarks, I usually worry about the
hidden impact of instruction bytes or cold branches, since
microbenchmarks tend to fail at surfacing these things.  I‚Äôm not as
worried for <code>hp_read_movs_spec</code>, and <code>hp_read_swf</code>: they both compile
down to roughly as many instructions as the incumbent, <code>hp_read_membarrier</code>,
and their forward untaken branch would be handled fine by a static predictor.</p>

<p>What I would take into account is the ability to transparently use
<code>hp_read_movs_spec</code> in code that already uses <code>hp_read_membarrier</code>,
and the added requirements of <code>hp_read_swf</code>.  In
addition to relying on <code>membarrier</code> for correctness, <code>hp_read_swf</code>
needs a pointer tagging scheme to distinguish target pointers from pinned ones, a way for cleanup threads to read stale pointers without
crashing, and also imposes mutual exclusion around the scanning of (sets
of) hazard pointer records.  These additional requirements don‚Äôt seem
impractical, but I can imagine code bases where they would constitute
hard blockers (e.g., library code, or when protecting arbitrary integers).
Finally, <code>hp_read_swf</code> can let protected values time travel in the future,
with read sequences returning values so long after they were overwritten
that the result violates pretty much any memory ordering model‚Ä¶ unless
you implement the addendum below.</p>

<p>TL;DR: <a href="#hp_read_swf">Use <code>hp_read_swf</code></a> if you‚Äôre willing to sacrifice wait-freedom on the reclamation path <a href="#addendum-2020-07-09">and remember to implement the cleanup function with time travel protection</a>.  When targeting <code>x86</code> and <code>amd64</code>, <a href="#hp_read_movs_spec"><code>hp_read_movs_spec</code> is a well rounded option</a>, and still wait-free.  Otherwise, <a href="#hp_read_wf"><code>hp_read_wf</code> uses standard operations</a>, but compiles down to more code.</p>

<p>P.S., <a href="https://travisdowns.github.io/">Travis Downs</a> notes that mem-mem <code>PUSH</code> might be an alternative to <code>MOVSQ</code>, but that requires either pointing <code>RSP</code> to arbitrary memory, or allocating hazard pointers on the stack (which isn‚Äôt necessarily a bad idea).  Another idea worthy of investigation!</p>

<p><small>Thank you, Travis, for deciphering and validating a much rougher draft when the preprint dropped, and Paul and Jacob, for helping me clarify this last iteration.</small></p>

<div id="addendum-2020-07-09"></div>

<h2 id="addendum-2020-07-09-fix-time-travel-in-hp_read_swf">Addendum 2020-07-09: fix time travel in <code>hp_read_swf</code></h2>

<p>There is one huge practical issue with
<a href="#hp_read_swf"><code>hp_read_swf</code>, our simple and wait-free hazard pointer read sequence</a>
that sacrifices lock-free reclamation to avoid x86-specific instructions:
<a href="#hp_read_swf-relaxed">when the cleanup loop must help a record forward, it can fill in old values in ways that violate causality</a>.</p>

<p>I noted that the reason for this hole is the lack of ABA protection in
HP records‚Ä¶ and <a href="#hp_read_wf"><code>hp_read_wf</code> is what we‚Äôd get if we were to add full ABA protection</a>.</p>

<p>However, given mutual exclusion around the ‚Äúhelp‚Äù loop in the cleanup
function, we don‚Äôt need full ABA protection.  What we really need to
know is whether a given in-flight record we‚Äôre about to CAS forward is
the same in-flight record for which we read the cell‚Äôs value, or was
overwritten by a read-side section.  We can encode that by
stealing one more bit from the target <code>cell</code> address in <code>cell_or_pin</code>.</p>

<p>We already steal the sign bit to distinguish the address of the <code>cell</code>
to read (positive), from pinned values (negative).  The split make
sense because 64 bit architectures tend to reserve high (negative)
addresses for kernel space.  I doubt we‚Äôll see full 64 bit address
spaces for a while, so it seems safe to steal the next bit (bit 62) to
tag <code>cell</code> addresses.  The next table summarises the tagging scheme.</p>

<pre><code>0b00xxxx: untagged cell address
0b01xxxx: tagged cell address
0b1yyyyy: helped pinned value
</code></pre>

<p>At a high level, we‚Äôll change the <code>hp_cleanup_swf</code> to tag
<code>cell_or_pin</code> before reading the value pointed by <code>cell</code>, and only CAS in the
new pinned value if the cell is still tagged.
Thanks to mutual exclusion, we know <code>cell_or_pin</code> can‚Äôt be re-tagged by another thread.
Only the <code>for record in records</code> block has to change.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_swf_no_time_travel.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_swf_no_time_travel</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="k">with</span> <span class="n">cleanup_lock</span><span class="p">:</span>
</span><span class="line">        <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">            <span class="n">cell</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">            <span class="k">if</span> <span class="n">cell</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="k">continue</span>
</span><span class="line">            <span class="c1"># We assume valid addresses do not have that bit set.</span>
</span><span class="line">            <span class="k">if</span> <span class="p">(</span><span class="n">cell</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">62</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="k">continue</span>
</span><span class="line">            <span class="n">tagged</span> <span class="o">=</span> <span class="n">cell</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">62</span><span class="p">)</span>
</span><span class="line">            <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">tagged</span><span class="p">):</span>
</span><span class="line">                <span class="c1"># Compare exchange should act as a store-load barrier.</span>
</span><span class="line">                <span class="c1"># XXX: How do we know this load is safe?!</span>
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">cell</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
</span><span class="line">                <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">tagged</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="o">-</span><span class="n">helped</span><span class="p">)</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We doubled the number of atomic operations in the helping loop, but
that‚Äôs acceptable on the slow path.  We also rely on strong
<code>compare_exchange</code> (compare-and-swap) acting as a store-load fence.
If that doesn‚Äôt come for free, we could also tag records in one pass,
issue a store-load fence, and help tagged records in a
second pass.</p>

<p>Another practical issue with the <code>swf</code>/<code>wf</code> cleanup approach is that
they require <em>two</em> membarriers, and
<a href="/Blog/2019/01/09/preemption-is-gc-for-memory-reordering/#loaded-preemption-latency">OS-provided implementations can be slow, especially under load</a>.
This is particularly important for the <code>swf</code> approach, since mutual
exclusion means that one slow <code>membarrier</code> delays the physical destruction of everything on the limbo list.</p>

<p>I don‚Äôt think we can get rid of mutual exclusion, and, while
<a href="https://github.com/pkhuong/barrierd">we can improve membarrier latency</a>,
reducing the number of membarriers on the reclamation path is always good.</p>

<p>We can software pipeline calls to the cleanup function,
and use the same <code>membarrier</code> for <code>C1</code> and <code>C2</code> in two consecutive
cleanup calls.  Overlapping cleanups decreases the worst-case reclaim
latency from 4 membarriers to 3, and that‚Äôs not negligible when each
<code>membarrier</code> can block for 30ms.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:but-also" role="doc-endnote">
      <p>I also tend to read anything by <a href="https://dblp.uni-trier.de/pers/b/Blelloch:Guy_E=.html">Guy Blelloch</a>.¬†<a href="#fnref:but-also" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:it-is-gc" role="doc-endnote">
      <p>In fact, I‚Äôve often argued that SMR <em>is</em> garbage collection, just not fully tracing GC.  Hazard pointers in particular look a lot like deferred reference counting, <a href="https://web.eecs.umich.edu/~weimerw/2012-4610/reading/bacon-garbage.pdf">a form of tracing GC</a>.¬†<a href="#fnref:it-is-gc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-obvious" role="doc-endnote">
      <p>Something that wasn‚Äôt necessarily obvious until then. See, for example, <a href="https://arxiv.org/abs/2001.01999">this article presented at PPoPP 2020</a>, which conjectures that ‚Äúmaking the original Hazard Pointers scheme or epoch-based reclamation completely wait-free seems infeasible;‚Äù Blelloch was in attendance, so this must have been a fun session.¬†<a href="#fnref:not-obvious" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:RCU" role="doc-endnote">
      <p>The SMR problem is essentially the same problem as determining when it‚Äôs safe to return from <code>rcu_synchronize</code> or execute a <code>rcu_call</code> callback.  Hence, the same <a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix03/tech/freenix03/full_papers/arcangeli/arcangeli.pdf#page=5">reader-writer lock analogy</a> holds.¬†<a href="#fnref:RCU" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:stable-alloc" role="doc-endnote">
      <p>Hazard pointer records must still be managed separately, e.g., with a type stable allocator, but we can bootstrap everything else once we have a few records per thread.¬†<a href="#fnref:stable-alloc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:even-with-pinned-nodes" role="doc-endnote">
      <p>We can even do that without keeping track of the number of nodes that were previously pinned by hazard pointer records and kept in the limbo list: each record can only pin at most one node, so we can wait until the limbo list is, e.g., twice the size of the record set.¬†<a href="#fnref:even-with-pinned-nodes" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:or-hw-level" role="doc-endnote">
      <p>Let‚Äôs also hope efforts like <a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/">CHERI</a> don‚Äôt have to break lock-free code.¬†<a href="#fnref:or-hw-level" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:guessing-is-fine" role="doc-endnote">
      <p>The scheme is correct with any actual guess; we could even use a random number generator. However, performance is ideal (the loop exits) when we ‚Äúguess‚Äù by reading current value of the pointer we want to borrow safely.¬†<a href="#fnref:guessing-is-fine" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:beware-accidental-success" role="doc-endnote">
      <p>I tend to implement lock-free algorithms with a heavy dose of inline assembly, or <a href="http://concurrencykit.org/">Concurrency Kit</a>‚Äôs wrappers: it‚Äôs far too easy to run into subtle undefined behaviour.  For example, comparing a pointer after it has been freed is UB in C and C++, even if we don‚Äôt access the pointee.  Even if we compare as <code>uintptr_t</code>, it‚Äôs apparently debatable whether the code is well defined when the comparison happens to succeed because the pointee was freed, then recycled in an allocation and published to <code>cell</code> again.¬†<a href="#fnref:beware-accidental-success" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:DEBRA-plus" role="doc-endnote">
      <p>In <a href="http://www.cs.utoronto.ca/~tabrown/debra/fullpaper.pdf">Reclaiming Memory for Lock-Free Data Structures: There has to be a Better Way</a>, Trevor Brown argues this requirement is a serious flaw in all hazard pointer schemes.  I think it mostly means applications should be careful to coarsen the definition of resource in order to ensure the resulting condensed heap graph is a DAG.  In extreme cases, we end up proxy collecting an epoch, but we can usually do much better.¬†<a href="#fnref:DEBRA-plus" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:travis-says-its-fine" role="doc-endnote">
      <p>That‚Äôs what <a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html">Travis Downs classifies as a Level 1 concurrency cost</a>, which is usually fine for writes, but adds a sizable overhead to simple read-only code.¬†<a href="#fnref:travis-says-its-fine" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:arguably-lock-ful" role="doc-endnote">
      <p>I suppose this means the reclamation path isn‚Äôt wait-free, or even lock-free, anymore, in the strict sense of the words. In practice, we‚Äôre simply waiting for periodic events that would occur regardless of the syscalls we issue.  People who really know what they‚Äôre doing might have fully isolated cores.  If they do, they most likely have a watchdog on their isolated and latency-sensitive tasks, so we can still rely on running some code periodically, potentially after some instrumentation: if an isolated task fails to check in for a short while, the whole box will probably be presumed wedged and taken offline.¬†<a href="#fnref:arguably-lock-ful" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:no-guarantees" role="doc-endnote">
      <p>With the caveat that the public documentation for <code>process_vm_readv</code> does not mention any atomic load guarantee. In practice, I saw a long-by-long copy loop the last time I looked at the code, and I‚Äôm pretty sure the kernel‚Äôs build flags prevent GCC/clang from converting it to <code>memcpy</code>. We could rely on the strong ‚Äúdon‚Äôt break userspace‚Äù culture, but it‚Äôs probably a better idea to try and get that guarantee in writing.¬†<a href="#fnref:no-guarantees" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hybrid-swf" role="doc-endnote">
      <p>This problem feels like something we could address with a coarse epoch-based versioning scheme.  It‚Äôs however not clear to me that the result would be much simpler than <code>hp_read_wf</code>, and we‚Äôd have to steal even more bits (2 bits, I expect) from <code>cell_or_pin</code> to make room for the epoch.  EDIT 2020-07-09: it turns out <a href="#addendum-2020-07-09">we only need to steal one bit</a>.¬†<a href="#fnref:hybrid-swf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:did-not-randomize-codegen" role="doc-endnote">
      <p>Although I did compare several independent executions and confirmed the reported cycle counts were stable, I did not try to randomise code generation‚Ä¶ mostly because I‚Äôm not looking for super fine differences as much as close enough runtimes.  Hopefully, aligning functions to 256 bytes leveled some bias away.¬†<a href="#fnref:did-not-randomize-codegen" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Check for borrows in bitwise operations]]></title>
    <link href="https://www.pvk.ca/Blog/2020/05/02/check-for-borrows-in-bitwise-operations/"/>
    <updated>2020-05-02T17:24:39-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/05/02/check-for-borrows-in-bitwise-operations</id>
    <content type="html"><![CDATA[<p><small>2020-05-03: Had to add a complement step in the ULEB
section. Seems I couldn‚Äôt actually avoid that crummy-looking
notation. Spotted by redditor /u/Y_Less.</small></p>

<p>In the <a href="https://bits.houmus.org/2020-02-01/this-goes-to-eleven-pt4">fourth installment of his series on sorting with AVX2</a>,
<a href="https://twitter.com/damageboy">@damageboy</a> has a short aside where he
tries to detect partitioning (pivot) patterns where elements less than
and greater than or equal to the pivot are already in the correct
order: in that case, the partitioning routine does not need to permute
the block of values.  The practical details are irrelevant for this
post; what matters is that we wish to quickly identify whether a byte
value matches any of the follow nine cases:</p>

<ul>
  <li><code>0b11111111</code></li>
  <li><code>0b11111110</code></li>
  <li><code>0b11111100</code></li>
  <li><code>0b11111000</code></li>
  <li><code>0b11110000</code></li>
  <li><code>0b11100000</code></li>
  <li><code>0b11000000</code></li>
  <li><code>0b10000000</code></li>
  <li><code>0b00000000</code></li>
</ul>

<p>Looking at the bit patterns,<sup id="fnref:b-for-bit-literal" role="doc-noteref"><a href="#fn:b-for-bit-literal" class="footnote">1</a></sup> the OP‚Äôs solution with <a href="https://www.felixcloutier.com/x86/popcnt">popcount</a> and <a href="https://www.felixcloutier.com/x86/bsf">bitscan</a>
is pretty natural.  These instructions are somewhat complex (latency
closer to 3 cycles than 1, and often port restricted),
and it seems like the sort of problem that would have had efficient
solutions before SSE4 finally graced x86 with a <a href="https://en.wikipedia.org/wiki/Hamming_weight">population count</a> instruction.</p>

<p>In the context of a sorting library‚Äôs partition loop, <code>popcnt</code> and
<code>bsf</code> is probably more than good enough:
<a href="https://bits.houmus.org/2020-02-01/this-goes-to-eleven-pt4">the post shows that the real issue is branch mispredictions</a>
being slower than permuting unconditionally.
This is just a fun challenge to think about (:</p>

<h2 id="warm-up-is_power_of_two">Warm-up: <code>is_power_of_two</code></h2>

<p>Detecting whether a machine integer is a power of two (or zero) is
another task that has a straightforward solution in terms of popcount
or bitscan.  There‚Äôs also a simpler classic solution to this problem:</p>

<p><code>x == 0 || is_power_of_two(x) &lt;==&gt; (x &amp; (x - 1)) == 0</code></p>

<p>How does that expression work?  Say <code>x</code> is a power of two. Its binary
representation is <code>0b0...010...0</code>: any number of leading zeros,<sup id="fnref:big-endian" role="doc-noteref"><a href="#fn:big-endian" class="footnote">2</a></sup>
a single ‚Äú1‚Äù bit, and trailing zeros (maybe none).  Let‚Äôs see what happens when
we subtract 1 from <code>x</code>:</p>

<pre><code>x           = 0b00...0010...0
     x - 1  = 0b00...0001...1
x &amp; (x - 1) = 0b00...0000...0
</code></pre>

<p>The subtraction triggered a chain of <a href="https://en.wikipedia.org/wiki/Carry_(arithmetic)">borrows</a>
throughout the trailing zeros, until we finally hit that 1 bit.
In decimal, subtracting one from <code>10...0</code> yields <code>09...9</code>;
in binary we instead find <code>01...1</code>.
If you ever studied the circuit depth (latency) of carry chains
(for me, that was for circuit complexity theory), you know
that this is difficult to do well.
Luckily for us, <a href="https://en.wikipedia.org/wiki/Kogge%E2%80%93Stone_adder">chip makers work hard to pull it off</a>,
and we can just use carries as a data-controlled
primitive to efficiently flip ranges of bits.</p>

<p>When <code>x</code> is a power of two, <code>x</code> and <code>x - 1</code> have no ‚Äú1‚Äù bit in common,
so taking the bitwise <code>and</code> yields zero.  That‚Äôs also true when <code>x</code> is 0,
since <code>and</code>ing anything with 0 yields zero.  Let‚Äôs see what happens
for non-zero, non-power-of-two values <code>x = 0bxx...xx10..0</code>,
i.e., where <code>x</code> consists of an arbitrary non-zero sequence of bits <code>xx..xx</code>
followed by the least set bit (there‚Äôs at least one, since <code>x</code> is neither zero nor a power of two), and the trailing zeros:</p>

<pre><code>x           = 0bxx...xx10...0
     x - 1  = 0bxx...xx01...1
x &amp; (x - 1) = 0bxx...xx000000
</code></pre>

<p>The leading not-all-zero <code>0bxx...xx</code> is unaffected by the subtraction,
so it passes through the bitwise <code>and</code> unscathed (<code>and</code>ing any bit with
itself yields that same bit), and we know there‚Äôs at least one non-zero
bit in there; our test correctly rejects it!</p>

<h2 id="stretching-decoding-varints">Stretching: decoding varints</h2>

<p>When decoding variable length integers in <a href="https://en.wikipedia.org/wiki/LEB128#Unsigned_LEB128">ULEB</a>
format, e.g., for <a href="https://developers.google.com/protocol-buffers/docs/encoding">protocol buffers</a>,
it quickly becomes clear that, in order to avoid byte-at-a-time logic,
we must rapidly segment (<a href="https://en.wikipedia.org/wiki/Lexical_analysis">lex or tokenize</a>, in a way) our byte stream to determine where each ULEB
ends.  Let‚Äôs focus on the fast path, when the encoded ULEB fits in a
machine register.</p>

<p>We have <code>uleb = 0bnnnnnnnnmmmmmmmm...0zzzzzzz1yyyyyyy1...</code>:
a sequence of bytes<sup id="fnref:remember-endianness" role="doc-noteref"><a href="#fn:remember-endianness" class="footnote">3</a></sup> with the topmost bit equal to 1,
terminated by a byte with the top bit set to 0,
and, finally, arbitrary nuisance bytes (<code>m...m</code>, <code>n...n</code>, etc.) we wish to ignore.
Ideally, we‚Äôd extract <code>data = 0b0000000000000000...?zzzzzzz?yyyyyyy?...</code> from <code>uleb</code>: we want to clear the
nuisance bytes, and are fine with arbitrary values in the
ULEB‚Äôs control bits.</p>

<p>It‚Äôs much easier to find bits set to 1 than to zero, so the first thing to do is
to complement the <code>ULEB</code> data and
clear out everything but potential ULEB control bits (the high bit of
each byte), with <code>c = ~uleb &amp; (128 * (WORD_MAX / 255))</code>, i.e.,
compute the bitwise <code>and</code> of <code>~uleb</code> with a bitmask of the high bit in each byte.</p>

<pre><code>   uleb = 0bnnnnnnnnmmmmmmmm...0zzzzzzz1yyyyyyy1...
  ~uleb = 0bÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖ...1zÃÖzÃÖzÃÖzÃÖzÃÖzÃÖz0yÃÖyÃÖyÃÖyÃÖyÃÖyÃÖy0...
      c = 0bÃÖnÃÖ0000000ÃÖmÃÖ0000000...10000000000000000...
</code></pre>

<p>We could now bitscan to find the index of the first 1 (marking the
last ULEB byte), and then generate a mask.  However, it seems wasteful to
generate an index with a scan, only to convert it back into bitmap
space with a shift.  We‚Äôll probably still want that index to know how
far to advance the decoder‚Äôs cursor, but we can hopefully update the
cursor in parallel with decoding the current ULEB value.</p>

<p>When we were trying to detect powers of two, we subtracted <code>1</code> from
<code>x</code>, a value kind of like <code>c</code>, in order to generate a new value
that differed from <code>x</code> in all the bits up to and including the first
set (equal to <code>1</code>) bit of <code>x</code>, and identical in the remaining bits.  We
then used the fact that <code>and</code>ing a bit with itself yields that same
bit to detect whether there was any non-zero bit in the remainder.</p>

<p>Here, we wish to do something else with the remaining untouched bits, we
wish to set them all to zero.  Another bitwise operator does
what we want: <code>xor</code>ing a bit with itself always yields zero, while
<code>xor</code>ing bits that differ yields <code>1</code>.  That‚Äôs the plan for ULEB. We‚Äôll
subtract 1 from <code>c</code> and <code>xor</code> that back with <code>c</code>.</p>

<pre><code>       uleb = 0bnnnnnnnnmmmmmmmm...0zzzzzzz1yyyyyyy1...
      ~uleb = 0bÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖnÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖmÃÖ...1zÃÖzÃÖzÃÖzÃÖzÃÖzÃÖz0yÃÖyÃÖyÃÖyÃÖyÃÖyÃÖy0...
c           = 0bÃÖnÃÖ0000000ÃÖmÃÖ0000000...10000000000000000...
     c - 1  = 0bÃÖnÃÖ0000000ÃÖmÃÖ0000000...01111111111111111...
c ^ (c - 1) = 0b0000000000000000...11111111111111111...
</code></pre>

<p>We now just have to bitwise <code>and</code> <code>uleb</code> with <code>c ^ (c - 1)</code>
to obtain the bits of the first <code>ULEB</code> value in <code>uleb</code>, while
overwriting everything else with 0.  Once we have that, we can either
<a href="https://www.felixcloutier.com/x86/pext">extract data bits with <code>PEXT</code></a>
on recent Intel chips, or otherwise dust off interesting stunts for <a href="https://en.wikipedia.org/wiki/SWAR">SWAR</a> shifts by variable amounts.</p>

<h2 id="now-for-damageboys-problem">Now for <a href="https://bits.houmus.org/2020-02-01/this-goes-to-eleven-pt4">damageboy</a>‚Äôs problem</h2>

<p>Let‚Äôs first repeat the question that motivated this post.  We want to detect when a byte <code>p</code> is one of the following nine values:</p>

<ul>
  <li><code>0b11111111</code></li>
  <li><code>0b11111110</code></li>
  <li><code>0b11111100</code></li>
  <li><code>0b11111000</code></li>
  <li><code>0b11110000</code></li>
  <li><code>0b11100000</code></li>
  <li><code>0b11000000</code></li>
  <li><code>0b10000000</code></li>
  <li><code>0b00000000</code></li>
</ul>

<p>These bit patterns feel similar to those for power of two bytes: if we
complement the bits, these values are all 1 less than a power of two
(or -1, one less than zero).  We already know how to detect when a
value <code>x</code> is zero or a power of two (<code>x &amp; (x - 1) == 0</code>), so it‚Äôs easy
to instead determine whether <code>~p</code> is one less than zero or a power of
two: <code>(~p + 1) &amp; ~p == 0</code>.</p>

<p>This is already pretty good: bitwise <code>not</code> the byte <code>p</code>,
and check if it‚Äôs one less than zero or a power of two (three simple
instructions on the critical path).  We can do better.</p>

<p>There‚Äôs another name for <code>~p + 1</code>, i.e., for bitwise complementing a value and
adding one: that‚Äôs simply <code>-p</code>, the additive inverse of <code>p</code> in two‚Äôs
complement!  We can use <code>-p &amp; ~p == 0</code>.  That‚Äôs one fewer
instruction on the critical path of our dependency graph (down to two, since we can <a href="https://www.felixcloutier.com/x86/test"><code>test</code> whether <code>and</code>ing yields zero</a>), and still only
uses simple instructions that are unlikely to be port constrained.</p>

<p>Let‚Äôs check our logic by enumerating all byte-sized values.</p>

<pre><code>CL-USER&gt; (dotimes (p 256)
           (when (zerop (logand (- p) (lognot p) 255))
             (format t "0b~2,8,'0r~%" p)))
0b00000000
0b10000000
0b11000000
0b11100000
0b11110000
0b11111000
0b11111100
0b11111110
0b11111111
</code></pre>

<p>These <em>are</em> the bytes we‚Äôre looking for (in ascending rather
than descending order)!</p>

<h2 id="remember-the-power-of-borrows">Remember the power of borrows</h2>

<p>I hope the examples above communicated a pattern I often observe when
mangling bits: operations that are annoying (not hard, just a bit more
complex than we‚Äôd like) in the bitmap domain can be simpler in two‚Äôs
complement arithmetic.  Arithmetic operations are powerful mutators
for bitmaps, but they‚Äôre often hard to control.  Subtracting or adding
1 are the main exceptions: it‚Äôs easy to describe their impact in terms
of the low bits of the bitmap.  In fact, we can extend that trick to
subtracting or adding powers of two: it‚Äôs the same carry/borrow chain effect as for 1,
except that bits smaller than the power of two pass straight
through‚Ä¶
which might be useful when we expect a known tag followed by a ULEB value that must be decoded.</p>

<p>If you find yourself wishing for a way to flip ranges of bits in a
data-dependent fashion, it‚Äôs always worth considering the two‚Äôs
complement representation of the problem for a couple minutes.  Adding
or subtracting powers of two doesn‚Äôt always work, but the payoff is
pretty good when it does.</p>

<p>P.S., <a href="http://0x80.pl/notesen/2016-10-16-detecting-bit-pattern.html">Wojciech Mu≈Ça offers a different 3-operation sequence with <code>-p</code></a>
to solve damageboy‚Äôs problem.
That‚Äôs another nice primitive to generate bitmasks dynamically.</p>

<p><small>Thank you Ruchir for helping me clarify the notation around the ULEB section.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:b-for-bit-literal" role="doc-endnote">
      <p>I use the <code>0b...</code> syntax throughout this post to denote bit literals, similarly to the usual <code>0x...</code> hexadecimal literals.¬†<a href="#fnref:b-for-bit-literal" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:big-endian" role="doc-endnote">
      <p>While I prefer to work with little-endian bytes, I find everything makes more sense with big-endian bits.¬†<a href="#fnref:big-endian" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:remember-endianness" role="doc-endnote">
      <p>Remember, while ULEB is little-endian, we use big bit-endianness.¬†<a href="#fnref:remember-endianness" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How hard is it to guide test case generators with branch coverage feedback?]]></title>
    <link href="https://www.pvk.ca/Blog/2020/03/11/how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/"/>
    <updated>2020-03-11T18:00:26-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/03/11/how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback</id>
    <content type="html"><![CDATA[<p>To make a long story short, it‚Äôs almost easy,
now that <a href="https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/intel-bts.txt">tracing</a> is available on commodity hardware,
especially with a <a href="https://gist.github.com/pkhuong/1ce34e33c6df4b9be3bc9beb22415a47">small library to handle platform-specific setup</a>.</p>

<p>Here‚Äôs a graph of the empirical distribution functions
for the number of calls into <a href="https://github.com/google/fuzzer-test-suite/tree/7e08b745d1e264eedb9a91b1fd0a931f1bec3b83/openssl-1.0.1f">OpenSSL 1.0.1f</a>
it takes for <a href="https://hypothesis.works/">Hypothesis</a>
to find <a href="https://en.wikipedia.org/wiki/Heartbleed">Heartbleed</a>,
given a description of the <a href="https://tools.ietf.org/html/rfc6520">format for Heartbeat requests</a> (‚Äúgrammar‚Äù),
and the same with additional branch coverage feedback (‚Äúbts‚Äù, for Intel Branch Trace Store).</p>

<!-- ggplot(data, aes(x=Calls, colour=Impl)) + stat_ecdf() + scale_color_discrete(name="Implementation") + scale_x_log10() + ylab("Cumulative fraction") + xlab("Calls to OpenSSL until crash") -->
<p><img class="center" src="/images/2020-03-11-how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/ecdf.png" /></p>

<p>On average, knowing the grammar for Heartbeat requests lets Hypothesis find
the vulnerability after 535 calls to OpenSSL;
when we add branch coverage feedback, the average goes down to 473<sup id="fnref:realtime" role="doc-noteref"><a href="#fn:realtime" class="footnote">1</a></sup> calls, 11.5% faster.
The plot shows that,
as long as we have time for more than 100 calls to OpenSSL,
branch coverage information makes it more likely that Hypothesis will find Heartbleed
for every attempt budget.
I‚Äôll describe this experiment in more details later in the post;
first, why did I ask that question?</p>

<h2 id="enumerating-test-cases-by-hand-is-a-waste-of-peoples-time">Enumerating test cases by hand is a waste of people‚Äôs time</h2>

<p>I care about efficiently generating test cases because
I believe that most of the time people spend writing, reviewing, and maintaining
classic <a href="https://testing.googleblog.com/2010/12/test-sizes.html">small or medium tests</a>
is misallocated.
There‚Äôs a place for them, same as there is for manual QA testing‚Ä¶ but their returns curve is strongly concave.</p>

<p>Here are the sort of things I want to know when I look at typical unit test code:</p>
<ol>
  <li>Why do we make this exact assertion; what makes a result acceptable?<sup id="fnref:matchers" role="doc-noteref"><a href="#fn:matchers" class="footnote">2</a></sup></li>
  <li>Why do we feed this or that input; what makes input data useful or interesting?</li>
  <li>What is expected to change when I tweak the external behaviour of the system under test;<sup id="fnref:change-detector" role="doc-noteref"><a href="#fn:change-detector" class="footnote">3</a></sup> how are answers to the previous two questions encoded?</li>
</ol>

<p>As programmers, I think it‚Äôs natural to say that,
if we need to formalize how we come up with assertions, or how we decide what inputs to use during testing,
we should express that in code.
But once we have that code, why would we enumerate test cases manually?
That‚Äôs a job for a computer!</p>

<h2 id="property-based-testing-at-backtrace"><a href="https://increment.com/testing/in-praise-of-property-based-testing/">Property-based testing</a> at Backtrace</h2>

<p>I have such conviction in this thesis that,
as soon as I took over the query processing component (a library written in C with a dash of intrinsics and inline assembly) at <a href="https://backtrace.io">Backtrace</a>,
I introduced a new testing approach based on <a href="https://hypothesis.readthedocs.io/en/latest/index.html">Hypothesis</a>.
The query engine was always designed and built as a discrete library,
so it didn‚Äôt take too much work to disentangle it from other pieces of business logic.
That let us quickly add coverage for the external interface (partial coverage, this is still work
in progress), as well as for key internal components that we chose to expose for testing.</p>

<p>Along the way, I had to answer a few reasonable questions:</p>

<ol>
  <li>What is Hypothesis?</li>
  <li>Why Hypothesis?</li>
  <li>Wait, there are multiple language backends for Hypothesis, but none for C or C++; how does that work?</li>
</ol>

<h3 id="what-is-hypothesis">What is Hypothesis?</h3>
<p><a href="https://hypothesis.works/">Hypothesis</a> is a Python library that helps programmers generate test cases programmatically,
and handles a lot of the annoying work involved in making such generators practical to use.
Here‚Äôs an excerpt from real test code for our vectorised operations on arrays of 64-bit integers.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_u64_block.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">given</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">check_u64_bitmask</span><span class="p">(</span><span class="n">booleans</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Given a list of 8 booleans, generates the corresponding array of</span>
</span><span class="line"><span class="sd">    u64 masks, and checks that they are correctly compressed to a u8</span>
</span><span class="line"><span class="sd">    bitmask.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">expected</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">booleans</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="n">value</span><span class="p">:</span>
</span><span class="line">            <span class="n">expected</span> <span class="o">|=</span> <span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">i</span>
</span><span class="line">    <span class="n">masks</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s2">&quot;8Q&quot;</span><span class="p">,</span> <span class="o">*</span><span class="p">[</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">64</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">boolean</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">boolean</span> <span class="ow">in</span> <span class="n">booleans</span><span class="p">])</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">expected</span> <span class="o">==</span> <span class="n">C</span><span class="o">.</span><span class="n">u64_block_bitmask</span><span class="p">(</span><span class="n">FFI</span><span class="o">.</span><span class="n">from_buffer</span><span class="p">(</span><span class="s2">&quot;uint64_t[]&quot;</span><span class="p">,</span> <span class="n">masks</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">TestU64BlockOp</span><span class="p">(</span><span class="n">TestCase</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@given</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">(),</span> <span class="n">min_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">8</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">test_u64_block_bitmask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bits</span><span class="p">):</span>
</span><span class="line">        <span class="n">check_u64_bitmask</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>A key component of these operations is a C (SSE intrinsics, really) function that accepts eight 64-bit masks as four 128-bit SSE registers (one pair of masks per register),
and returns a byte, with one bit per mask.
In order to test that logic, we expose <code>u64_block_bitmask</code>, a wrapper that accepts an array of eight masks
and forwards its contents to the mask-to-bit function.</p>

<p><img class="center" src="/images/2020-03-11-how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/reduction.jpg" /></p>

<p><code>check_u64_bitmask</code> calls that wrapper function from Python, and asserts that its return value is as expected: 1 for each mask that‚Äôs all 1s, and 0 for each mask that‚Äôs all 0s.
We turn <code>check_u64_bitmask</code> into a test cases generator
by annotating the test method <code>test_u64_block_bitmask</code> with a <a href="https://hypothesis.readthedocs.io/en/latest/details.html#hypothesis.given"><code>@given</code> decorator</a>
that asks Hypothesis to generate arbitrary lists of eight booleans.</p>

<p>Of course, there are only 256 lists of eight booleans; we should test that exhaustively.
Hypothesis has the <a href="https://hypothesis.readthedocs.io/en/latest/reproducing.html#hypothesis.example"><code>@example</code> decorator</a>,
so it‚Äôs really easy to implement our own Python decorator to apply
<code>hypothesis.example</code> once for each element in an iterable.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_u64_block.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">all_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Applies a list of hypothesis.examples.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">apply_examples</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
</span><span class="line">        <span class="k">for</span> <span class="n">ex</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">examples</span><span class="p">)):</span>
</span><span class="line">            <span class="n">fn</span> <span class="o">=</span> <span class="n">example</span><span class="p">(</span><span class="n">ex</span><span class="p">)(</span><span class="n">fn</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">apply_examples</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">extract_bits</span><span class="p">(</span><span class="n">number</span><span class="p">,</span> <span class="n">num_bits</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Converts the num_bits bit number to a list of booleans.&quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">return</span> <span class="p">[(</span><span class="n">number</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">shift</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">shift</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_bits</span><span class="p">)]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">TestU64BlockOp</span><span class="p">(</span><span class="n">TestCase</span><span class="p">):</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@all_examples</span><span class="p">(</span><span class="n">extract_bits</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">256</span><span class="p">))</span>
</span><span class="line">    <span class="nd">@given</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">(),</span> <span class="n">min_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_size</span><span class="o">=</span><span class="mi">8</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">test_u64_block_bitmask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bits</span><span class="p">):</span>
</span><span class="line">        <span class="n">check_u64_bitmask</span><span class="p">(</span><span class="n">bits</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>That‚Äôs pretty cool, but we could enumerate that more efficiently in C.
Where Hypothesis shines is in reporting easily actionable failures.
We‚Äôre wasting computer time in order to save developer time, and that‚Äôs usually a reasonable trade-off.
For example, here‚Äôs what Hypothesis spits back via pytest when I introduce a bug to ignore the
last pair of masks, masks 6 and 7, and instead reuse the register that holds masks 4 and 5 (a mistake that‚Äôs
surprisingly easy to make when writing intrinsics by hand).</p>

<p><img class="center" src="/images/2020-03-11-how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/bad-reduction.jpg" /></p>

<pre><code>booleans = [False, False, False, False, False, False, ...]

    def check_u64_bitmask(booleans):
        """Given a list of 8 booleans, generates the corresponding array of
        u64 masks, and checks that they are correctly compressed to a u8
        bitmask.
        """
        expected = 0
        for i, value in enumerate(booleans):
            if value:
                expected |= 1 &lt;&lt; i
        masks = struct.pack("8Q", *[2 ** 64 - 1 if boolean else 0 for boolean in booleans])
&gt;       assert expected == C.u64_block_bitmask(FFI.from_buffer("uint64_t[]", masks))
E       AssertionError: assert 128 == 0
E         -128
E         +0

test_u64_block.py:40: AssertionError
-------------------------------------- Hypothesis ----------------------------------------------
Falsifying example: test_u64_block_bitmask(
    self=&lt;test_u64_block.TestU64BlockOp testMethod=test_u64_block_bitmask&gt;,
    bits=[False, False, False, False, False, False, False, True],
)
</code></pre>

<p>Hypothesis finds the bug in a fraction of a second,
but, more importantly, it then works harder to report a minimal counter-example.<sup id="fnref:unless-examples" role="doc-noteref"><a href="#fn:unless-examples" class="footnote">4</a></sup>
With all but the last mask set to 0,
it‚Äôs easy to guess that we‚Äôre probably ignoring the value of
the last mask (and maybe more), which would be why we found a bitmask of
0 rather than 128.</p>

<h3 id="why-hypothesis">Why Hypothesis?</h3>
<p>So far this is all regular property-based testing,
with a hint of more production-readiness than we‚Äôve come to expect from clever software correctness tools.
What really sold me was <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html">Hypothesis‚Äôs stateful testing</a>
capability, which makes it easy to test not only individual functions, but also methods on stateful objects.</p>

<p>For example, here is the test code for our specialised representation of lists of row ids (of 64-bit integers),
which reuses internal bits as inline storage in the common case when the list is small (the type is called <code>entry</code> because it‚Äôs
a pair of a key and a list of values).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_inlined_list.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">class</span> <span class="nc">PregrouperEntryList</span><span class="p">(</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Populate a pregrouper entry in LIST mode; it should have all row</span>
</span><span class="line"><span class="sd">    ids in insertion order.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">
</span><span class="line">    <span class="n">MODE</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">PREGROUP_MODE_LIST</span>
</span><span class="line">    <span class="n">U64S</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">one_of</span><span class="p">(</span>
</span><span class="line">        <span class="c1"># Either one of the sentinel values</span>
</span><span class="line">        <span class="n">st</span><span class="o">.</span><span class="n">sampled_from</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">64</span> <span class="o">-</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">64</span><span class="p">))),</span>
</span><span class="line">        <span class="c1"># or any row id.</span>
</span><span class="line">        <span class="n">st</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">64</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
</span><span class="line">    <span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">error</span> <span class="o">=</span> <span class="n">FFI</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="s2">&quot;crdb_error_t *&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entry</span> <span class="o">=</span> <span class="bp">None</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">entry</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
</span><span class="line">            <span class="n">C</span><span class="o">.</span><span class="n">pregrouper_entry_deinit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MODE</span><span class="p">)</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">entry</span> <span class="o">=</span> <span class="bp">None</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">perror</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: </span><span class="si">%s</span><span class="s2"> </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">FFI</span><span class="o">.</span><span class="n">string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="o">.</span><span class="n">message</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span><span class="o">.</span><span class="n">error</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">entry</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">U64S</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">create_entry</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">entry</span> <span class="o">=</span> <span class="n">FFI</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="s2">&quot;struct crdb_pregrouper_entry *&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="n">C</span><span class="o">.</span><span class="n">pregrouper_entry_init</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">,</span> <span class="p">[</span><span class="n">row</span><span class="p">])</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">U64S</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">add_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">pregrouper_entry_add_row</span><span class="p">(</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MODE</span><span class="p">,</span> <span class="p">[</span><span class="n">row</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span>
</span><span class="line">        <span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">perror</span><span class="p">(</span><span class="s2">&quot;add_row&quot;</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">row</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">pregrouper_entry_finalize</span><span class="p">(</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">MODE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">error</span>
</span><span class="line">        <span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">perror</span><span class="p">(</span><span class="s2">&quot;finalize&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">()</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">finalize</span><span class="p">()</span>
</span><span class="line">        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">rows</span> <span class="o">==</span> <span class="p">[</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="o">.</span><span class="n">payload</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span><span class="line">            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">entry</span><span class="o">.</span><span class="n">payload</span><span class="o">.</span><span class="n">list</span><span class="o">.</span><span class="n">n_entries</span><span class="p">)</span>
</span><span class="line">        <span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">TestPregrouperEntryList</span> <span class="o">=</span> <span class="n">PregrouperEntryList</span><span class="o">.</span><span class="n">TestCase</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Most <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html#hypothesis.stateful.rule"><code>@rule</code>-decorated</a> methods call a C function
before updating a Python-side model of the values we expect to find in the list.
The <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html#hypothesis.stateful.rule"><code>@rule</code> decorator</a> mark methods that Hypothesis may call to generate examples;
the decorator‚Äôs arguments declare how each method should be invoked.
Some methods also have a <a href="https://hypothesis.readthedocs.io/en/latest/stateful.html#preconditions"><code>@precondition</code> decorator</a> to specify when they can be invoked
(a <code>@rule</code> without any <code>@precondition</code> is always safe to call).
There is one method (<code>create_entry</code>) to create a new list populated with an initial row id,
another (<code>add_row</code>) to add a row id to the list,
one to <code>finalize</code> the list, something which should always be safe to call and <em>must</em> be done before reading the list‚Äôs contents
(finalizing converts away from the inline representation),
and a <code>check</code> to compare the list of row ids in C to the one we maintained in Python.</p>

<p>Row ids are arbitrary 64-bit integers, so we could simply
ask Hypothesis to generate integers in \([0, 2^{64} - 1]\).
However, we also know that the implementation uses high row id values around
<code>UINT64_MAX</code> as sentinels in its implementation of inline storage, as shown below.</p>

<pre><code>struct inlined_list {
    union {
        uint64_t data[2];
        struct {
            uint64_t *arr;
            unsigned int capacity;
            unsigned int length;
        };
    };
    uint64_t control;
};

          Out of line list      Inline list of 2      Inline list of 3
          ----------------      ----------------      ----------------
data[0]:    arr                   elt #0                elt #0
data[1]:    capacity &amp; length     elt #1                elt #1
control:    UINT64_MAX            UINT64_MAX - 2        elt #2 &lt; UINT64_MAX - 2
</code></pre>

<p>That‚Äôs why we
bias the data generation towards row ids that could also be mistaken for sentinel values:
we generate each row id by either sampling from a set of sentinel-like values, 
or by sampling from all 64-bit integers.
This approach to defining the input data is decidedly non-magical,
compared to the way I work with fuzzers.
However, fuzzers also tend to be slow and somewhat fickle.
I think it makes sense to ask programmers to think about how their own code should be tested,
instead of hoping a computer program will eventually intuit what edge cases look like.<sup id="fnref:no-preconception" role="doc-noteref"><a href="#fn:no-preconception" class="footnote">5</a></sup></p>

<p>It‚Äôs pretty cool that Hypothesis will now generate a bunch of API calls for me, but,
again, what makes Hypothesis really valuable is the way it
minimises long sequences of random calls into
understandable counter-examples.
Here‚Äôs what Hypothesis/pytest reports when I remove a guard that saves us from writing a row id
to a control word when that id would look like a sentinel value.</p>

<pre><code>self = PregrouperEntryList({})

    @precondition(lambda self: self.entry)
    @rule()
    def check(self):
        self.finalize()
&gt;       assert self.rows == [
            self.entry.payload.list.values[i]
            for i in range(self.entry.payload.list.n_entries)
        ]
E       AssertionError: assert [184467440737...4073709551613] == [184467440737...4073709551613]
E         Left contains one more item: 18446744073709551613
E         Full diff:
E         - [18446744073709551613, 18446744073709551613, 18446744073709551613]
E         ?                        ----------------------
E         + [18446744073709551613, 18446744073709551613]

test_pregroup_merge.py:81: AssertionError
------------------------------------------ Hypothesis ------------------------------------------
Falsifying example:
state = PregrouperEntryList()
state.create_entry(row=18446744073709551613)
state.add_row(row=18446744073709551613)
state.add_row(row=18446744073709551613)
state.check()
state.teardown()
</code></pre>

<p>We can see that we populated a list
thrice with the same row id, <code>18446744073709551613</code>,
but only found it twice in the final C-side list.
That row id is \(2^{64} - 3,\) the value we use to denote inline lists of two values.
This drawing shows how the last write ended up going to a control word where it was treated as a sentinel, making the inline representation look
like a list of two values, instead of three values.</p>

<pre><code>           Bad list of 3                Identical list of 2
          ----------------             ---------------------
data[0]:    UINT64_MAX - 2              UINT64_MAX - 2
data[1]:    UINT64_MAX - 2              UINT64_MAX - 2
control:    UINT64_MAX - 2 (payload)    UINT64_MAX - 2 (sentinel)

</code></pre>

<p>I restored the logic to prematurely stop using the inline representation
and convert to a heap-allocated vector whenever the row value would be interpreted as a sentinel,
and now Hypothesis doesn‚Äôt find anything wrong.
We also know that this specific failure is fixed,
because Hypothesis retries examples from its <a href="https://hypothesis.readthedocs.io/en/latest/database.html">failure database</a><sup id="fnref:failure-in-ci" role="doc-noteref"><a href="#fn:failure-in-ci" class="footnote">6</a></sup> on every rerun.</p>

<h3 id="but-hypothesis-is-a-python-library">But Hypothesis is a Python library?!</h3>
<p>There are multiple language-specific libraries under the <a href="https://github.com/HypothesisWorks/hypothesis">Hypothesis project</a>;
none of them is in C, and only the Python implementation is actively maintained.
One might think that makes Hypothesis inappropriate for testing C.
However, the big ideas in Hypothesis are language-independent.
There is a practical reason for the multiple implementations:
for a tool to be really usable, it has to integrate well with the programming language‚Äôs surrounding ecosystem.
In most languages, we also expect to write test code in the same language as the system under test.</p>

<p>C (and C++, I would argue) is an exception.
When I tell an experienced C developer they should write test code in Python,
I expect a sigh of relief.
The fact that Hypothesis is written in Python,
as opposed to another managed language like Ruby or C# also helps:
embedding and calling C libraries is Python‚Äôs bread and butter.
The weak state of C tooling is another factor:
no one has a strong opinion regarding how to invoke C test code,
or how the results should be reported.
I‚Äôll work with anything standard enough (e.g., pytest‚Äôs JUnit dump) to be ingested by Jenkins.</p>

<p>The last thing that sealed the deal for me is <a href="https://cffi.readthedocs.io/en/latest/">Python‚Äôs CFFI</a>.
With that library, I simply had to make sure my public header files were clean enough to be parsed without a full-blown compiler;
I could then write some simple Python code to <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-crdb-py-L58">strip away preprocessor directive</a>,
<a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-crdb-py-L70">read headers in dependency order</a>,
and <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-crdb-py-L77">load the production shared object</a>,
without any test-specific build step.
The snippets of test code above don‚Äôt look that different from tests for a regular Python module,
but there‚Äôs also a clear mapping from each Python call to a C call.
The level of magic is just right.</p>

<p>There is one testing concern that‚Äôs almost specific to C and C++ programs:
we must be on the lookout for memory management bugs.
For that, we use our <a href="https://github.com/google/sanitizers/wiki/AddressSanitizer">ASan</a>
build and bubble up <em>some</em> issues (e.g., read overflows or leaks) back to Python;
everything else results in an <code>abort()</code>, which, while suboptimal for minimisation, is still useful.
I simplified our test harness for the Heartbleed experiment;
see below for <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb">gists that anyone can use as starting points</a>.</p>

<h2 id="why-not-fuzzing">Why not fuzzing?</h2>

<p>I abused fuzzers a lot at Google,
mostly because the tooling was there
and it was trivial to burn hundreds of CPU-hours.
Technical workarounds seemed much easier than trying to add support for actual property-based testers,
so I would add domain specific assertions in order to detect more than crash conditions,
and translate bytes into more structured inputs or even convert them to series of method calls.
Now that I don‚Äôt have access to prebuilt infrastructure for fuzzers,
that approach doesn‚Äôt make as much sense.
That‚Äôs particularly true when byte arrays don‚Äôt naturally fit the entry point:
I spent a lot of time making sure the fuzzer was exercising branches in the system under test, not the test harness,
and designing input formats such that common fuzzer mutations, e.g., removing a byte, had a local effect and not, e.g., cause a frameshift mutation for the rest of the input‚Ä¶
and that‚Äôs before considering the time I wasted manually converting bytes to their structured interpretation on failures.</p>

<p>Hypothesis natively supports structured inputs and function calls,
and reports buggy inputs in terms of these high level concepts.
It is admittedly slower than fuzzers,
especially when compared to fuzzers that (like Hypothesis) don‚Äôt fork/exec for each evaluation.
I‚Äôm comfortable with that: my experience with NP-Hard problems tells me it‚Äôs better
to start by trying to do smart things with the structure of the problem, and later speed that up,
rather than putting all our hopes in making bad decisions really really fast.
Brute force can only do so much to an exponential-time cliff.</p>

<p>I had one niggling concern when leaving the magical world of fuzzers for staid property-based testing.
In some cases, I had seen coverage information steer fuzzers towards really subtle bugs;
could I benefit from the same smartness in Hypothesis?
That‚Äôs how I came up with the idea of simulating a reasonable testing process that ought to find <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0160">CVE-2014-0160</a>,
<a href="https://heartbleed.com/">Heartbleed</a>.</p>

<h2 id="role-playing-cve-2014-0160">Role-playing CVE-2014-0160</h2>

<p><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0160">CVE-2014-0160</a> a.k.a. <a href="https://heartbleed.com/">Heartbleed</a>
is a read heap overflow in OpenSSL 1.0.1 (\(\leq\) 1.0.1f) that leaks potentially private data over the wire.
It‚Äôs a straightforward logic bug in the implementation of <a href="https://tools.ietf.org/html/rfc6520">RFC 6520, a small optional (D)TLS extension</a>
that lets clients or servers ask for a ping.
<a href="https://www.theregister.co.uk/2014/04/09/heartbleed_explained/">The Register</a> has a concise visualisation of a bad packet.  We must send correctly sized packets that happen to ask for more pingback data than was sent to OpenSSL.</p>

<p><a href="https://www.theregister.co.uk/2014/04/09/heartbleed_explained/">
<img class="center" src="/images/2020-03-11-how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/the_register_heartbleed_diagram.png" />
</a></p>

<p>State of the art fuzzers like <a href="http://lcamtuf.coredump.cx/afl/">AFL</a> or <a href="https://github.com/google/honggfuzz">Honggfuzz</a> find that bug in seconds when given an entry point that passes bytes to the connection handler,
like data that had just come over the wire.
When provided with a corpus of valid messages, they‚Äôre even faster.</p>

<p>It‚Äôs a really impressive showing of brute force that these programs to come up with almost valid packets so quickly,
and traditional property-based testing frameworks are really not up to that level of magic.
However, I don‚Äôt find the black box setting that interesting.
It‚Äôs probably different for security testing,
since not leaking invalid data is apparently seen as a feature one can slap on after the fact:
there are so many things to test that it‚Äôs probably best to focus on what fuzzing can easily find, and, in any case,
it doesn‚Äôt seem practical to manually boil that ocean one functionality at a time.</p>

<p>From a software testing point of view however, 
I would expect the people who send in a patch to implement something like the Heartbeat extension
to also have a decent idea how to send bytes that exercise their new code.
Of course, a sufficiently diligent coder would have found the heap overflow during testing,
or simply not have introduced that bug.
That‚Äôs not a useful scenario to explore; 
I‚Äôm interested in something that does find Heartbleed, and also looks like a repeatable process.
The question becomes ‚ÄúWithin this repeatable process, can branch coverage feedback help Hypothesis find Heartbeat?‚Äù</p>

<p>Here‚Äôs what I settled on: let‚Äôs only assume the new feature code also comes
with a packet generator to exercise that code.  In Hypothesis, it might look like the following.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_with_grammar.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">class</span> <span class="nc">GrammarTester</span><span class="p">(</span><span class="n">sanitizers</span><span class="o">.</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span class="line">        <span class="c1"># Avoid reporting initialisers as leaks.</span>
</span><span class="line">        <span class="k">with</span> <span class="n">sanitizers</span><span class="o">.</span><span class="n">leaky_region</span><span class="p">():</span>
</span><span class="line">            <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">LLVMFuzzerTestOneInput</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">LLVMFuzzerTestOneInput</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@initialize</span><span class="p">(</span>
</span><span class="line">        <span class="n">tls_ver</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">sampled_from</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>
</span><span class="line">        <span class="n">payload</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">binary</span><span class="p">(),</span>
</span><span class="line">        <span class="n">padding</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">binary</span><span class="p">(</span><span class="n">min_size</span><span class="o">=</span><span class="mi">16</span><span class="p">),</span>
</span><span class="line">    <span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">add_heartbeat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tls_ver</span><span class="p">,</span> <span class="n">payload</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
</span><span class="line">        <span class="n">hb_payload</span> <span class="o">=</span> <span class="nb">bytes</span><span class="p">([</span><span class="mh">0x01</span><span class="p">])</span> <span class="o">+</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s2">&quot;&gt;H&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span> <span class="o">+</span> <span class="n">payload</span> <span class="o">+</span> <span class="n">padding</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">+=</span> <span class="nb">bytes</span><span class="p">([</span><span class="mh">0x18</span><span class="p">,</span> <span class="mh">0x03</span><span class="p">,</span> <span class="n">tls_ver</span><span class="p">])</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">+=</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s2">&quot;&gt;H&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">hb_payload</span><span class="p">))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">+=</span> <span class="n">hb_payload</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We have a single rule to initialize the test buffer with a valid heartbeat packet, 
and we send the whole buffer to a standard fuzzer entry point at the end.
In practice, I would also ask for some actual assertions that the system under test handles the packets correctly,
but that‚Äôs not important when it comes to Heartbleed:
we just need to run with ASan and look for heap overflows, which are essentially never OK.</p>

<p>Being able to provide a happy-path-only ‚Äútest‚Äù like the above should be less than table stakes in a healthy project.
Let‚Äôs simulate a bug-finding process that 
looks for crashes after adding three generic buffer mutating primitives:
one that replaces a single byte in the message buffer,
another one that removes some bytes from the end of the buffer,
and a last one that appends some bytes to the buffer.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_with_grammar.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">class</span> <span class="nc">GrammarTester</span><span class="p">(</span><span class="n">sanitizers</span><span class="o">.</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="o">...</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">value</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">255</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">replace_byte</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span><span class="line">        <span class="n">index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">        <span class="n">prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">index</span><span class="p">]</span>
</span><span class="line">        <span class="n">suffix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="nb">bytes</span><span class="p">([</span><span class="n">value</span><span class="p">])</span> <span class="o">+</span> <span class="n">suffix</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@precondition</span><span class="p">(</span><span class="k">lambda</span> <span class="bp">self</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">)</span>
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">data</span><span class="p">())</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">strip_suffix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
</span><span class="line">        <span class="n">count</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">)))</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="n">count</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">    <span class="nd">@rule</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">binary</span><span class="p">(</span><span class="n">min_size</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">add_suffix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">suffix</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">+=</span> <span class="n">suffix</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Given the initial generator rules and these three mutators,
Hypothesis will assemble sequences of calls to create and mutate buffers before sending them to OpenSSL at the end of the ‚Äútest.‚Äù</p>

<p>The first time I ran this, Hypothesis found the bug in a second or two</p>

<pre><code>==12810==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x62900012b748 at pc 0x7fb70d372f4e bp 0x7ffe55e56a30 sp 0x7ffe55e561e0
READ of size 30728 at 0x62900012b748 thread T0

0x62900012b748 is located 0 bytes to the right of 17736-byte region [0x629000127200,0x62900012b748)
allocated by thread T0 here:
    #0 0x7fb70d3e3628 in malloc (/usr/lib/x86_64-linux-gnu/libasan.so.5+0x107628)
    #1 0x7fb7058ee939  (fuzzer-test-suite/openssl-1.0.1f-fsanitize.so+0x19a939)
    #2 0x7fb7058caa08  (fuzzer-test-suite/openssl-1.0.1f-fsanitize.so+0x176a08)
    #3 0x7fb7058caf41  (fuzzer-test-suite/openssl-1.0.1f-fsanitize.so+0x176f41)
    #4 0x7fb7058a658d  (fuzzer-test-suite/openssl-1.0.1f-fsanitize.so+0x15258d)
    #5 0x7fb705865a12  (fuzzer-test-suite/openssl-1.0.1f-fsanitize.so+0x111a12)
    #6 0x7fb708326deb in ffi_call_unix64 (/home/pkhuong/follicle/follicle/lib/Python3.7/site-packages/.libs_cffi_backend/libffi-806b1a9d.so.6.0.4+0x6deb)
    #7 0x7fb7081b4f0f  (&lt;unknown module&gt;)
</code></pre>

<p>and then spent the 30 seconds trying to minimise the failure.
Unfortunately, it seems that ASan tries to be smart and avoids reporting duplicate errors, so minimisation does not work for memory errors.
It also doesn‚Äôt matter that much if we find a minimal test case: a core dump and a reproducer is usually good enough.</p>

<p>You can find the complete code for <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-test_with_grammar-py">GrammarTester</a>,
along with the <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-sanitizers-py">ASan wrapper</a>,
and <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-llvm_one_input-py">CFFI glue</a> in <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb">this gist</a>.</p>

<p>For the rest of this post, we‚Äôll make ASan crash on the first error, and count the number of test cases generated (i.e., the number of calls into the OpenSSL fuzzing entry point)
until ASan made OpenSSL crash.</p>

<h2 id="how-can-we-do-better-with-hardware-tracing">How can we do better with hardware tracing?</h2>

<p>Intel chips have offered <a href="https://github.com/torvalds/linux/blob/master/tools/perf/Documentation/intel-bts.txt">BTS, a branch trace store</a>, since Nehalem (2008-9), if not earlier.
BTS is slower than <a href="https://software.intel.com/en-us/blogs/2013/09/18/processor-tracing">PT, its full blown hardware tracing successor</a>,
and only traces branches, but it does so in a dead simple format.
I wrapped Linux perf‚Äôs interface for BTS in a <a href="https://gist.github.com/pkhuong/1ce34e33c6df4b9be3bc9beb22415a47">small library</a>;
let‚Äôs see if we can somehow feed that trace to Hypothesis and consistently find Heartbeat faster.</p>

<p>But first, how do fuzzers use coverage information?</p>

<p>A lot of the impressive stuff that fuzzers find stems from their ability to infer the constants that were compared with the fuzzing input by the system under test.
That‚Äôs not really that important when we assume a more white box testing setting,
where the same person or group responsible for writing the code is also tasked with testing it, or at least specifying how to do so.</p>

<p>Apart from guessing what valid inputs look like,
fuzzers also use branch coverage information to diversify their inputs.
In the initial search phase, none of the inputs cause a crash, and fuzzers can only mutate existing, equally non-crashy, inputs or create new ones from scratch.
The goal is to maintain a small population that can trigger all the behaviours (branches) we‚Äôve observed so far,
and search locally around each member of that population.
Pruning to keep the population small is beneficial for two reasons: first, it‚Äôs faster to iterate over a smaller population, and second,
it avoids redundantly exploring the neighbourhood of nearly identical inputs.</p>

<p>Of course, this isn‚Äôt ideal.
We‚Äôd prefer to keep one input per program state,
but we don‚Äôt know what the distinct program states are.
Instead, we only know what branches we took on the way to wherever we ended up.
It‚Äôs as if we were trying to generate directions to every landmark in a city,
but the only feedback we received was the set of streets we walked on while following the directions.
That‚Äôs far from perfect, but, with enough brute force, it might just be good enough.</p>

<p>We can emulate this diversification and local exploration logic with multi-dimensional <a href="http://proper.softlab.ntua.gr/Publications.html">Targeted example generation</a>,
for which <a href="https://hypothesis.readthedocs.io/en/latest/details.html#targeted-example-generation">Hypothesis has experimental support</a>.</p>

<p>We‚Äôll assign an arbitrary unique label to each origin/destination pair we observe via BTS,
and assign a score of 1.0 to every such label (regardless of how many times we observed each pair).
Whenever Hypothesis compares two score vectors, a missing value is treated as \(-\infty\), so, everything else being equal, covering a branch is better than not covering it.
After that, we‚Äôll rely on the fact that multidimensional discrete optimisation is
also all about maintaining a small but diverse population:
Hypothesis regularly prunes redundant examples (examples that exercises a subset of
the branches triggered by another example),
and generates new examples by mutating members of the population.
With our scoring scheme, the multidimensional search will split its efforts between families of examples that trigger different sets of branches,
and will also stop looking around examples that trigger a strict subset of another example‚Äôs branches.</p>

<p>Here‚Äôs the plan to give BTS feedback to Hypothesis and diversify its initial search for failing examples.
I‚Äôll use my <a href="https://gist.github.com/pkhuong/1ce34e33c6df4b9be3bc9beb22415a47">libbts</a> to wrap perf syscalls into something usable,
and wrap that in Python to more easily <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-bts-py">gather origin/destination pairs</a>.
Even though I enable BTS tracing only around FFI calls,
there will be some noise from libffi, as well as dummy transitions for interrupts or context switches; <code>bts.py</code> attempts to only consider interesting branches
by <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-bts-py-L105">remembering the set of executable mappings present at startup</a>,
before loading the library under test,
and dropping jumps to or from addresses that were mapped at startup
(presumably, that‚Äôs from the test harness), or invalid zero or negative addresses
(which I‚Äôm pretty sure denote interrupts and syscalls).</p>

<p>We‚Äôll then wrap the Python functions that call into OpenSSL to record the set of branches executed during that call,
and convert that set to a multidimensional score at the end of the test, in the <code>teardown</code> method.</p>

<p>The only difference is in the <code>__init__</code> method, which must also reset BTS state,
and in the <code>teardown</code> method, where we score the example if it failed to crash.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_with_grammar_bts.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">class</span> <span class="nc">GrammarBtsTester</span><span class="p">(</span><span class="n">sanitizers</span><span class="o">.</span><span class="n">RuleBasedStateMachine</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span><span class="line">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</span><span class="line">        <span class="n">bts</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</span><span class="line">        <span class="k">with</span> <span class="n">sanitizers</span><span class="o">.</span><span class="n">leaky_region</span><span class="p">():</span>
</span><span class="line">            <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">LLVMFuzzerTestOneInput</span><span class="p">(</span><span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</span><span class="line">        <span class="n">bts</span><span class="o">.</span><span class="n">update_useless_edges</span><span class="p">()</span>
</span><span class="line">        <span class="n">bts</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">buf</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="k">assert</span> <span class="n">C</span><span class="o">.</span><span class="n">LLVMFuzzerTestOneInput</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buf</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span>
</span><span class="line">        <span class="c1"># bts.report() returns a set of control flow edge ids.</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bts</span><span class="o">.</span><span class="n">report</span><span class="p">():</span>
</span><span class="line">            <span class="n">target</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
</span><span class="line">        <span class="n">bts</span><span class="o">.</span><span class="n">teardown</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let‚Äôs instrument the <code>teardown</code> methods to print ‚ÄúCHECK‚Äù and flush before every call to the fuzzing entry point,
and make sure <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-run-tests-sh">ASan crashes when it finds an issue</a>.
We‚Äôll run the test files, grep for ‚ÄúCHECK‚Äù, and, assuming we don‚Äôt trigger into any runtime limit,
find out how many examples Hypothesis had to generate before causing a crash.</p>

<p>I ran this 12000 times for both <code>test_with_grammar.py</code> and <code>test_with_grammar_bts.py</code>.
Let‚Äôs take a look at the empirical distribution functions for the number of calls until a crash, for <code>test_with_grammar.py</code> (<code>grammar</code>)
and <code>test_with_grammar_bts.py</code> (<code>bts</code>).</p>

<!-- ggplot(data, aes(x=Calls, colour=Impl)) + stat_ecdf() + scale_color_discrete(name="Implementation") + scale_x_log10() + ylab("Cumulative fraction") + xlab("Calls to OpenSSL until crash") -->
<p><img class="center" src="/images/2020-03-11-how-hard-is-it-to-guide-test-case-generators-with-branch-coverage-feedback/ecdf.png" /></p>

<p>There‚Äôs a cross-over around 100 calls:
as long as we have enough time for least 100 calls to OpenSSL,
we‚Äôre more likely to find Heartbleed with coverage feedback than by rapidly searching blindly.
With fewer than 100 calls, it seems likely that branch coverage only guides the search
towards clearly invalid inputs that trigger early exit conditions.
Crucially, the curves are smooth and tap out before our limit of 10000 examples per execution,
so we‚Äôre probably not measuring a side-effect of the experimental setup.</p>

<p>In theory, the distribution function for the uninstrumented <code>grammar</code> search should look a lot like a geometric distribution,
but I don‚Äôt want to assume too much of the <code>bts</code> implementation.  Let‚Äôs confirm our gut feeling in <code>R</code> with a simple non-parametric test,
the <a href="https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test">Wilcoxon rank sum test</a>, an unpaired analogue to the sign test:</p>

<pre><code>&gt; wilcox.test(data$Calls[data$Impl == 'bts'], data$Calls[data$Impl == 'grammar'], 'less')

        Wilcoxon rank sum test with continuity correction

data:  data$Calls[data$Impl == "bts"] and data$Calls[data$Impl == "grammar"]
W = 68514000, p-value = 4.156e-11
alternative hypothesis: true location shift is less than 0
</code></pre>

<p>We‚Äôre reasonably certain that the number of calls for a random run of <code>bts</code> is more
frequently less than the number of calls for an independently chosen random run of <code>grammar</code>
than the opposite.  Since the tracing overhead is negligible,
this means branch feedback saves us time more often than not.
For the 24 000 separate runs we observed,
<code>bts</code> is only faster 52% of the time, 
but that‚Äôs mostly because both distributions of calls to the system under test go pretty high.
On average, vanilla Hypothesis, without coverage feedback, found the vulnerability after 535 calls to OpenSSL;
with feedback, Hypothesis is 11.5% faster on average, and only needs 473 calls.</p>

<h2 id="time-to-use-this-in-anger">Time to use this in anger</h2>

<p>We have been using this testing approach (without branch coverage) at Backtrace for a couple months now,
and it‚Äôs working well as a developer tool that offers rapid feedback, enough that we‚Äôre considering running these tests in a commit hook.
Most of the work involved in making the approach useful was just plumbing, e.g., dealing with the way ASan reports errors,
or making sure we don‚Äôt report leaks that happen in Python, outside the system under test.
Once the commit hook is proven solid, we‚Äôll probably want to look into running tests in the background for a long time.
That‚Äôs a very different setting from pre-commit or commit -time runs, where every bug is sacred.
If I let something run over the weekend, I must be able to rely on deduplication (i.e., minimisation is even more useful),
and I will probably want to silence or otherwise triage some issues.
That‚Äôs the sort of thing Backtrace already handles, so we are looking into sending Hypothesis reports directly to Backtrace,
the same way we do for clang-analyzer reports and for crashes or leaks found by ASan in our end-to-end tests.</p>

<p>The challenges are more research-y for coverage feedback. There‚Äôs no doubt a lot of mundane plumbing issues involved in making this feedback robust
(see, e.g., the <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-bts-py-L172">logic in bts.py</a> to filter out interrupts and branches from the kernel, or branches from code that‚Äôs probably not in the system under test).
However, there‚Äôs also a fundamental question that we unfortunately can‚Äôt answer by copying fuzzers.</p>

<p>How should we score coverage over multiple calls?
After all, the ability to test a stateful interface via a sequence of method calls and expections is what really sold me on Hypothesis.
Fuzzers have it easy: they assume each call is atomic and independent of any other call to the system under test, even when they don‚Äôt fork for each execution.
This seems like a key reason why simple coverage metrics work well in fuzzers,
and I don‚Äôt know that we can trivially port ideas from fuzzing to stateful testing.</p>

<p>For example, my first stab at this experiment found no statistically significant improvement from BTS feedback.  The only difference is that the assertion lived in a <code>check</code> rule,
and not in the <code>teardown</code> method, which let Hypothesis trigger a call to OpenSSL at various points in the buffer mutation sequence‚Ä¶ usually a good thing for test coverage.
I‚Äôm pretty sure the problem is that a single example could collect ‚Äúpoints‚Äù for covering branches in multiple unrelated calls to OpenSSL,
while we would rather cover many branches in a single call.
What does it mean for stateful testing, where we want to invoke different functions multiple times in a test?</p>

<p>I have no idea; maybe we should come up with some synthetic stateful testing benchmarks that are expected to benefit from coverage information. However, the experiment in this post gives me hope that there exists some way to exploit coverage information in stateful testing.
<a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb">The MIT-licensed support code in this gist</a>
(along <a href="https://gist.github.com/pkhuong/1ce34e33c6df4b9be3bc9beb22415a47">with libbts</a>)
should give you all a headstart to try more stuff and report your experiences.</p>

<p><small>Thank you David, Ruchir, Samy, and Travis for your comments on an early draft.</small></p>

<p><hr style="width: 50%" /></p>

<h2 id="appendix-how-to-get-started-with-your-own-project">Appendix: How to get started with your own project</h2>

<p>In a performance-oriented C library,
the main obstable to testing with Hypothesis and CFFI will probably be
that the headers are too complex for <a href="https://github.com/eliben/pycparser"><code>pycparser</code></a>.  I had to use a few tricks
to make ours parse.</p>

<p>First, I retro-actively imposed more discipline on what was really public and what
implementation details should live in private headers:
we want <a href="https://cffi.readthedocs.io/en/latest/">CFFI</a> to handle every public header, and only some private headers for more targeted testing.
This separation is a good thing to maintain in general, and, if anything,
having <a href="https://github.com/eliben/pycparser">pycparser</a> yell at us when we make our public interface depend on internals is a net benefit.</p>

<p>I then had to reduce our reliance on the C preprocessor.  In some cases,
that meant making types opaque and adding getters.
In many more cases, I simply converted small <code>#define</code>d integer constants to
anonymous enums <code>enum { CONSTANT_FOO = ... }</code>.</p>

<p>Finally, especially when testing internal functionality, I had to remove
<code>static inline</code> functions.
That‚Äôs another case where pycparser forces us to maintain cleaner headers,
and the fix is usually simple:
declare <code>inline</code> (not static) functions in the header,
<code>#include</code> a <code>.inl</code> file with the <code>inline</code> definition (we can easily drop directives in Python),
and re-declare the function as <code>extern</code> in the main source file for that header.
With this approach, the header can focus on documenting the interface,
the compiler still has access to an inline definition,
and we don‚Äôt waste instruction bytes on duplicate out-of-line definitions.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>u64_block.h </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cm">/**</span>
</span><span class="line"><span class="cm"> * This file documents the external interface.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kr">inline</span> <span class="kt">uint8_t</span> <span class="nf">crdb_u64_block_lt</span><span class="p">(</span><span class="k">const</span> <span class="kt">uint64_t</span> <span class="o">*</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">k</span><span class="p">);</span>
</span><span class="line">
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&quot;u64_block.inl&quot;  /* Stripped in Python. */</span><span class="cp"></span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>u64_block.inl </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cm">/**</span>
</span><span class="line"><span class="cm"> * Describe implementation details here.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kr">inline</span> <span class="kt">uint8_t</span>
</span><span class="line"><span class="nf">crdb_u64_block_lt</span><span class="p">(</span><span class="k">const</span> <span class="kt">uint64_t</span> <span class="o">*</span><span class="n">block</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">k</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">    <span class="p">....</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>u64_block.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cp">#include</span> <span class="cpf">&quot;u64_block.h&quot;</span><span class="cp"></span>
</span><span class="line">
</span><span class="line"><span class="cm">/* Provide the unique out-of-line definition. */</span>
</span><span class="line"><span class="kt">uint8_t</span> <span class="nf">crdb_u64_block_lt</span><span class="p">(</span><span class="k">const</span> <span class="kt">uint64_t</span> <span class="o">*</span><span class="n">block</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">k</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>That doesn‚Äôt always work, mostly because regular <code>inline</code> functions aren‚Äôt
supposed to call <code>static inline</code> functions.  When I ran into that issue,
I either tried to factor the more complex slow path out to an out-of-line definition,
or, more rarely, resorted to CPP tricks (also hidden in a <code>.inl</code> file) to rewrite calls to
<code>extern int foo(...)</code>
with macros like <code>#define foo(...) static_inline_foo(__VA_ARGS__)</code>.</p>

<p>All that work isn‚Äôt really testing overhead; they‚Äôre mostly things
that library maintainers <em>should</em> do, but are easy to forget when
when we only hear from C programmers.</p>

<p>Once the headers were close enough to being accepted by CFFI,
I closed the gap with string munging in Python.
All the tests depend on the <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-crdb-py">same file that parses all the headers we care about in the correct order and loads the shared object</a>.
Loading everything in the same order also enforces a reasonable dependency
graph, another unintended benefit.
Once everything is loaded, that file also <a href="https://gist.github.com/pkhuong/fe81822fd6adab723f91601f39dce4fb#file-crdb-py-L83">post-processes the CFFI functions</a> to hook
in ASan (and branch tracing), and strip away any namespacing
prefix.</p>

<p>The end result is a library that‚Äôs more easily used from managed
languages like Python, and which we can now test it like any other
Python module.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:realtime" role="doc-endnote">
      <p>The graph shows time in terms of calls to the SUT, not real time. However, the additional overhead of gathering and considering coverage information is small enough that the feedback also improves wallclock and CPU time.¬†<a href="#fnref:realtime" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:matchers" role="doc-endnote">
      <p>High level <a href="https://github.com/google/googletest">matchers like GMock‚Äôs</a> help, but they don‚Äôt always explain why a given matcher makes sense.¬†<a href="#fnref:matchers" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:change-detector" role="doc-endnote">
      <p>There‚Äôs also a place for <a href="https://github.com/approvals/ApprovalTests.cpp/blob/master/doc/Overview.md">smart change detectors, especially when refactoring ill understood code</a>.¬†<a href="#fnref:change-detector" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:unless-examples" role="doc-endnote">
      <p>I had to disable the exhaustive list of examples to benefit from minimisation. Maybe one day I‚Äôll figure out how to make Hypothesis treat explicit examples more like an initial test corpus.¬†<a href="#fnref:unless-examples" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:no-preconception" role="doc-endnote">
      <p>That reminds me of an AI Lab Koan. ¬´In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. ‚ÄúWhat are you doing?‚Äù asked Minsky. ‚ÄúI am training a randomly wired neural net to play Tic-tac-toe,‚Äù Sussman replied. ‚ÄúWhy is the net wired randomly?‚Äù asked Minsky. Sussman replied, ‚ÄúI do not want it to have any preconceptions of how to play.‚Äù Minsky then shut his eyes. ‚ÄúWhy do you close your eyes?‚Äù Sussman asked his teacher. ‚ÄúSo that the room will be empty,‚Äù replied Minsky. At that moment, Sussman was enlightened.¬ª¬†<a href="#fnref:no-preconception" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:failure-in-ci" role="doc-endnote">
      <p>The database is a directory of files full of binary data.  The specific meaning of these bytes depends on the Hypothesis version and on the test code, so the database should probably not be checked in.  Important examples (e.g., for regression testing) should instead be made persistent with <code>@example</code> decorators.  Hypothesis does guarantee that tese files are always valid (i.e., any sequence of bytes in the database will result in an example that can be generated by the version of Hypothesis and of the test code that reads it), so we don‚Äôt have to invalidate the cache when we update the test harness.¬†<a href="#fnref:failure-in-ci" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
