<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Fingerprint | Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/Blog/categories/fingerprint/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2025-08-14T17:06:00-04:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[UMASH: a fast and universal enough hash]]></title>
    <link href="https://www.pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting/"/>
    <updated>2020-08-24T00:00:15-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/08/24/umash-fast-enough-almost-universal-fingerprinting</id>
    <content type="html"><![CDATA[<p><small>Originally posted on the <a href="https://engineering.backtrace.io/2020-08-24-umash-fast-enough-almost-universal-fingerprinting/">Backtrace I/O tech blog</a>.</small></p>

<p>We accidentally a whole hash function… but we had a good reason!
Our
<a href="https://github.com/backtrace-labs/umash">MIT-licensed UMASH hash function</a>
is a decently fast non-cryptographic hash function that guarantees
a worst-case bound on the  probability of collision
<a href="https://en.wikipedia.org/wiki/Universal_hashing#Mathematical_guarantees">between any two inputs generated independently of the UMASH parameters</a>.</p>

<p>On the
<a href="https://en.wikichip.org/wiki/intel/xeon_platinum/8175m">2.5 GHz Intel 8175M</a>
servers that power <a href="https://backtrace.io/">Backtrace</a>’s hosted
offering, UMASH computes a 64-bit hash for short cached inputs of up
to 64 bytes in 9-22 ns, and for longer ones at up to 22 GB/s, while
guaranteeing that two distinct inputs of at most \(s\) bytes collide
with probability less than \(\lceil s / 2048 \rceil \cdot 2^{-56}\).
If that’s not good enough, we can also reuse most of the parameters to
compute two independent UMASH values. The resulting 128-bit
<a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprint function</a>
offers a short-input latency of 9-26 ns, a peak throughput of 11.2
GB/s, and a collision probability of \(\lceil s / 2048 \rceil^2 \cdot
2^{-112}\) (better than \(2^{-70}\) for input size up to 7.5 GB).
These collision bounds hold for all inputs constructed without any
feedback about the randomly chosen UMASH parameters.</p>

<p>The latency on short cached inputs (9-22 ns for 64 bits, 9-26 ns for
128) is somewhat worse than the state of the art for non-cryptographic
hashes—
<a href="https://github.com/wangyi-fudan/wyhash">wyhash</a> achieves
8-15 ns and <a href="http://fastcompression.blogspot.com/2019/03/presenting-xxh3.html">xxh3</a>
 8-12 ns—but still in the same ballpark.  It also
compares well with latency-optimised hash functions like
<a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function#FNV-1a_hash">FNV-1a</a>
(5-86 ns) and
<a href="https://en.wikipedia.org/wiki/MurmurHash#MurmurHash2">MurmurHash64A</a>
(7-23 ns).</p>

<p>Similarly, UMASH’s peak throughput (22 GB/s) does not match
the current best hash throughput (37 GB/s with
<a href="https://github.com/Cyan4973/xxHash">xxh3</a>
and <a href="https://github.com/gamozolabs/falkhash">falkhash</a>, apparently
10% higher with <a href="https://github.com/cmuratori/meow_hash">Meow hash</a>),
but does comes within a factor of two; it’s actually higher than that of
some performance-optimised hashes, like
<a href="https://github.com/wangyi-fudan/wyhash">wyhash</a> (16 GB/s) and
<a href="https://github.com/google/farmhash">farmhash32</a>
(19 GB/s).  In fact, even the 128-bit fingerprint (11.2 GB/s) is
comparable to respectable options like
<a href="https://github.com/aappleby/smhasher/blob/master/src/MurmurHash2.cpp#L89">MurmurHash64A</a>
(5.8 GB/s) and
<a href="https://burtleburtle.net/bob/hash/spooky.html">SpookyHash</a> (11.6 GB/s).</p>

<p>What sets UMASH apart from these other non-cryptographic hash
functions is its proof of a collision probability bound.  In the
absence of an adversary that adaptively constructs pathological inputs
as it infers more information about the randomly chosen parameters, we
know that two distinct inputs of \(s\) or fewer bytes will have the
same 64-bit hash with probability at most \(\lceil s / 2048 \rceil
\cdot 2^{-56},\) where the expectation is taken over the random
“key” parameters.</p>

<p>Only one non-cryptographic hash function in
<a href="https://github.com/rurban/smhasher">Reini Urban’s fork of SMHasher</a>
provides this sort of bound: <a href="https://github.com/lemire/clhash">CLHash</a>
<a href="https://arxiv.org/abs/1503.03465">guarantees a collision probability \(\approx 2^{-63}\)</a>
in the same
<a href="https://en.wikipedia.org/wiki/Universal_hashing#Mathematical_guarantees">universal hashing</a>
model as UMASH.  While CLHash’s peak throughput (22 GB/s) is
equal to UMASH’s, its latency on short inputs is worse (23-25 ns
instead of 9-22ns).  We will also see that its stronger collision
bound remains too weak for many practical applications.  In order to
compute a <a href="https://en.wikipedia.org/wiki/Fingerprint_(computing)">fingerprint</a>
with CLHash, one would have to combine multiple hashes, exactly like
we did for the 128-bit UMASH fingerprint.</p>

<p>Actual cryptographic hash functions provide stronger bounds in a much
more pessimistic model; however they’re also markedly slower than
non-cryptographic hashes.  <a href="https://github.com/BLAKE3-team/BLAKE3">BLAKE3</a>
needs at least 66 ns to hash short inputs, and achieves a peak throughput
of 5.5 GB/s.  Even the <a href="https://github.com/rust-lang/rust/issues/29754">reduced-round SipHash-1-3</a>
hashes short inputs in 18-40 ns and longer ones at a peak throughput
of 2.8 GB/s.  That’s the price of their pessimistically adversarial
security model.  Depending on the application, it can make sense to
consider a more restricted adversary that must prepare its dirty deed
before the hash function’s parameters are generated at random, and
still ask for provable bounds on the probability of collisions.
That’s the niche we’re targeting with UMASH.</p>

<p>Clearly, the industry is comfortable with no bound at all.
However, even in the absence of
<a href="https://www.131002.net/siphash/#at">seed-independent collisions</a>,
timing side-channels in a data structure implementation could
theoretically leak information about colliding inputs, and iterating
over a hash table’s entries to print its contents can divulge even more
bits.  A sufficiently motivated adversary could use something like
that to learn more about the key and deploy an algorithmic denial of
service attack.  For example, the linear structure of UMASH (and of
other polynomial hashes like CLHash) makes it easy to combine known
collisions to create exponentially more colliding inputs.  There is no
universal answer; UMASH is simply another point in the solution space.</p>

<p>If reasonable performance coupled with an actual bound on collision
probability <em>for data that does not adaptively break the hash</em> sounds
useful to you,
<a href="https://github.com/backtrace-labs/umash">take a look at UMASH on GitHub</a>!</p>

<p>The <a href="#but-why">next section</a> will explain why we found it useful to
design another hash function.  The rest of the post
<a href="#umash-high-level">sketches how UMASH works</a> and
<a href="#implementation-tricks">how it balances short-input latency and strength</a>,
before <a href="#usage">describing a few interesting usage patterns.</a></p>

<p><small>The latency and throughput results above were all measured on
the same unloaded 2.5 GHz Xeon 8175M.  While we did not disable
frequency scaling (#cloud), the clock rate seemed stable at 3.1
GHz during our run.</small></p>

<h2 id="how-did-we-even-get-here"><a id="but-why"></a>How did we even get here?</h2>

<p>Engineering is the discipline of satisficisation: crisply defined
problems with perfect solutions rarely exist in reality, so we must
resign ourselves to satisfying approximate constraint sets “well
enough.”  However, there are times when all options are not only
imperfect, but downright sucky.  That’s when one has to put on a
different hat, and question the problem itself: are our constraints
irremediably at odds, or are we looking at an under-explored
solution space?</p>

<p>In the former case, we simply have to want something else.  In the
latter, it might make sense to spend time to really understand the
current set of options and hand-roll a specialised approach.</p>

<p>That’s the choice we faced when we started caching intermediate
results in
<a href="https://help.backtrace.io/en/articles/2428859-web-console-overview">Backtrace’s database</a>
and found a dearth of acceptable hash functions.  Our in-memory
columnar database is a core component of the backend, and, like most
analytics databases, it tends to process streams of similar queries.
However, a naïve query cache would be ineffective: our more heavily
loaded servers handle a constant write load of more than 100 events
per second with dozens of indexed attributes (populated column values)
each.  Moreover, queries invariably select a large number of data
points with a time windowing predicate that excludes old data… and
the endpoints of these time windows advance with each wall-clock
second.  The queries evolve over time, and must usually consider newly
ingested data points.</p>

<p><a href="https://www.gsd.inesc-id.pt/~rodrigo/slider_middleware14.pdf">Bhatotia et al’s Slider</a>
show how we can specialise the idea of
<a href="http://adapton.org/">self-adjusting or incremental computation</a>
for repeated MapReduce-style queries over a sliding window.
The key idea is to split the data set at stable boundaries (e.g., on
date change boundaries rather than 24 hours from the beginning of the
current time window) in order to expose memoisation opportunities, and
to do so recursively to repair around point mutations to older data.</p>

<p>Caching fully aggregated partial results works well for static
queries, like scheduled reports… but the first step towards creating
a great report is interactive data exploration, and that’s an activity
we strive to support well, even when drilling down tens of millions of
rich data points.  That’s why we want to also cache intermediate
results, in order to improve response times when tweaking a saved
report, or when crafting ad hoc queries to better understand how and
when an application fails.</p>

<p>We must go back to a
<a href="http://www.umut-acar.org/self-adjusting-computation">more general incremental computation strategy</a>:
rather than only splitting up inputs, we want to stably partition the
data dependency graph of each query, in order to identify shared
subcomponents whose results can be reused.  This finer grained
strategy surfaces opportunities to “resynchronise” computations, to
recognize when different expressions end up generating a subset of
identical results, enabling reuse in later steps.  For example, when
someone updates a query by adding a selection predicate that only
rejects a small fraction of the data, we can expect to reuse some of
the post-selection work executed for earlier incarnations of the
query, if we remember to key on the selected data points rather than
the predicates.</p>

<p>The complication here is that these intermediate results tend to be
large.  Useful analytical queries start small (a reasonable query
coupled with cache/transaction invalidation metadata to stand in for
the full data set), grow larger as we select data points, arrange them
in groups, and materialise their attributes, and shrink again at the
end, as we summarise data and throw out less interesting groups.</p>

<p>When caching the latter shrinking steps, where resynchronised reuse
opportunities abound and can save a lot of CPU time, we often
find that storing a fully materialised representation of the cache key
would take up more space than the cached result.</p>

<p>A classic approach in this situation is to fingerprint cache keys with
a cryptographic hash function like
<a href="https://en.wikipedia.org/wiki/BLAKE_(hash_function)">BLAKE</a>
or <a href="https://en.wikipedia.org/wiki/SHA-3">SHA-3</a>, and store a
compact (128 or 256 bits) fingerprint instead of the cache key: the
probability of a collision is then so low that we might as well assume
any false positive will have been caused by a bug in the code or a
hardware failure.  For example,
<a href="https://users.ece.cmu.edu/~omutlu/pub/memory-errors-at-facebook_dsn15.pdf#page=3">a study of memory errors at Facebook</a>
found that uncorrectable memory errors affect 0.03% of servers each
month.  Assuming a generous clock rate of 5 GHz, this means each
clock cycle may be afflicted by such a memory error with probability
\(\approx 2.2\cdot 10^{-20} &gt; 2^{-66}.\) If we can guarantee that
distinct inputs collide with probability significantly less than
\(2^{-66}\), e.g., \(&lt; 2^{-70},\) any collision is far
more likely to have been caused by a bug in our code or by
hardware failure than by the fingerprinting algorithm itself.</p>

<p>Using cryptographic hashes is certainly safe enough, but requires a lot of
CPU time, and, more importantly, worsens latency on smaller keys (for
which caching may not be that beneficial, such that our goal should be
to minimise overhead).  It’s not that state-of-the-art cryptographic
hash functions are wasteful, but that they defend against attacks like key
recovery or collision amplification that we may not care to consider
in our design.</p>

<p>At the other extreme of the hash spectrum, there is a plethora of fast
hash functions with no proof of collision probability.  However, most
of them are keyed on just a 64-bit “seed” integer, and that’s already
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=4">enough for a pigeonhole argument</a>
to show we can construct sets of strings of length \(64m\) bits
where any two members collide with probability at least \(m/
2^{64}\). In practice, <a href="https://131002.net/siphash/#at">security researchers seem to find key-independent collisions wherever they look</a>
(i.e., the collision probability is on the order of 1 for some
particularly pathological sets of inputs), so it’s safe to assume that
lacking a proof of collision probability implies a horrible worst
case.  I personally wouldn’t put too much faith in “security claims”
taking the form of failed attempts at breaking a proposal.</p>

<p><a href="https://arxiv.org/abs/1503.03465">Lemire and Kaser’s CLHash</a> is one
of the few exceptions we found: it achieves a high throughput of 22
GB/s and comes with a proof of \(2^{-63}\)-almost-universality.
However, its finalisation step is slow (23 ns for one-byte inputs), due
to a <a href="https://en.wikipedia.org/wiki/Barrett_reduction">Barrett reduction</a>
followed by
<a href="https://github.com/lemire/clhash/blob/742f81a66c8e2ae7889d1bc4c4b4d8734bdcd5af/src/clhash.c#L243">three rounds of <code>xorshift</code>/multiply mixing</a>.
<a href="https://eprint.iacr.org/2007/338">Dai and Krovetz’s VHASH</a>,
which inspired CLHash, offers similar guarantees, with worse
performance.</p>

<p>Unfortunately, \(2^{-63}\) is also not quite good enough for our
purposes: we estimate that the probability of uncorrectable memory
errors is on the order of \(2^{-66}\) per clock cycle, so we want
the collision probability for any two distinct inputs to be
comfortably less than that, around \(2^{-70}\) (i.e., \(10^{-21}\)) or
less.  This also tells us that any acceptable fingerprint must consist
of more than 64 bits, so we will have to either work in slower
multi-word domains, or combine independent hashes.</p>

<p>Interestingly, we also don’t need much more than that for
(non-adversarial) fingerprinting: at some point, the theoretical
probability of a collision is dominated by the practical possibility of
a hardware or networking issue making our program execute the
fingerprinting function incorrectly, or pass the wrong data to
that function.</p>

<p>While CLHash and VHASH aren’t quite what we want, they’re pretty
close, so we felt it made sense to come up with a specialised solution
for our fingerprinting use case.</p>

<p><a href="https://tools.ietf.org/html/rfc4418">Krovetz et al’s RFC 4418</a> brings
an interesting idea: we can come up with a fast 64-bit hash function
structured to make it easy to compute a second independent hash value,
and concatenate two independent 64-bit outputs.  The hash function can
heavily favour computational efficiency and let each 64-bit half
collide with probability \(\varepsilon\) significantly worse than
\(2^{-64}\), as long as the collision probability for the
concatenated fingerprint, \(\varepsilon^2\), is small enough, i.e.,
as long as
\(\varepsilon^2 &lt; 2^{-70} \Longleftrightarrow \varepsilon &lt; 2^{-35}\).
We get a more general purpose hash function out of the deal, and the
fingerprint comparison logic is now free to only compute and look at
half the fingerprint when it makes sense (e.g., in a prepass that
tolerates spurious matches).</p>

<h2 id="umash-at-a-high-level"><a id="umash-high-level"></a>UMASH, at a high level</h2>

<p>The design of UMASH is driven by two observations:</p>

<ol>
  <li>
    <p>CLHash achieves a high throughput, but introduces a lot of
latency to finalise its 127-bit state into a 64 bits result.</p>
  </li>
  <li>
    <p>We can get away with a significantly weaker hash, since we plan to
combine two of them when we need a strong fingerprint.</p>
  </li>
</ol>

<p>That’s why we started with the high-level structure diagrammed below,
the same as
<a href="https://web.cs.ucdavis.edu/~rogaway/papers/umac.html">UMAC</a>,
<a href="https://eprint.iacr.org/2007/338">VHASH</a>, and
<a href="https://arxiv.org/abs/1503.03465">CLHash</a>:
a fast first-level block compression function based on
<a href="https://dl.acm.org/doi/10.1109/TC.1968.227420">Winograd’s pseudo dot-product</a>,
and a second-level <a href="https://link.springer.com/chapter/10.1007/3-540-55719-9_77">Carter-Wegman polynomial hash function</a>
to accumulate the compressed outputs in a fixed-size state.</p>

<iframe frameborder="0" style="width:100%;height:523px;" src="https://viewer.diagrams.net/?highlight=0000ff&amp;edit=_blank&amp;layers=1&amp;nav=1&amp;title=UMASH%20overview#R7VzLcqM4FP0aL5MCIR5ZxulMuxepTpWre5ZTipGNJgJRIGJ7vn4kA7ZB6jSpsgUN2SRwAQHnHMR94ZnzEO%2B%2BZiiNnliI6QxY4W7mfJkBEHie%2BCsN%2B9Lg%2BVZp2GQkLE32ybAk%2F%2BHKWO9WkBDnjR05Y5STtGlcsSTBK96woSxj2%2BZua0abZ03RBiuG5QpR1fo3CXlU3ZZrnewLTDZRfWbbqrbEqN65MuQRCtn2zOQ8zpyHjDFeLsW7B0wldjUu5XF%2F%2FWLr8cIynPAuBxTwpxMvof9j8f0nCZ4eg%2BTVugmqYd4QLao7rq6W72sIxDACbbEyF7eQSuOKskKMOt9GhONlilbSuBX8C1vEYyrWbLFYDY0zjne%2FvGj7CIWQEGYx5tle7FId4MMKvX0ti2p9eyLDrrUSnRFxV9lQxf%2FmOPQJIrFQofQBxODvActYkYRYDmIZAckOmiBBTwXJczQgedcCyVVA%2BpakBT%2FAlJFkIxbYWvyRA73suXjA2xgKNHgTKHEke8UPjLJMWBKWSEWuCaUtE6Jkk0iRCvSwsM8ltkQ80ffVhpiEoTyNlpkmdxcgB7gtcgKVnKPtnBxwLXKC4SvYgZrHHJhU8J0CEnC9G6lVKVnKVq%2Fjlax95zbZ0MwndyYVW0%2FwQ5KsYw9Nsrb6Lp%2BOZh0wNM2C4WkWwsFp1pmwZqE7NM0O0Ll1%2FcFpVvVup6NZNxiaZr3hada3OmhWF7VeT7O%2BgtIzEjpC8kichOMXrt%2F213oXrhpiKOifw16njyQeIcqjIzjdga7zN%2FFuI5N1t2WCDJT%2F5bASG%2BvW8eSyxA2IFShXSIZXnLDkQG8m8ZonjK%2Biy%2FEDbP%2B2ObVAW2UIuipD8FoMgQ7JsQkx5MDhMdTByZ4QQ9AfHkOqgz9lhjxreAx1cPonxJDvgKEx5HTwFE7%2Bkm3ExQVNbwrq6jJGo7Ia%2FTOQliklsuZQpPLgpPZxR1xsaOV3dMrVlRquplzYwYPqWblAU48xrFzViXlezGRJO04znOcyQkNiiht9iNZS7zEc6029n77L%2BxGajiGjb0b46bu8H6H1z5CaLJ0yQ2qE1j9DHRKoE2LIs9r%2Bf%2F8MqcnbKTOkRmj9MzTEnppWFwdwVJB0CW%2F3aiCpYax9LJsdPF5W8EOX2FgdXIWSWqR91SBcNWjuXbftTo7edeuqQezEdKtQ0rtuO0Rmxptr3KHpVg2OJqZbhZLeddshGjLeYNPqve1ft2pAMjHdKpT0rtsOEYjxJht7aLpVg4CJ6VahpHfdDr9yZvuqbM3WHzw1CHhmVF5xhHJZdghqEUeIvo35W532x2aa0pDRAoT3ofKZITfXaYEEetcvUFDy4M3LofSLaMxyflMkRNx0fuh2LEU9WgnDpstra1pPjU7BXodQrcS6%2Fr4WtIhopSLXlG1XEcr4LUoSxpFMNv4DzpigeC25pOgF02eWkyobmZU3O08ZkUw9vglE88s9F3Z78tDM60a%2FwfQ61MbGCLyuFcUs8B2CvDECr%2BukMAt8hyy8oTrJv0Wcfkvuz6sltlvXSpxAnkNuW2Akz%2Bm6F3It23OQ7t3sayg5foxwcU581bn8gzhZU5IuLvfEtL9l0vKj8zCvyM%2Bf4GJ29DCvFtn773iYP57ul1OI7WHrdwyAGtob%2FRUD%2F7M5q0GP124s0fifRmvi%2Fody3b3kXjSfgJkNXX011f1EdsIg5hbZ95lhGbaSF7of8cTS5EQjW6M5F79DqnuM3rvmWTDqvAeqo7gTqgUWxcmGyzTkoYOfoly%2BdaeRTvd%2Bn8vxLvPOFaunXyI7bDv7OTfn8X8%3D"></iframe>

<p>The inner loop in this two-level strategy is the block compressor,
which divides each 256-byte block \(m\) into 32 64-bit values
\(m_i\), combines them with randomly generated parameters \(k_i\),
and converts the resulting sequence of machine words to a 16-byte
output.  The performance of that component will largely determine the
hash function’s global peak throughput.  After playing around with
<a href="https://web.cs.ucdavis.edu/~rogaway/umac/umac_thesis.pdf#page=41">the <code>NH</code> inner loop</a>,</p>

<div>
\[ \mathtt{NH}_k(m) = \sum_{i=0}^{|m| / 2 - 1} (m_{2i} + k_{2i} \bmod 2^{64})\cdot(m_{2i + 1} + k_{2i + 1} \bmod 2^{64})\mod 2^{128},\]
</div>

<p>we came to the
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=13">same conclusion as Lemire and Kaser</a>:
the scalar operations, the outer 128-bit ones in particular, map to too
many µops.  We thus focused on the
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.105.9929&amp;rep=rep1&amp;type=pdf#page=3">same <code>PH</code> inner loop</a>
as CLHash,</p>

<div>
\[ \mathtt{PH}_k(m) = \bigoplus_{i=0}^{|m| / 2 - 1} (m_{2i} \oplus k_{2i})\odot (m_{2i + 1} \oplus k_{2i + 1}).\]
</div>

<p>While the similarity to <code>NH</code> is striking, analysing <code>PH</code> is actually
much simpler: we can see the <code>xor</code> and carry-less multiplications as
working in the same ring of polynomials over \(\mathrm{GF}(2)\), unlike
<code>NH</code>’s mixing of \(\bmod 2^{64}\) for the innermost additions with
\(\bmod 2^{128}\) for the outer multiplications and sum.  In
fact, as
<a href="http://cr.yp.to/antiforgery/pema-20071022.pdf#page=6">Bernstein points out</a>,
<code>PH</code> is a direct application of
<a href="https://dl.acm.org/doi/10.1109/TC.1968.227420">Winograd’s pseudo dot-product</a>
to compute a multiplicative vector hash in half the multiplications.</p>

<p>CLHash uses an aggressively throughput-optimised block size of 1024
bytes.  We found diminishing returns after 256 bytes, and stopped
there.</p>

<p>With modular or polynomial ring arithmetic, the collision probability
is \(2^{-64}\) for any pair of blocks.  Given this fast compression
function, the rest of the hashing algorithm must chop the input in
blocks, accumulate compressed outputs in a constant-size state, and
handle the potentially shorter final block while avoiding
<a href="https://en.wikipedia.org/wiki/Length_extension_attack">length extension issues</a>.</p>

<p>Both VHASH and CLHash accumulate compressed outputs in a
polynomial string hash over a large field
(\(\mathbb{Z}/M_{127}\mathbb{Z}\) for VHASH, and
\(\mathrm{GF}(2^{127})\) with irreducible polynomial \(x^{127} + x + 1\)
for CLHash): the collision probability for polynomial string hashes is
inversely proportional to the field size and grows with the string
length (number of compressed blocks), so working in fields much larger
than \(2^{64}\) lets the <code>NH</code>/<code>PH</code> term dominate.</p>

<p>Arithmetic in such large fields is slow, and reducing the 127-bit
state to 64 bits is also not fast.  CLHash and VHASH make the
situation worse by zero-padding the final block, and CLHash defends
against length extension attacks with
<a href="https://arxiv.org/pdf/1503.03465.pdf#page=10">a more complex mechanism</a>
than <a href="https://eprint.iacr.org/2007/338">the one in VHASH</a>.</p>

<p>Similarly to VHASH, UMASH uses a polynomial hash over the (much
smaller) prime field \(\mathbb{F} = \mathbb{Z}/M_{61}\mathbb{Z},\)</p>

<div>
\[ CW_f(y) = \left(\sum_{j=0}^{d - 1} y_j \cdot f^{d - j}\right) \mod 2^{61} - 1, \]
</div>

<p>where \(f\in\mathbb{F}\) is the randomly chosen point at which we
evaluate the polynomial, and \(y\), the polynomial’s coefficients, is
the stream of 64-bit values obtained by splitting in half the <code>PH</code>
output for each block.  This choice saves 20-30 cycles of latency in
the final block, compared to CLHash: modular multiplications have
lower latency than carry-less multiplications for judiciously picked
machine-integer-sized moduli, and integer multiplications seem to mix
better, so we need less work in the finaliser.</p>

<p>Of course, UMASH sacrifices a lot of strength by working in
\(\mathbb{F} =\, \bmod 2^{61} - 1:\) the resulting field is much
smaller than \(2^{127}\), and we now have to update the polynomial
twice for the same number of blocks.  This means the collision
probability starts worse \((\approx 2^{-61}\) instead of \(\approx
2^{-127})\), and grows twice as fast with the number of blocks
\(n\) \((\approx 2n\cdot 2^{-61}\) instead of \(\approx n\cdot
2^{-61})\).  But remember, we’re only aiming for collision
probability \(&lt; 2^{-35}\) and each block represents 256 bytes of
input data, so this is acceptable, assuming that multi-gigabyte inputs
are out of scope.</p>

<p>We protect against <a href="https://en.wikipedia.org/wiki/Length_extension_attack">length extension collisions</a>
by <code>xor</code>ing (adding, in the polynomial ring) the original byte size of
the final block to its compressed <code>PH</code> output.  This <code>xor</code> is simpler
than CLHash’s finalisation step with a carry-less multiplication, but
still sufficient: we can adapt
<a href="http://krovetz.net/csus/papers/vmac.pdf#page=11">Krovetz’s proof for VHASH</a>
by replacing <code>NH</code>’s almost-\(\Delta\)-universality with <code>PH</code>’s
almost-XOR-universality.</p>

<p>Having this protection means we can extend short final blocks however
we want.  Rather than conceptually zero-padding our inputs (which adds
complexity and thus latency on short inputs), we allow redundant
reads.  We bifurcate inputs shorter than 16 bytes to a completely
different latency-optimised code path, and let the final <code>PH</code>
iteration read the last 16 bytes of the input, regardless of how
redundant that might be.</p>

<p>The
<a href="https://github.com/backtrace-labs/umash/blob/master/umash_reference.py">semi-literate Python reference implementation</a>
has the full code and includes more detailed analysis and rationale
for the design decisions.</p>

<h2 id="internal-implementation-tricks"><a id="implementation-tricks"></a>Internal implementation tricks</h2>

<p>The previous section already showed how we let micro-optimisation
inform the high-level structure of UMASH.  The use of <code>PH</code> over
<code>NH</code>, our choice of a polynomial hash in a small modular field, and
the way we handle short blocks all aim to improve the performance of
production implementations.  We also made sure to enable a couple more
implementation tricks with lower level design decisions.</p>

<p>The block size is set to 256 bytes because we observed diminishing
returns for larger blocks… but also because it’s reasonable to cache
the <code>PH</code> loop’s parameters in 8 AVX registers, if we need to shave load
µops.</p>

<p>More importantly, it’s easy to implement a
<a href="https://en.wikipedia.org/wiki/Horner%27s_method">Horner update</a>
with the prime modulus \(2^{61} - 1\).  Better, that’s also true for a
“double-pumped” Horner update, \(h^\prime = H_f(h, a, b) = af + (b +
h)f^2.\)</p>

<p>The trick is to work in \(\bmod 2^{64} - 8 = \bmod 8\cdot(2^{-61} -
1),\) which lets us implement modular multiplication of an arbitrary
64-bit integer \(a\) by a multiplier \(0 &lt; f &lt; 2^{61} - 1\)
without worrying too much about overflow. \(2^{64} \equiv 8 \mod
2^{64} - 8,\) so we can reduce a value \(x\) to a smaller
representative with</p>

<div>
\[af = x \equiv 8\lfloor x / 2^{64}\rfloor + (x\bmod 2^{64}) \mod 2^{64} - 8;\]
</div>

<p>this equivalence is particularly useful when \(x &lt; 2^{125}\): in that
case, \(x / 2^{64} &lt; 2^{61},\) and the intermediate product
\(8\lfloor x / 2^{64}\rfloor &lt; 2^{64}\) never overflows 64 bits.
That’s exactly what happens when \(x = af\) is the product of
\(0\leq a &lt; 2^{64}\) and \(0 &lt; f &lt; 2^{61} - 1\).  This also
holds when we square the multiplier \(f\): it’s sampled from the
field \(\mathbb{Z}/(2^{61} - 1)\mathbb{Z},\) so its square also
satisfies \(f^2 &lt; 2^{61}\) once fully reduced.</p>

<p>Integer multiplication instructions for 64-bit values will naturally
split the product \(x = af\) in its high and low 64-bit half; we get
\(\lfloor x / 2^{64}\rfloor\) and \(x\bmod 2^{64}\) for free.  The
rest of the double-pumped Horner update is a pair of modular
additions, where only the final sum must be reduced to fit in \(\bmod
2^{64} - 8\).  The resulting instruction-parallel double Horner
update is only a few cycles slower than a single Horner update.</p>

<p>We also never fully reduce to \(\bmod 2^{61} - 1\). While the
collision bound assumes that prime field, we simply work in its \(\bmod
2^{64} - 8\) extension.  This does not affect the collision bound,
and the resulting expression is still amenable to algebraic
manipulation: modular arithmetic is a well defined ring even for
composite moduli.</p>

<h2 id="smhasher-tricks">SMHasher tricks</h2>

<p>A proof of almost-universality doesn’t mean a hash passes the SMHasher
test suite.  It should definitely guarantee collisions are (probably)
rare enough, but SMHasher also looks at bit avalanching and bias, and
universality is oblivious to these issues.  Even XOR- or
\(\Delta\)-universality doesn’t suffice: the hash values for a
given string are well distributed when parameters are chosen
uniformly at random, but this does not imply that hashes are always (or
usually) well distributed for fixed parameters.</p>

<p>The most stringent SMHasher tests focus on short inputs: mostly up to
128 or 256 bits, unless “Extra” torture testing is enabled.  In a way,
this makes sense, given that arbitrary-length string hashing is
provably harder than the bounded-length vector case.  Moreover, a
specialised code path for these inputs is beneficial, since they’re
relatively common and deserve strong and low-latency hashes.  That’s
why UMASH uses a completely different code path for inputs of at
most 8 bytes, and a specialised <code>PH</code> iteration for inputs of 9 to 16
bytes.</p>

<p>However, this means that SMHasher’s best avalanche and
bias tests often tell us very little about the general case.  For
UMASH, the medium length (9 to 16 bytes) code path at least
shares the same structure and finalisation logic as the code for
longer inputs.</p>

<p>There may also be a bit of co-evolution between the test harness and
the design of hash functions: the sort of <code>xorshift</code>/multiply mixers
favoured by Appleby in the various versions of MurmurHash tends to do
well on SMHasher.  These mixers are also invertible, so we can take
any hash function with good collision properties, mix its output with
someone else’s series of <code>xorshift</code> and multiplications (in UMASH’s
case, the
<a href="http://prng.di.unimi.it/splitmix64.c">SplitMix64 update function</a>
or a subset thereof), and usually find that the result satisfies
SMHasher’s bias and avalanche tests.</p>

<p>It definitely looks like interleaving rightward bitwise operations and
integer multiplications is a good mixing strategy.  However, I find it
interesting that the hash evaluation harness created by the author of
MurmurHash steers implementations towards MurmurHash-syle mixing code.</p>

<h2 id="fun-things-to-do-with-umash"><a id="usage"></a>Fun things to do with UMASH</h2>

<p>The structure of UMASH lets us support more sophisticated usage
patterns than merely hashing or fingerprinting an array of bytes.</p>

<p>The <code>PH</code> loop needs less than 17 bytes of state for its 16-byte
accumulator and an iteration count, and the polynomial hash also needs
17 bytes, for its own 8-byte accumulator, the 8-byte “seed,” and a
counter for the final block size (up to 256 bytes).  The total comes up to
34 bytes of state, plus a 16-byte input buffer, since the <code>PH</code> loop
consumes 16-byte chunks at a time.  Coupled with the way we only
consider the input size at the end of UMASH, this makes it easy to
implement incremental hashing.</p>

<p>In fact, the state is small enough that our implementation stashes some
parameter data inline in the state struct, and uses the same
layout for hashing and fingerprinting with a pair of hashes (and thus
double the state): most of the work happens in <code>PH</code>, which only
accesses the constant parameter array, the shared input buffer and iteration
counter, and its private 16-byte accumulator.</p>

<p>Incremental fingerprinting is a crucial capability for our caching
system: cache keys may be large, so we want to avoid serialising them
to an array of contiguous bytes just to compute a fingerprint.
Efficient incrementality also means we can hash NUL-terminated C
strings with a fused UMASH / <code>strlen</code> loop, a nice speed-up when the
data is in cache.</p>

<p>The outer polynomial hash in UMASH is so simple to analyse that we
can easily process blocks out of order.  In my experience, such a
“parallel hashing” capability is more important than peak
throughput when checksumming large amounts of data coming over the
wire.  We usually maximise transfer throughput by asking for several
ranges of data in parallel.  Having to checksum these ranges in
order introduces a serial bottleneck and the usual head-of-line
blocking challenges; more importantly, checksumming in order adds
complexity to code that should be as obviously correct as possible.
The polynomial hash lets us hash an arbitrary subsequence of 256-byte
aligned blocks and use modular exponentiation to figure out its impact
on the final hash value, given the subsequence’s position in the
checksummed data.  Parallel hashing can exploit multiple cores (more
cores, more bandwidth!) with simpler code.</p>

<p>The <a href="https://tools.ietf.org/html/rfc4418">UMAC RFC</a> uses a Toeplitz
extension scheme to compute independent <code>NH</code> values while recycling
most of the parameters.  We do the same with <code>PH</code>, by adapting
<a href="https://web.cs.ucdavis.edu/~rogaway/umac/umac_thesis.pdf#page=51">Krovetz’s proof</a>
to exploit <code>PH</code>’s almost-XOR-universality instead of <code>NH</code>’s
almost-\(\Delta\)-universality.  Our fingerprinting code reuses all
but the first 32 bytes of <code>PH</code> parameters for the second hash: that’s
the size of an AVX register, which makes is trivial to avoid loading
parameters twice in a fused <code>PH</code> loop.</p>

<p>The same RFC also points out that concatenating the output of fast
hashes lets validation code decide which speed-security trade-off
makes sense for each situation: some applications may be willing to
only compute and compare half the hashes.</p>

<p>We use that freedom when reading from large hash tables keyed on
the UMASH fingerprint of strings.  We compute a single UMASH
hash value to probe the hash tables, and only hash the second half
of the fingerprint when we find a probable hit.  The idea is that
hashing the search key (now hot in cache) a second time will be faster
than comparing it against the hash entry’s string key in cold storage.</p>

<p>When we add this sort of trickery to our code base, it’s important to
make sure the interfaces are hard to misuse.  For example, it would be
unfortunate if only one half of the 128-bit fingerprint were well
distributed and protected against collisions: this would make it far
too easy to implement the two-step lookup-by-fingerprint above
<em>correctly but inefficiently</em>.  That’s why we maximise the symmetry in
the fingerprint: the two 64-bit halves are computed with the same
algorithm to guarantee the same worst-case collision probability and
distribution quality.  This choice leaves fingerprinting throughput on
the table when a weaker secondary hash would suffice.  However, I
prefer a safer if slightly slower interface to one ripe for silent
performance bugs.</p>

<h2 id="caveat-programmator">Caveat programmator</h2>

<p>While we intend for UMASH to become our default hash and fingerprint
function, it can’t be the right choice for every application.</p>

<p>First, it shouldn’t be used for authentication or similar
cryptographic purposes: the implementation is probably riddled with
side-channels, the function has no protection against parameter
extraction or adaptive attacks, and collisions are too frequent
anyway.</p>

<p>Obviously, this rules out using UMASH in a
<a href="https://en.wikipedia.org/wiki/Message_authentication_code">MAC</a>, but
might also be an issue for, e.g., hash tables where attackers control
the keys and can extrapolate the hash values.  A timing side-channel
may let attackers determine when keys collide; once a set of colliding
keys is known, the linear structure of UMASH makes it trivial to
create more collisions by combining keys from that set.  Worse,
iterating over the hash table’s entries can leak the hash values,
which would let an attacker slowly extract the parameters.  We
conservatively avoid non-cryptographic hashes and even hashed data
structures for sections of the <a href="https://backtrace.io">Backtrace</a> code
base where such attacks are in scope.</p>

<p>Second, the performance numbers reported by
<a href="https://github.com/rurban/smhasher">SMHasher</a> (up to 22 ns when
hashing 64 bytes or less, and 22 GB/s peak throughput) are probably a
lie for real applications, even when running on the exact same 2.5 GHz
Xeon 8175M hardware.  These are best-case values, when the code and
the parameters are all hot in cache… and that’s a fair amount of
bytes for UMASH.  The instruction footprint for a 64-bit hash is 1435
bytes (comparable to heavier high-throughput hashes, like the
1600-byte <a href="https://github.com/Cyan4973/xxHash">xxh3_64</a> or 1350-byte
<a href="https://github.com/google/farmhash">farmhash64</a>), and the parameters
span 288 bytes (320 for a fingerprint).</p>

<p>There is a saving grace for UMASH and other complex hash functions:
the amount of bytes executed is proportional to the input size (e.g.,
the code for 8 or fewer byte only needs 141 bytes, and would inline to
around 100 bytes), and the number of parameters read is bounded by the
input length.  Although UMASH can need a lot of instruction and
parameter bytes, the worst case only happens for larger inputs, where
the cache misses can hopefully be absorbed by the work of loading and
hashing the data.</p>

<p>The numbers are also only representative of powerful CPUs with
carry-less multiplication in hardware.  The <code>PH</code> inner loop has 50%
higher throughput than <code>NH</code> (22 vs 14 GB/s) on contemporary Intel
servers.  The carry-less approach still has an edge over 128-bit
modular arithmetic on <a href="https://en.wikichip.org/wiki/amd/cores/naples">AMD’s Naples</a>,
but less so, around 20-30%. We did not test on ARM (the
<a href="https://help.backtrace.io/en/articles/2428859-web-console-overview">Backtrace database</a>
only runs on x86-64), but I would assume the situation there is closer
to AMD’s than Intel’s.</p>

<p>However, I also believe we’re more likely to observe improved
performance for <code>PH</code> than <code>NH</code> in future micro-architectures: the core
of <code>NH</code>, full-width integer multiplication, has been aggressively
optimised by now, while the gap between Intel and AMD shows there
may still be low-hanging fruits for the carry-less multiplications
at the heart of <code>PH</code>.  So, <code>NH</code> is probably already as good as
it’s going to be, but we can hope that <code>PH</code> will continue to benefit from
hardware optimisations, as chip designers improve the performance of
cryptographic algorithms like
<a href="https://en.wikipedia.org/wiki/Galois/Counter_Mode">AES-GCM</a>.</p>

<p>Third and last, UMASH isn’t fully stabilised yet.  We do not plan to
modify the high level structure of UMASH, a <code>PH</code> block compressor
that feeds into a polynomial string hash.  However, we are looking for
suggestions to improve its latency on short inputs, and to simplify
the finaliser while satisfying SMHasher’s distribution tests.</p>

<h2 id="help-us-improve-umash">Help us improve UMASH</h2>

<p>We believe UMASH is ready for non-persistent usage: we’re confident in
its quality, but the algorithm isn’t set in stone yet, so hash or
fingerprint values should not reach long-term storage.  We do not
plan to change anything that will affect the proof of collision
bound, but improvements to the rest of the code are more than
welcome.</p>

<p>In particular:</p>

<ol>
  <li>The short (8 or fewer bytes) input code can hopefully be simpler.</li>
  <li>The medium-length (9-15 bytes) input code path is a micro-optimised
version of the general case, but does not actually share any machine
code; can we improve the latency and maintain the collision bound
by replacing it with something completely different?</li>
  <li>It’s already nice that we can get away with a single round of
<code>xorshift</code> / multiply in the finaliser, but can we shave even
more latency there?</li>
  <li>We only looked at straightforward x86-64 implementations; we will
consider tweaks that improve performance on x86-64, or on
other platforms without penalising x86-64.</li>
  <li>We currently only use incremental and one-shot hashing interfaces.
If someone needs parallel hashing, we can collaborate to find out
what that interface could look like.</li>
</ol>

<p>A hash function is a perfect target for automated correctness and
performance testing.  I hope to use UMASH as a test bed for the
automatic evaluation (and approval?!) of pull requests.</p>

<p>Of course, you’re also welcome to just use
<a href="https://github.com/backtrace-labs/umash/blob/master/umash.c">UMASH as a single-file C library</a>
or re-implement it to fit your requirements.
<a href="https://github.com/backtrace-labs/umash">The MIT-licensed C code is on GitHub</a>,
and we can definitely discuss validation strategies for alternative
implementations.</p>

<p>Finally, our fingerprinting use case shows collision rates are
probably not something to minimise, but closer to soft constraints.
We estimate that, once the probability reaches \(2^{-70}\),
collisions are rare enough to only compare fingerprints instead of the
fingerprinted values.  However, going lower than \(2^{-70}\)
doesn’t do anything for us.</p>

<p>It would be useful to document other back-of-the-envelope requirements
for a hash function’s output size or collision rate.  Now that most
developers work on powerful 64-bit machines, it seems far too easy to
add complexity and waste resources for improved collision bounds that
may not unlock any additional application.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Any error in the analysis or the code is mine, but a few people
helped improve UMASH and its presentation.</p>

<p><a href="https://www.daemonology.net/blog/">Colin Percival</a> scanned an earlier
version of the reference implementation for obvious issues, encouraged
me to simplify the parameter generation process, and prodded us to
think about side channels, even in data structures.</p>

<p>Joonas Pihlaja helped streamline my initial attempt while making the
reference implementation easier to understand.</p>

<p><a href="https://github.com/jshufro">Jacob Shufro</a> independently confirmed
that he too found the reference implementation understandable, and
tightened the natural language.</p>

<p>Phil Vachon helped me gain more confidence in the implementation
tricks borrowed from VHASH after replacing the <code>NH</code> compression
function with <code>PH</code>.</p>
]]></content>
  </entry>
  
</feed>
