<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: appnexus | Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/Blog/categories/appnexus/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2020-07-09T12:19:41-04:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Pointer-less Scapegoat Trees]]></title>
    <link href="https://www.pvk.ca/Blog/2015/04/26/pointer-less-scapegoat-trees/"/>
    <updated>2015-04-26T23:30:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2015/04/26/pointer-less-scapegoat-trees</id>
    <content type="html"><![CDATA[<p>I’m trying something new this week: gathering a small group after
<a href="http://www.appnexus.com">work</a> for 90 minutes of short talks and
discussions.  We’ll also have one longer slot because not everything
fits in a postcard, but my main goal is really to create opportunities
for everyone to infect us with their excitement for and interest in an
idea or a question.  I successfully encouraged a couple people to
present, although many seemed intimidated by the notion… perhaps because we
have grown to expect well researched and rehearsed performances.
However, I believe that simple presentations of preliminary work are
worthwhile, and probably more likely to spark fresh conversations than
the usual polished fare: it’s healthy to expose our doubts,
trials, and errors, and there’s certainly value in reminding ourselves
that everyone else is in that same boat.</p>

<p>Here’s what I quickly (so quickly that my phone failed to focus
correctly) put together on embedding search trees in sorted
arrays.  You’ll note that the “slides” are very low tech; hopefully,
more people will contribute their own style to the potluck next time
(:</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat01.jpg" /></p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat02.jpg" /></p>

<p>I didn’t really think about implementing search trees until 3-4 years
ago.  I met an online collaborator in Paris who, after a couple G&amp;T,
brought up the topic of “desert island” data structures: if you were
stuck with a computer and a system programming guide on a desert
island, how would you rebuild a standard library from scratch?
Most data structures and algorithms that we use every day are fairly
easy to remember, especially if we don’t care about proofs of
performance: basic dynamic memory allocation, hash tables, sorting,
not-so-bignum arithmetic, etc. are all straightforward.  He even had a
mergeable priority queue, with
<a href="http://en.wikipedia.org/wiki/Skew_heap">skew heaps</a>.  However, 
we both got stuck on balanced search trees: why would anyone want to
remember rotation rules?  (Tries were rejected on what I argue are
purely theoretical grounds ;)</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat03.jpg" /></p>

<p>I
<a href="http://www.pvk.ca/Blog/2012/07/03/binary-search-star-eliminates-star-branch-mispredictions/">love searching</a>
in <a href="http://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/">sorted arrays</a>, so I kept looking for a way
to build simpler search trees on top of that.  That lead me to
<a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=3453&amp;context=compsci">Bentley and Saxe’s (PDF)</a> dynamisation trick.  The gist of it is
that there’s a family of methods to build dynamic sets on top of
static versions.  For sorted arrays, one extreme is an unsorted list
with fast inserts and slow reads, and the other exactly <em>one</em> sorted
array, with slow inserts and fast lookups.  The most interesting
design point lies in the middle, with \( \log n \) sorted arrays,
yielding \( \mathcal{O}(\lg n) \) time inserts and
\( \mathcal{O}(\lg\sp{2}n) \) lookups; we can see that design in
write-optimised databases.  The problem is that my workloads tend to
be <em>read</em> heavy.</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat04.jpg" /></p>

<p>Some time later, I revisited a <a href="http://www.cs.au.dk/~gerth/papers/soda02.pdf">paper by Brodal, Fagerberg, and Jacob (PDF)</a>.  They
do a lot of clever things to get interesting performance bounds, but
I’m really not convinced it’s all worth the complexity<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>… especially in
the context of our desert island challenge.  I did find one trick very
interesting: they preserve logarithmic time lookups when binary
searching arrays with missing values by recasting these arrays as
implicit binary trees and guaranteeing that “NULLs” never have valid
entries as descendants.  That’s a lot simpler than other arguments
based on guaranteeing a minimum density.  It’s so much simpler that we
can easily make it work with a branch-free binary search:  we only
need to treat NULLs as \( \pm \infty \) (depending on whether we
want a predecessor or a successor).</p>

<p>While lookups are logarithmic time, inserts are
\(\mathcal{O}(\lg\sp{2} n) \) time.  Still no satisfying
answer to the desert island challenge.</p>

<p>I went back to my real research in optimisation, and somehow
stumbled on
<a href="http://dspace.mit.edu/handle/1721.1/10639">Igal Galperin’s PhD thesis</a>
on both on-line optimisation/learning and… simpler balanced binary search trees!</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat05.jpg" /></p>

<p><a href="http://www.akira.ruc.dk/~keld/teaching/algoritmedesign_f03/Artikler/03/Galperin93.pdf">Scapegoat trees (PDF)</a>
rebalance by guaranteeing a bound \( \alpha &gt; 0 \) on the
relative difference between the optimal depth
(\( \lceil\lg n\rceil \)) for a set of \(n\) values and the height (maximal depth) of
the balanced tree (at most \( (1+\alpha)\lceil\lg n\rceil \)).  The
only property that a scapegoat tree has (in addition to those of
binary search trees) is this bound on the height of the 
tree, as a function of its size.  Whenever a new node would be
inserted at a level too deep for the size of the tree, we go up
its ancestors to find a subtree that is small enough to accomodate
the newcomer and rebuild it from scratch.  I will try to provide an
intuition of how they work, but the paper is a much better source.</p>

<p>For a tree of \(n = 14\) elements, we could have \(\alpha =
0.25\), for a maximum depth of \(1.25 \lceil\lg 14\rceil = 5\).
Let’s say we attempt to insert a new value, but the tree is structured such
that the value would be the child of a leaf that’s already at depth \(5\);
we’d violate the (im)balance bound.  Instead, we go up until we find
an ancestor \(A\) at depth, e.g., \(3\) with \(4\) descendants.  The
ancestor is shallow enough that it has space for \(5 - 3 = 2\)
levels of descendants, for a total height of \(2 + 1 = 3\) for the
subtree.  A full binary tree of height \(3\) has 
\(2\sp{3} - 1 = 7\) nodes, and we thus have enough space for
\(A\), its \(4\) descendants, and the new node!  These 6 values
are rebuilt in a near-perfect binary tree: every level must be
fully populated, except for the last one.</p>

<p>The criteria to find the scapegoat subtree are a bit
annoying to remember–especially given that we don’t want to
constantly rebuild the whole tree–but definitely simpler than rotation
rules.  I feel like that finally solves the desert island balanced
search tree challenge… but we still have gapped sorted arrays to
address.</p>

<p>What’s interesting about scapegoat trees is that rebalancing is always
localised to a subtree.  Rotating without explicit pointers is hard
(not impossible, amazingly enough), but scapegoat trees just
reconstruct the whole subtree, i.e., a contiguous section of the
sorted array.  That’s easy: slide non-empty values to the right, and
redistribute recursively.  But, again, finding the scapegoat subtree
is annoying.</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat06.jpg" /></p>
<center><small>The \\(\alpha\lg (n)\\) above should read \\((1 + \alpha)\lg n\\), and \\(\lg\lg n + \mathrm{Exp}\cdot\lg n\\) should be \\((\lg\lg n) (1 + \mathrm{Exp})\\).</small></center>

<p>That made me think: what if I randomised scapegoat selection?  Rather than
counting elements in subtrees, I could approximate that
probabilistically by sampling from an exponential distribution… which
we can easily approximate with the geometric for \(p = 0.5\) by
counting leading zeros in bitstrings.</p>

<p>I’m still not totally convinced that it works, but I vaguely remember
successfully testing an implementation and sketching a proof that we
can find the scapegoat subtree by going up according to a scaled
geometric to preserve amortised logarithmic time inserts.  The
probability function decreases quickly enough that we preserve
logarithmic time inserts on average, yet slowly enough that we can
expect to redistribute a region before it runs out of space.</p>

<p>The argument is convoluted, but the general idea is based
on the observation that, in a tree of maximum height \(m\), a
subtree at depth \(k\) can contain at most
\(n\sp\prime = 2\sp{m - k + 1} - 1\) elements (including the
subtree’s root).</p>

<p>We only violate the imbalance bound in a subtree if
we attempt to insert more than \(n\sp\prime\) elements in it.
Rebalancing works by designating the shallowest subtree that’s not yet
full as the scapegoat.  We could simplify the selection of the
scapegoat tree by counting the number of inserts in each subtree, but
that’d waste a lot of space.  Instead, we count probabilistically and
ensure that there’s a high probability (that’s why we always go up by
at least \(\lg \lg n\) levels) that each subtree will be
rebalanced at least once before it hits its insertion count limit.
The memoryless property of the geometric distribution means that
this works even after a rebalance.  If we eventually fail to find
space, it’s time to completely rebuild the subtree; this case happens
rarely enough (\(p \approx \frac{\lg n}{n}\)) that the amortised
time for insertions is still logarithmic.</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat07.jpg" /></p>
<center><small>Again \\(2\sp{\alpha \lg n}\\) should be \\(2\sp{(1 + \alpha)\lg n}\\).</small></center>

<p>We can do the same thing when embedding scapegoat trees in implicit
trees.  The problem is that a multiplicative overhead in depth results
in an exponential space blowup.  The upside is that the overhead is
tunable: we can use less space at the expense of slowing down
inserts.</p>

<p>In fact, if we let \( \alpha \rightarrow 0 \), we find Brodal et
al’s scheme (I don’t know why they didn’t just cite Galperin and
Rivest on scapegoat trees)!  The difference is that we are now pretty
sure that we can easily let a random number generator guide our
redistribution.</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat09.jpg" /></p>

<p>I only covered insertions and lookups so far.  It turns out that
deletions in scapegoat trees are easy: replace the deleted node with
one of its leaves.  Deletions should also eventually trigger
a full rebalance to guarantee logarithmic time lookups.</p>

<p><img class="center" src="/images/2015-04-26-pointer-less-scapegoat-trees/scapegoat10.jpg" /></p>

<p>Classical implicit representations for sorted sets make us choose
between appallingly slow (linear time) inserts and slow lookups.
With stochastic scapegoat trees embedded in implicit binary trees, we
get logarithmic time lookups, and we have a continuum of choices
between wasting an exponential amount of space and slow
\( \mathcal{O}(\lg\sp{2} n) \) inserts.  In order to get there, we
had to break one rule: we allowed ourselves \(\mathcal{O}(n)\)
additional space, rather than \(\mathcal{o}(n)\), but it’s all
<em>empty</em> space.</p>

<p>What other rule or assumption can we challenge (while staying true to
the spirit of searching in arrays)?</p>

<p>I’ve been thinking about interpolation lately: what if we had a
monotone (not necessarily injective) function to map from the set’s
domain to machine integers?  That’d let us bucket values or
interpolate to skip the first iterations of the search.  If we can
also assume that the keys are uniformly distributed once mapped to
integers, we can use a linear Robin Hood hash table: with a linear
(but small) space overhead, we get constant time expected inserts and
lookups, and what seems to be \( O(\lg \lg n) \) worst case<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>
lookups with high probability.</p>

<p>Something else is bothering me.  We embed in full binary trees, and
thus binary search over arrays of size \(2\sp{n} - 1\)… and we know
that’s a
<a href="http://www.pvk.ca/Blog/2012/07/30/binary-search-is-a-pathological-case-for-caches/">bad idea</a>.
We could switch to ternary trees, but that means inserts and deletes
must round to the next power of three.  Regular div-by-mul and
scaling back up by the divisor always works; is there a simpler way to round
to a power of three or to find the remainder by such a number?</p>

<p>I don’t know!  Can anyone offer insights or suggest new paths to explore?</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>I think jumping the van Emde Boa[s] is a thing, but they at least went for the minor version, the van Emde Boas layout ;) <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>The maximal distance between the interpolation point and the actual location appears to scale logarithmically with the number of elements.  We perform a binary search over a logarithmic-size range, treating empty entries as \(\infty\). <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance tuning ~ writing an essay]]></title>
    <link href="https://www.pvk.ca/Blog/2014/10/19/performance-optimisation-~-writing-an-essay/"/>
    <updated>2014-10-19T20:05:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2014/10/19/performance-optimisation-~-writing-an-essay</id>
    <content type="html"><![CDATA[<p>&lt;a href=”#trust-no-one”“&gt;Skip to the meaty bits.&lt;/a&gt;</p>

<p>My work at <a href="http://www.appnexus.com/">AppNexus</a> mostly involves
performance optimisation, at any level from microarchitecture-driven
improvements to data layout and assembly code to improving the
responsiveness of our distributed system under load.  Technically,
this is similar to what I was doing as a lone developer on
research-grade programs.  However, the scale of our (constantly
changing) code base and collaboration with a dozen other coders mean
that I approach the task differently: e.g., rather than
single-mindedly improving throughput <em>now</em>, I aim to pick an evolution
path that improves throughput today without imposing too much of a
burden on future development or fossilising ourselves in a design
dead-end.  So, although numbers still don’t lie (hah), my current
approach also calls for something like judgment and taste, as well as
a fair bit of empathy for others.  Rare are the obviously correct
choices, and, in that regard, determining what changes to make and
which to discard as
<a href="http://fun.irq.dk/funroll-loops.org/">over-the-top ricing</a> feels like
I’m drafting a literary essay.</p>

<p>This view is probably tainted by the fact that, between English and
French classes, I spent something like half of my time in High School
critiquing essays, writing essays, or preparing to write one.
Initially, there was a striking difference between the two languages:
English teachers had us begin with the five paragraph format where one
presents multiple arguments for the same thesis, while French teachers
imposed a thesis/antithesis/synthesis triad (and never really let it
go until CÉGEP, but that’s another topic).  When I write that
performance optimisation feels like drafting essays, I’m referring to
the latter “Hegelian” process, where one exposes arguments and
counterarguments alike in order to finally make a stronger case.</p>

<p>I’ll stretch the analogy further.  Reading between the lines gives us
access to more arguments, but it’s also easy to get the context wrong and
come up with hilariously far-fetched interpretations.  When I try to
understand a system’s performance, the most robust metrics treat the
system as a black box: it’s hard to get throughput under production
data wrong.  However, I need finer grained information (e.g.,
performance counters, instruction-level profiling, or
application-specific metrics) to guide my work, and, the more useful
that information can be – like domain specific metrics that highlight
what we could do differently rather than how to do the same thing more
efficiently – the easier it is to measure incorrectly.  That’s not a
cause for despair, but rather a fruitful line of skepticism that helps
me find more opportunities.</p>

<p>Just two weeks ago, questioning our application-specific metrics
lead to an easy 10% improvement in throughput for our biggest
consumer of CPU cycles.  The consumer is an application that
determines whether internet advertising campaigns are eligible to bid
on an ad slot, and if so, which creative (ad) to show and at what bid
price.  For the longest time, the most time-consuming part of that
process was the first step, testing for campaign eligibility.
Consequently, we tracked the execution of that step precisely and
worked hard to minimise the time spent on ineligible campaigns,
without paying much attention to the rest of the pipeline.  However,
we were clearly hitting diminishing returns in that area, so I asked
myself how an adversary could use our statistics to mislead us.  The
easiest way I could think of was to have campaigns that are eligible
to bid, but without any creative compatible with the ad slot (e.g.,
because it’s the wrong size or because the website forbids Flash ads):
although the campaigns are technically eligible, they are unable to
bid on the ad slot.  We added code to track these cases and found that
almost half of our “eligible” campaigns simply had no creative in the
right size.  Filtering these campaigns early proved to be a
low-hanging fruit with an ideal code complexity:performance
improvement ratio.</p>

<h1 id="trust-no-one-not-even-performance-counters"><a href="#trust-no-one" name="trust-no-one">Trust no one, not even performance counters</a></h1>

<p>I recently learned that we also had to second-guess instruction level
profiles.  Contemporary x86oids are out of order, superscalar, and
speculative machines, so profiles are always messy: “blame” is
scattered around the real culprit, and some instructions (pipeline
hazards like conditional jumps and uncached memory accesses, mostly)
seem to account for more than their actual share.  What I never
realised is that, in effect, some instructions systematically mislead
and push their cycles to others.</p>

<p>Some of our internal spinlocks use <code>mfence</code>.  I expected that to be
suboptimal, since it’s
<a href="https://blogs.oracle.com/dave/resource/NHM-Pipeline-Blog-V2.txt">common</a>
<a href="http://shipilev.net/blog/2014/on-the-fence-with-dependencies/">knowledge</a>
that <code>lock</code>ed instruction are more efficient barriers: serialising
instructions like <code>mfence</code> have to affect streaming stores and other
weakly ordered memory accesses, and that’s a lot more work than just
preventing store/load reordering.  However, our profiles showed that
we spent very little time on spinlocking so I never gave it much thought…
until eliminating a set of spinlocks had a much better impact on
performance than I would have expected from the profile.  Faced with
this puzzle, I had to take a closer look at the way <code>mfence</code> and
locked instructions affect hardware-assisted instruction profiles on
our production Xeon E5s (Sandy Bridge).</p>

<p>I came up with a simple synthetic microbenchmark to simulate locking
on my E5-4617: the loop body is an adjustable set of memory accesses
(reads and writes of out-of-TLB or uncached locations) or computations
(divisions) bracketed by pairs of normal stores, <code>mfence</code>, or <code>lock
inc/dec</code> to cached memory (I would replace the fences with an
increment/decrement pair and it looks like all read-modify-write
instructions are implemented similarly on Intel).  Comparing runtimes
for normal stores with the other instructions helps us gauge their
overhead.  I can then execute each version under <code>perf</code> and estimate
the overhead from the instruction-level profile.  If <code>mfence</code> is
indeed extra misleading, there should be a greater discrepancy between
the empirical impact of the <code>mfence</code> pair and my estimate from the
profile.</p>

<p>You can find the
<a href="/images/2014-10-19-performance-optimisation-~-writing-an-essay/fence.c">super crufty code here</a>,
<a href="/images/2014-10-19-performance-optimisation-~-writing-an-essay/cycle.h">along with an <code>rdtscp</code> version of cycle.h</a>.</p>

<p>With <code>lock</code>ed instructions and random reads that miss the L3 cache,
the (cycle) profile for the microbenchmark loop is:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ perf annotate -s cache_misses
</span><span class="line">[...]
</span><span class="line">    0.06 :        4006b0:       and    %rdx,%r10
</span><span class="line">    0.00 :        4006b3:       add    $0x1,%r9
</span><span class="line">    ;; random (out of last level cache) read
</span><span class="line">    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
</span><span class="line">   30.37 :        4006bb:       mov    %rcx,%r10
</span><span class="line">    ;; foo is cached, to simulate our internal lock
</span><span class="line">    0.12 :        4006be:       mov    %r9,0x200fbb(%rip)        # 601680 &lt;foo&gt;
</span><span class="line">    0.00 :        4006c5:       shl    $0x17,%r10
</span><span class="line">    [... Skipping arithmetic with &lt; 1% weight in the profile]
</span><span class="line">    ;; locked increment of an in-cache "lock" byte
</span><span class="line">    1.00 :        4006e7:       lock incb 0x200d92(%rip)        # 601480 &lt;private+0x200&gt;
</span><span class="line">   21.57 :        4006ee:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    ;; random out of cache read
</span><span class="line">    0.00 :        400704:       xor    (%rsi,%r10,8),%rbp
</span><span class="line">   21.99 :        400708:       xor    %r9,%r8
</span><span class="line">    [...]
</span><span class="line">    ;; locked in-cache decrement
</span><span class="line">    0.00 :        400729:       lock decb 0x200d50(%rip)        # 601480 &lt;private+0x200&gt;
</span><span class="line">   18.61 :        400730:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    0.92 :        400755:       jne    4006b0 &lt;cache_misses+0x30&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Looking at that profile, I’d estimate that the two random reads
account for ~50% of runtime, and the pair of <code>lock inc/dec</code> for ~40%.</p>

<p>The picture is completely different for <code>mfence</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ perf annotate -s cache_misses
</span><span class="line">[...]
</span><span class="line">    0.00 :        4006b0:       and    %rdx,%r10
</span><span class="line">    0.00 :        4006b3:       add    $0x1,%r9
</span><span class="line">    ;; random read
</span><span class="line">    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
</span><span class="line">   42.04 :        4006bb:       mov    %rcx,%r10
</span><span class="line">    ;; store to cached memory (lock word)
</span><span class="line">    0.00 :        4006be:       mov    %r9,0x200fbb(%rip)        # 601680 &lt;foo&gt;
</span><span class="line">    [...]
</span><span class="line">    0.20 :        4006e7:       mfence 
</span><span class="line">    5.26 :        4006ea:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    ;; random read
</span><span class="line">    0.19 :        400700:       xor    (%rsi,%r10,8),%rbp
</span><span class="line">   43.13 :        400704:       xor    %r9,%r8
</span><span class="line">    [...]
</span><span class="line">    0.00 :        400725:       mfence 
</span><span class="line">    4.96 :        400728:       add    $0x1,%rax
</span><span class="line">    0.92 :        40072c:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    0.36 :        40074d:       jne    4006b0 &lt;cache_misses+0x30&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It looks like the loads from uncached memory represent ~85% of the
runtime, while the <code>mfence</code> pair might account for <em>at most</em> ~15%, if
I include all the noise from surrounding instructions.</p>

<p>If I trusted the profile, I would worry about eliminating <code>lock</code>ed
instructions, but not so much for <code>mfence</code>.  However, runtimes (in
cycles), which is what I’m ultimately interested in, tell a different
story.  The same loop of LLC load misses takes 2.81e9 cycles for 32M
iterations without any atomic or fence, versus 3.66e9 for <code>lock
inc/dec</code> and 19.60e9 cycles for <code>mfence</code>.  So, while the profile for
the <code>mfence</code> loop would let me believe that only ~15% of the time is
spent on synchronisation, the <code>mfence</code> pair really represents 86%
\(((19.6 - 2.81) / 19.6)\) of the runtime for that loop!  Inversely,
the profile for the <code>lock</code>ed pair would make me guess that we spend
about 40% of the time there, but, according to the timings, the real
figure is around 23%.</p>

<p>The other tests all point to the same conclusion: the overhead of
<code>mfence</code> is strongly underestimated by instruction level profiling,
and that of <code>lock</code>ed instructions exaggerated, especially when
adjacent instructions write to memory.</p>

<pre><code>  setup     cycles   (est. overhead)  ~actual overhead

div [ALU] (100 Mi iterations)
 atomic: 20153782848   (20%)          ~ 3.8%
 mfence: 28202315112   (25%)          ~31.3%
vanilla: 19385020088

Reads:

TLB misses (64Mi iterations)
 atomic:  3776164048   (80%)          ~39.3%
 mfence: 12108883816   (50%)          ~81.1%
vanilla:  2293219400 

LLC misses (32Mi iterations)
 atomic:  3661686632   (40%)          ~23.3%
 mfence: 19596840824   (15%)          ~85.7%
vanilla:  2807258536

Writes:

TLB (64Mi iterations)
 atomic:  3864497496   (80%)          ~10.4%
 mfence: 13860666388   (50%)          ~75.0%
vanilla:  3461354848

LLC (32Mi iterations)
 atomic:  4023626584   (60%)          ~16.9%
 mfence: 21425039912   (20%)          ~84.4%
 vanilla: 3345564432
</code></pre>

<p>I can guess why we observe this effect; it’s not like Intel is
intentionally messing with us.  <code>mfence</code> is a full pipeline flush: it
slows code down because it waits for all in-flight instructions to
complete their execution.  Thus, while it’s flushing that slows us
down, the profiling machinery will attribute these cycles to the
instructions that are being flushed.  Locked instructions instead
affect stores that are still queued.  By forcing such stores to
retire, locked instructions become responsible for the extra cycles
and end up “paying” for writes that would have taken up time anyway.</p>

<p>Losing faith in hardware profiling being remotely representative of
reality makes me a sad panda; I now have to double check <code>perf</code>
profiles when hunting for misleading metrics.  At least I can tell
myself that knowing about this phenomenon helps us make better
informed – if less definite – decisions and ferret out more easy
wins.</p>

<p>P.S., if you find this stuff interesting, feel free to send an email
(pkhuong at $WORK.com).  My team is hiring both experienced developers
and recent graduates (:</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-ary heapsort: more comparisons, less memory traffic]]></title>
    <link href="https://www.pvk.ca/Blog/2014/04/13/k-ary-heapsort/"/>
    <updated>2014-04-13T12:25:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2014/04/13/k-ary-heapsort</id>
    <content type="html"><![CDATA[<p><em>This post first appeared on <a href="http://techblog.appnexus.com/2014/k-ary-heapsort-more-comparisons-less-memory-traffic/">AppNexus’s tech blog</a>.</em></p>

<p>The impetus for this post was a max heap routine I had to write
because libc, unlike the STL, does not support
<a href="http://www.sgi.com/tech/stl/make_heap.html">incremental</a> or even
<a href="https://www.sgi.com/tech/stl/partial_sort.html">partial sorting</a>.
After staring at the standard implicit binary heap for a while, I
realised how to
<a href="http://www.pvk.ca/Blog/2014/04/13/number-systems-for-implicit-data-structures/">generalise it to arbitrary arity</a>.  The
routine will be used for medium size elements (a couple dozen bytes)
and with a trivial comparison function; in that situation, it makes
sense to implement a high arity heap and perform fewer swaps in return
for additional comparisons.  In fact, the trade-off is interesting
enough that a heapsort based on this routine is competitive with
<a href="http://fxr.watson.org/fxr/source/stdlib/merge.c?v=FREEBSD-LIBC">BSD</a>
and
<a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/qsort.c;h=04c25b984f74a8f738233cc6da8a738b6437833c;hb=HEAD">glibc</a>
sorts.  This post will present the k-ary heap code and explore the
impact of memory traffic on the performance of a few classical sort
routines.  The worst performing sorts in BSD’s and GNU’s libc overlook
swaps and focus on minimising comparisons.  I argue this is rarely the
correct choice, although our hands are partly tied by POSIX.</p>

<h1 id="a-k-ary-max-heap">A k-ary max heap</h1>

<p>The heart of a heap – and of heapsort – is the sift-down function.
I explored the indexing scheme below in details
<a href="http://www.pvk.ca/Blog/2014/04/13/number-systems-for-implicit-data-structures/">somewhere else</a>, but the gist of it is
that we work with regular 0-indexed arrays and the children of
<code>heap[i]</code> lie at indices <code>heap[way * i + 1]</code> to <code>heap[way * i + way]</code>,
inclusively.  Like sifting for regular binary heaps, the routine
restores the max-heap property under the assumption that only the root
(head) may be smaller than any of its children.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="cp">#define REF(I) ((void *)((char *)base + ((I) * size)))</span>
</span><span class="line">
</span><span class="line"><span class="cp">#define ALIGNED_SIZE 16</span>
</span><span class="line">
</span><span class="line"><span class="k">typedef</span> <span class="nf">int</span> <span class="p">(</span><span class="o">*</span><span class="n">comparator</span><span class="p">)(</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">);</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Similar arguments to qsort. way is the arity of the heap and</span>
</span><span class="line"><span class="cm"> * head the index from which to sift down.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="k">static</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">nheap_sift_inner</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">head</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">                 <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">aligned</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span><span class="line">                <span class="p">(((</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">base</span> <span class="o">-</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">void</span> <span class="o">*</span><span class="n">parent</span><span class="p">,</span> <span class="o">*</span><span class="n">max</span><span class="p">;</span>
</span><span class="line">                <span class="kt">size_t</span> <span class="n">children_offset</span> <span class="o">=</span> <span class="n">head</span> <span class="o">*</span> <span class="n">way</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span><span class="line">                <span class="kt">size_t</span> <span class="n">max_head</span> <span class="o">=</span> <span class="n">head</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">children_offset</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="cm">/* We&#39;re at a leaf. */</span>
</span><span class="line">                        <span class="k">return</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Find the max of parent and all children. */</span>
</span><span class="line">                <span class="n">parent</span> <span class="o">=</span> <span class="n">max</span> <span class="o">=</span> <span class="n">REF</span><span class="p">(</span><span class="n">head</span><span class="p">);</span>
</span><span class="line">                <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">way</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="kt">void</span> <span class="o">*</span><span class="n">child</span><span class="p">;</span>
</span><span class="line">                        <span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="n">children_offset</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                        <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                                <span class="k">break</span><span class="p">;</span>
</span><span class="line">                        <span class="p">}</span>
</span><span class="line">
</span><span class="line">                        <span class="n">child</span> <span class="o">=</span> <span class="n">REF</span><span class="p">(</span><span class="n">j</span><span class="p">);</span>
</span><span class="line">                        <span class="k">if</span> <span class="p">(</span><span class="n">compar</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">max</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                                <span class="n">max</span> <span class="o">=</span> <span class="n">child</span><span class="p">;</span>
</span><span class="line">                                <span class="n">max_head</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
</span><span class="line">                        <span class="p">}</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">max</span> <span class="o">==</span> <span class="n">parent</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="k">return</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Swap the parent down and re-iterate. */</span>
</span><span class="line">                <span class="n">swap</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">max</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">aligned</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">max_head</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The expected use case for this routine is that <code>compar</code> is a trivial
function (e.g., comparing two longs) and <code>size</code> relatively large, up
to fifty or even a few hundred bytes.  That’s why we do some more
work to not only minimise the number of swaps, but also to accelerate
each swap.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="k">typedef</span> <span class="kt">int</span> <span class="n">vector</span> <span class="nf">__attribute__</span><span class="p">((</span><span class="n">vector_size</span><span class="p">(</span><span class="n">ALIGNED_SIZE</span><span class="p">)));</span>
</span><span class="line">
</span><span class="line"><span class="cp">#define CHUNK_SIZE sizeof(long)</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">swap</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">aligned</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">chunk</span><span class="p">[</span><span class="n">CHUNK_SIZE</span><span class="p">];</span>
</span><span class="line">        <span class="kt">size_t</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">aligned</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="n">vector</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span>
</span><span class="line">                        <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span>
</span><span class="line">                        <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="k">return</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">+</span> <span class="n">CHUNK_SIZE</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">CHUNK_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When the data will always be aligned to 16-byte boundaries, we swap
vector registers.  Otherwise, we resort to register-size calls to
<code>memcpy(3)</code> – decent compilers will turn them into unaligned accesses
– and handle any slop with byte-by-byte swaps.</p>

<p>It’s a quick coding job to wrap the above in a linear-time <code>heapify</code>
and a heapsort.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Given a way-ary heap, sift the head&#39;th element down to restore the</span>
</span><span class="line"><span class="cm"> * heap property.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">nheap_sift</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">head</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">           <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">way</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">3</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Turn the nmemb elements of size bytes in base into a compar-max</span>
</span><span class="line"><span class="cm"> * way-ary heap.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span> <span class="nf">nheapify</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">             <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">way</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">nmemb</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* ceiling(nmemb, way) is a leaf. */</span>
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">nmemb</span> <span class="o">/</span> <span class="n">way</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">--&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Like BSD heapsort(3), except that the first argument specifies the</span>
</span><span class="line"><span class="cm"> * arity of the internal max-heap.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">nheapsort</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">          <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">aligned</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span><span class="line">                <span class="p">(((</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">base</span> <span class="o">-</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">r</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">r</span> <span class="o">=</span> <span class="n">nheapify</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">r</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="n">r</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">nmemb</span><span class="p">;</span> <span class="n">i</span> <span class="o">--&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">swap</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">REF</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">size</span><span class="p">,</span> <span class="n">aligned</span><span class="p">);</span>
</span><span class="line">                <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This heap implementation proved more than good enough for our
production use.  In fact, it is so efficient that <code>nheapsort</code> is
competitive with battle-tested sort implementations, particularly when
sorting medium or large structs.  However, these classical
implementations were primarily tuned with comparison counts in mind…
which I argue is now inappropriate.  In the remainder of this post,
I’ll explore the impact of swaps on the performance of sort routines.</p>

<h1 id="meet-the-competition">Meet the competition</h1>

<p>I will compare with the sort routines in two libc: GNU’s glibc and
BSD’s (FreeBSD and OS X, at least, and other *BSD I expect) libc.
All are more than 20 years old.</p>

<p>The <code>qsort(3)</code> routine in glibc is usually a mergesort that allocates
scratch space.  It includes some tests to try and detect out of memory
conditions, but I doubt the logic is very effective on contemporary
hardware and with Linux’s overcommit by default.  BSD manpages specify
that <code>qsort(3)</code> is in-place.  GNU’s don’t… because glibc’s qsort is not
a quicksort.</p>

<p>I can see many arguments for glibc’s design choice.  For one, a
nicely tuned mergesort may be quicker than a safe (guaranteed
\(\mathcal{O}(n \log n)\) time) quicksort.  It also enables msort’s
indirect sorting strategy for large elements: when elements span more
than 32 chars, the sort phase only manipulates pointers and a final
linear-time pass shuffles the actual data in place.</p>

<p>On the other hand, I would be annoyed if qsort triggered the 
<a href="http://linux-mm.org/OOM_Killer">OOM killer</a>.</p>

<p>For this reason, I’ll explicitly consider the real in-place
quicksort that gets called when it’s clear that allocating more memory
is infeasible.  Sadly, I doubt that quicksort received much
attention since it was tuned for a
<a href="http://en.wikipedia.org/wiki/Sun-4">Sun 4/260</a>.  It’s clearly a
product of the 80’s.  It has an explicit stack, the recursion order
guarantees logarithmic stack depth, the pivot is always chosen with a
median of three, and the base case (a partition of 4 or fewer
elements) switches to insertion sort.</p>

<p>The code and the very design of the routine seem to completely
disregard memory traffic.  The swap loop is always char-by-char, and
the base case (insertion sort) isn’t actually called when recursion
stops at a leaf.  Instead, small unsorted partitions are left as is.
Insertion sort is only executed at the end, over the whole array:
unsorted partitions contain at most 4 elements and are otherwise
ordered correctly, so each insertion sort inner loop iterates at most
4 times.  This strategy wastes the locality advantage of
divide-and-conquer sorts: the insertion sort pass is almost streaming,
but it would likely operate on warm data if leaf partitions were
sorted on the fly.</p>

<p>It also includes unsafe code like</p>

<pre><code>char *const end_ptr = &amp;base_ptr[size * (total_elems - 1)];
char *tmp_ptr = base_ptr;
char *thresh = min(end_ptr, base_ptr + max_thresh);
</code></pre>

<p>I can only pray that compilers never becomes smart enough to exploit
<code>base_ptr + max_thresh</code> to determine that this routine will never sort
arrays of fewer than 4 elements.</p>

<p>glibc’s implementation of inserts only aggravates the problem.  After
finding a sorted position for the new element, insertion sort must
insert it there.  Usually one would implement that by copying the new
element (one past the end of the sorted section) to a temporary
buffer, sliding the tail of the sorted array one slot toward the end,
and writing the new element in its expected position.  Unfortunately,
we can’t allocate an arbitrary-size temporary buffer if our quicksort
is to be in-place.  The insertion loop in glibc circumvents the
problem by doing the copy/slide/write dance… one char at a time.
That’s the worst implementation I can think of with respect to memory
access patterns.  It operates on a set of contiguous addresses (the
tail of the sorted array and the new element at its end) with a double
loop of strided accesses that still end up touching each of these addresses.
When the element size is a power of two, it might even hit aliasing
issues and exhaust the associativity of data caches.</p>

<p>The quicksort in *BSD is saner, perhaps because it is still called by
<code>qsort(3)</code>.  While glibc pays lip service to <a href="http://dl.acm.org/citation.cfm?id=172710">Bentley and McIlroy’s “Engineering a Sort Function”,</a> BSD actually uses their implementation.
The swap macro works one <code>long</code> at a time if possible, and the
recursive (with tail recursion replaced by a <code>goto</code>) function switches
to insertion sort as soon as the input comprises fewwer than 7
elements.  Finally, unlike GNU’s insertion sort, BSD’s rotates in
place with a series of swaps.  This approach executes more writes than
glibc’s strided rotations, but only accesses pairs of contiguous addresses.
There’s one tweak that I find worrying: whenever a pivoting step
leaves everything in place, quicksort directly switches to insertion
sort, regardless of the partition size.  This is an attempt to detect
pre-sorted subsequences, but malicious or unlucky input can easily
turn BSD’s quicksort into a quadratic-time insertion sort.</p>

<p>GNU’s quicksort finishes with insertion sort because the latter copes
well with almost sorted data, including the result of its partial
quicksort.  BSD’s qsort is a straight, cache-friendly,
divide-and-conquer implementation.  It looks like it’d make sense to
replace its base case with a selection sort: selection sort will
perform more comparisons than insertion sort (the latter could even
use binary search), but, with a single swap per iteration, will move
much less data around.  I tried it out and the change had little to no
impact, even for large elements.</p>

<p>For completeness’s sake, I’ll also include results with BSD libc’s
heapsort and mergesort.  The heapsort doesn’t look like it’s been
touched in a long while; it too gives compilers licence to kill code,
via</p>

<pre><code>/*
 * Items are numbered from 1 to nmemb, so offset from size bytes
 * below the starting address.
 */
base = (char *)vbase - size;
</code></pre>

<p>Even libcs can’t get <a href="http://blog.regehr.org/archives/767">undefined behaviour</a> straight.</p>

<h1 id="and-the-test-range">And the test range</h1>

<p>Ideally, we could expose the effect of both element size and element
count on the performance of these sort routines.  However, even if I
were to measure runtimes on the cross product of a range of element
size, range of element count, and a set of sort routine, I don’t know
how I would make sense of the results.  Instead, I’ll let element
sizes vary from one word to a few hundred bytes, and bin element
counts in a few representative ranges:</p>

<ol>
  <li>Tiny arrays: 4-7 elements.</li>
  <li>X-Small arrays: 8-15 elements;</li>
  <li>Small arrays: 16-31 elements</li>
  <li>Medium arrays: 32-64 elements.</li>
</ol>

<p>The test program times each sort routine on the same randomly
generated input (sampling with replacement from <code>random(3)</code>), for each
element size.  I try and smooth out outliers by repeating this process
(including regenerating a fresh input) 20 times for each element size and
count.</p>

<p>Regardless of the element size, the comparison function is the same:
it compares a single <code>unsigned</code> field at the head of the member.  I
chose this comparator to reflect real world workloads in which
comparisons are trivial.  When comparisons are slow, element size
becomes irrelevant and classical performance analyses apply.</p>

<p>The idea is to track, within each count bin, relative speedup/slowdown
compared to BSD’s quicksort: any difference in multiplicative factors,
as a function of element size, should show itself.  The element count
bins can instead highlight differences in additive factors.</p>

<h1 id="pretty-pictures">Pretty pictures</h1>

<p>A test program logged runtimes for this cross product of all element
sizes from 8 to 512 bytes (inclusively), all element counts (4 to 64,
inclusively), and various sort routines.  For each size/count pair,
the program regenerated a number of <code>random(3)</code> input; each sort
routine received a copy of the same input.  Given such random input,
we can expect all runtimes to scale with \(\Theta(n\log n)\), and it
makes sense to report cycle counts scaled by a baseline (BSD’s
quicksort).</p>

<p>I compiled each sort routine (with gcc 4.8.2 at <code>-O2</code>) separately from
the benchmark driver.  This prevented fancy interprocedural
optimisation or specialisation from taking place, exactly like we
would expect from calling to libc.  Everything below reports (scaled)
cycle counts on my machine, an E5-4617.  I executed the benchmark with
four processes pinned to different sockets, so the results should
reflect a regular single-threaded setting.  I definitely could have
run the benchmark on more machines, but I doubt that the relative
computational overhead of copying structs versus comparing two machine
words have varied much in the past couple years.</p>

<h2 id="finding-the-perfect-k">Finding the perfect k</h2>

<p>I first tested our k-ary heapsort with many values for <code>k</code>: 2-9, 12,
15-17.</p>

<p>When sorting 8-byte values, <code>k</code> doesn’t have too much of an impact.
Nevertheless, it’s clear that large k (12 and over) are slower than even
binary heapsort.  The sweetspot seems to be around 4 to 7.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_8s.png" /></p>

<p>Larger elements (32 and 64 bytes) show that the usual choice of 
<code>k = 2</code> causes more than a 30% slowdown when swaps are slow.  The
ideal <code>k</code> falls somewhere between 5 and 9 and between 6 and 17.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_32s.png" />
<img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_64s.png" /></p>

<p>Finally, at the extreme, with 512 byte elements, binary heapsort is
almost twice as slow as 7-ary to 17-ary heapsorts.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_512s.png" /></p>

<p>The best choice of <code>k</code> will vary depending on how much slower it is to
swap two elements than to compare them.  A fixed value between 5 and 7
should be close to optimal for elements of 8 to 512 bytes.</p>

<h2 id="tweaks-to-gnu-quicksort">Tweaks to GNU quicksort</h2>

<p>I already mentioned that GNU’s quicksort has a slow swap macro, and
that it ends with an insertion sort pass rather than completely
sorting recursion leaves.  I tested versions of that quicksort with a
different swap function:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>swap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span class="cp">#define CHUNK_SIZE 8</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">SWAP</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">chunk</span><span class="p">[</span><span class="n">CHUNK_SIZE</span><span class="p">];</span>
</span><span class="line">        <span class="kt">size_t</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">+</span> <span class="n">CHUNK_SIZE</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">CHUNK_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I also tested versions that only stops recursing on inputs of size 1
(with <code>#define MAX_THRESH 1</code>).  This is a simple-minded way to
implement quicksort, but avoids the final insertion sort pass (which
proved too hairy to convert to the new swap function).</p>

<p>When elements are small (8 bytes), the trivial recursion base case
(GQL) is a bad idea; it’s slower than the original GNU quicksort (GQ).
The new swap function (GQS), however, is always useful.  In fact, the
combination of trivial base cases and improved swap (GQLS) is pretty
much identical to the original GNU quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_8s.png" /></p>

<p>Switching to larger elements (16, 32, 64 or 512 bytes) shakes things
up a bit.  The impact of faster swaps increases, and GQLS – the
version with faster swap and trivial base case – is markedly quicker
than the other variants.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_16s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_32s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_64s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_512s.png" /></p>

<p>In the sequel, I will only consider the original GNU quicksort (GQ)
and the variant with trivial base case and faster swaps.  From now on,
I’ll refer to that variant as GQ’.</p>

<h2 id="sanity-checking-the-size-bins">Sanity checking the size bins</h2>

<p>I proposed to partition array lengths in 4 bins.  For that to work,
the behaviour of the sort routines must be fairly homogeneous within
each bin.</p>

<p>The graph below fixes the element size at 8 bytes and presents
normalised times for a few sort routines and a range of array sizes.
We report the (geometric) average of 20 runtimes, scaled by the
runtime for BSD quicksort on the same input, on a logarithmic scale.
The logarithmic scale preserves the symmetry between a routine that is
twice as slow as BSD quicksort (log ratio = 1) and one that is twice
as fast (log ratio = -1).  The sort routines are:</p>

<ul>
  <li>GM: GNU mergesort;</li>
  <li>GQ: GNU quicksort;</li>
  <li>GQ’: tweaked GNU quicksort;</li>
  <li>BM: BSD mergesort;</li>
  <li>BH: BSD heapsort;</li>
  <li>NH5: 5-ary heapsort;</li>
  <li>NH7: 7-ary heapsort.</li>
</ul>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_8s.png" /></p>

<p>The graph is pretty noisy for smaller arrays, but there’s no obvious
discrepancy within size bins.  The same holds for larger element sizes
(16, 64 and 512 bytes).</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_16s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_64s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_512s.png" /></p>

<p>We can already see that BSD’s quicksort is hard to beat on the element
sizes considered above.</p>

<h2 id="impact-of-element-size-on-tiny-arrays">Impact of element size on tiny arrays</h2>

<p>The next graph shows the evolution of average (log) normalised sort
times for tiny arrays (4-7 elements), as the size of each element
grows from 8 to 512 bytes.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-512s.png" /></p>

<p>There’s some overplotting, but it’s clear that there are actually two
sets of curves: the main one, and another that only comprises sizes
that are multiples of 8 (word-aligned).  I believe this is caused by
special optimisations for nice element sizes in some (but not all) sort
routines, including our baseline, BSD’s quicksort.</p>

<p>For word-unaligned sizes, GNU’s mergesort and our tweaked GNU
quicksort are never slower than the baseline; for large items, they
are around 4x as quick.  On the same input, BSD’s mergesort and our
k-ary heapsorts are on par with one another and slightly slower than
the baseline, while GNU’s quicksort is slightly quicker.  The only
sort routine that’s markedly slower than BSD’s quicksort is BSD’s
binary heapsort (around 2-3x as slow).</p>

<p>BSD’s word-alignment optimisation makes sense in practice: larger
structs will often include at least one field that requires
non-trivial alignment.  I think it makes sense to look at arbitrary
sizes only up to a small limit (e.g., 32 bytes), and then only
consider word-aligned sizes.</p>

<p>For small element sizes, all but two routines are comparable to the
baseline.  BSD’s heapsort is markedly slower, and our tweaked GNU
quicksort somewhat faster.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-32s.png" /></p>

<p>When element sizes are word-aligned, BSD’s quicksort is hard to beat.
The only routine that manages to be quicker is GNU’s mergesort, for
very large element sizes: the routine exploits dynamic storage to sort
pointers and only permute elements in-place during a final linear-time
pass.  Our k-ary heapsorts remain in the middle of the pack, slightly
slower than BSD’s mergesort and GNU’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-128s.png" />
<img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_128-512s.png" /></p>

<h2 id="impact-of-element-size-on-extra-small-arrays">Impact of element size on extra small arrays</h2>

<p>The results in the previous section were noisy: that’s expected when
sorting arrays of 4 to 7 elements.  This section looks at arrays of 8
to 15 elements; there’s less random variations and the graphs are much
clearer.</p>

<p>The graph for overall speeds is similar to the one for tiny arrays.
However, it’s clear that, for general element sizes, our k-ary
heapsorts are slightly faster than BSD’s mergesort and a bit slower
than GNU’s quicksort.  Again, GNU’s mergesort and our tweaked
quicksort are even quicker than BSD’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_4-512s.png" /></p>

<p>When we only consider word-aligned element sizes, the picture changes.
The most important change is that our baseline is a lot quicker, and
even GNU’s mergesort is slightly slower than BSD’s quicksort.  Our
heapsorts are now slower than BSD’s mergesort, which is now comparable
to GNU’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_4-128s.png" /></p>

<p>It’s only when we consider almost ridiculous element sizes that GNU’s
mergesort edges out BSD’s quicksort: GNU’s indirect sorting strategy
finally pays off.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_128-512s.png" /></p>

<h2 id="finally-small-and-medium-arrays">Finally, small and medium arrays</h2>

<p>Small and medium arrays lead to the same conclusion as small ones,
only more clearly.  For arbitrary element sizes, GNU’s mergesort and
tweaked quicksorts are much quicker than everything else.  As for
k-ary heapsorts, they are comparable to BSD’s quicksort and quicker
than BSD’s mergesort and heapsort, except for small elements (fewer
than a dozen bytes each).</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_4-512s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_4-512s.png" /></p>

<p>For word-aligned element sizes, GNU’s quicksort is still only
outperformed by GNU’s indirect mergesort, while k-ary heapsorts are
still slightly slower than GNU’s quicksort and BSD’s mergesort, but
always quicker than the latter’s heapsort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_4-128s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_128-512s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_4-128s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_128-512s.png" /></p>

<h1 id="wrap-up">Wrap-up</h1>

<p>I’d say our results show that heapsort gets a bad rap because we
always implement binary heaps.  Higher arity (around 7-way) heaps are
just as easy to implement, and much more efficient when comparators
are trivial and data movement slow.  For small arrays of medium-size
elements, k-way heapsorts are competitive with BSD’s mergesort and
GNU’s quicksort, while guaranteeing \(\mathcal{O}(n \log n)\) worst-case performance
and without allocating storage; they’re almost magical when we only
need partial or incremental sorts.</p>

<p>There’s a more general conclusion from these experiments: <code>qsort(3)</code>’s
very interface is a bad fit for a lot of workloads.  <code>qsort</code> lets us
implement arbitrary comparisons with a callback, but must rely on
generic swap routines.  This probably reflects a mindset according to
which comparisons (logic) are slow and swaps (accessing data)
trivially quick; GNU’s quicksort was definitely written with these
assumptions in mind.  The opposite is often the case, nowadays.  Most
comparators I’ve written are lexicographic comparisons of machine
words, floating point values, and, very rarely, strings.  Such
comparators could be handled with a few specialised sort routines:
lexicographic sorts can either exploit stability (à la bottom-up radix
sort), or recursion (top-down radix sort).  However, there were some
cases when I could have implemented swaps more cleverly than with
<code>memcpy(3)</code>, and I could always have passed a size-specialised swap
routine.  There were also cases when I wanted to sort multiple arrays
with respect to the same key, and had to allocate temporary storage,
transpose the data, sort, and transpose back.  Mostly, this happened
because of hybrid
<a href="https://software.intel.com/en-us/articles/how-to-manipulate-data-structure-to-optimize-memory-use-on-32-bit-intel-architecture">struct of arrays</a>
data layout imposed by memory performance considerations, but
implementing the
<a href="http://c2.com/cgi/wiki?SchwartzianTransform">Schwartzian transform</a>
in C (which avoids recomputing keys) is basically the same thing.  A
swap function argument would be ideal: it’s quicker than generic
swaps, supports memory-optimised layouts, and helps express a classic
Perl(!) idiom for sorting on derived keys.</p>

<p>Behold, the modern sort interface:</p>

<pre><code>struct comparison {
        void *base;
        size_t stride; 
        union { void *ptr; unsigned long mask; } auxiliary;
        enum comparison_kind kind;
};

void
generic_sort(const struct comparison *cmp, size_t ncmp, size_t length,
        void (*swap)(void *, size_t i, size_t j), void *swap_data);
</code></pre>

<p>Interpreted DSLs for logic and compiled data shuffling; it’s the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How bad can 1GB pages be?]]></title>
    <link href="https://www.pvk.ca/Blog/2014/02/18/how-bad-can-1gb-pages-be/"/>
    <updated>2014-02-18T00:06:00-05:00</updated>
    <id>https://www.pvk.ca/Blog/2014/02/18/how-bad-can-1gb-pages-be</id>
    <content type="html"><![CDATA[<p>I joined the ad server team at <a href="http://www.appnexus.com/">AppNexus</a>
two weeks ago.  It’s a new problem domain for me, one that I find
pretty interesting so far: the workload is definitely on the
branchy/pointer-chasing side of things, and, although aggregate
throughput matters, we work under stringent latency goals.  There are
a few hash table lookups on the code path I’ve been working on, and
some micro-benchmarking revealed that 1GB pages could easily shave 25%
off the latency for lookups in large (hundreds of MB) tables.</p>

<p>That’s not too surprising.  Given any change, it’s usually easy to
design a situation that makes it shine.  I find it just as important to
ask the opposite question: what’s the worst slowdown I can get from
switching from regular 4KB pages to huge 1GB pages?  This way, I can
make more robust decisions, by taking into account both a good (if not
best) and a worst case. By the end of this post, I’ll multiply runtime
by 250% for the same operation, simply by switching from 4KB to 1GB
pages… with a setup is so contrived that it seems unlikely to occur by
accident.</p>

<p>But first, a quick overview of the benefits and downsides of huge
pages.</p>

<h1 id="why-are-huge-pages-interesting">Why are huge pages interesting?</h1>

<p>In short, manufacturers are adding huge pages because translating
virtual addresses to physical ones is slow.</p>

<p>On x86-64, the mapping from virtual to physical pages is represented
with a trie; each level dispatches on 9 bits (i.e., each node has 512
children), and leaves correspond to 4KB pages. There are 4 levels from
the root node to the leaves, which covers the (currently) standard 48-bit
virtual address space.</p>

<p>The address translation table is (mostly) stored in normal memory and
is too large to fit in cache. Thus, translating a virtual address
requires four reads, any of which can hit uncached memory.</p>

<p>This is where the translation lookaside buffer (TLB) comes in. On my
E5-4617, each core has 64 entries for regular 4KB pages in its L1
dTLB, and 512 (shared) entries in its L2 TLB. I don’t know if the TLBs
are exclusive or inclusive, but even if they’re exclusive, that’s only
enough for 2.25MB worth of data. Assuming that the working set is
completely contiguous (i.e., the best case), the TLB space for 4KB
pages is less than the total cache/core (2.5MB L3/core + 256 KB L2 +
32 KB L1D).</p>

<p>2MB and 1GB “huge pages” address this imbalance: nine 2MB pages
suffice to cover more address space than all the caches in a 6-core
E5-4617. However, there are only 32 L1dTLB entries for 2MB pages –
and 4 entries for 1GB pages – on my E5.</p>

<p>In addition to covering more address space in the TLB, huge pages
offer secondary benefits: there are fewer page table entries, and the
trie is shallower. Fewer page table entries means that a larger
fraction of memory can be used by data, rather than metadata, and that
the page table walk is more likely to stay in cache. Moreover, larger
pages are closer to the trie’s root: while the processor traverses 4
levels to get to a 4KB page, it only traverses 3 levels to reach a 2MB
page and 2 levels to for a 1GB page. These two effects compound to
make TLB misses quicker to handle.</p>

<h1 id="now-the-downsides">Now, the downsides…</h1>

<p>This idea that one must cover as much address space as possible with
the TLB is most relevant in two settings: trivially, if the working
set is completely covered by the (L1d)TLB, or, more interestingly,
when the access patterns show a lot of locality.  Examples of the
latter case are BLAS routines: with appropriate blocking, they can
usually access each page once or twice, but read almost every byte in
a page before switching to the next.</p>

<p>The opposite, worst, case would be something like lookups in a large
(too big for the TLB) hash table: we choose a virtual address
pseudorandomly and painstakingly translate it to a physical address,
only to read a handful of words from that page. In that situation, we
want as many TLB entries as possible, regardless of the address space
each one covers… and that’s where 4KB pages ought to shine. Taking
into account both the L1DTLB and the L2TLB, each core has 576 TLB
entries for 4KB (data) pages, versus 64x2MB and 4x1GB. Now, I don’t
know if the TLBs are exclusive or not, so I’ll assume the worst case
and work with 512*4KB entries.</p>

<p>The thing is, 512 TLB entries aren’t that many. If, by chance, our
hash table lookups keep hitting the same 512 pages, a contrived
microbenchmark will show that 4KB pages are a big win (but really, a
software cache might be a better way to exploit the situation). It’s
more likely that it’ll be hard to avoid TLB misses regardless of page
size, and huge pages then become useful because each TLB miss is
handled more quickly. Regardless, I’ll try to approximate this
worst-case behaviour to see how bad things can get.</p>

<h1 id="a-first-stab-at-pessimising-huge-pages">A first stab at pessimising huge pages</h1>

<p>Ideally, I would want to read from 512 (or a bit fewer) locations 1GB
apart, but I don’t have that much RAM. In the interest of realism, I
decided to “only” allocate 24GB.</p>

<p>My first microbenchmark follows: I allocate 24GB, divide that space in
512 chunks, and read the first word of each chunk in a loop. At first,
I didn’t even randomise the traversal order (so as to abuse LRU), but
there seems to be some prefetching for 1GB pages.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define _GNU_SOURCE
</span><span class="line">#include &lt;assert.h&gt;
</span><span class="line">#include &lt;stddef.h&gt;
</span><span class="line">#include &lt;stdlib.h&gt;
</span><span class="line">#include &lt;stdio.h&gt;
</span><span class="line">#include &lt;string.h&gt;
</span><span class="line">#include &lt;sys/mman.h&gt;
</span><span class="line">
</span><span class="line">#include "cycle.h"
</span><span class="line">
</span><span class="line">#ifndef MAP_HUGETLB
</span><span class="line"># define MAP_HUGETLB 0x40000
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">#ifndef MAP_HUGE_1GB
</span><span class="line"># define MAP_HUGE_1GB (30 &lt;&lt; 26)
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">#if defined(ONE_G)
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB | MAP_HUGE_1GB
</span><span class="line">#elif defined(TWO_M)
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB
</span><span class="line">#else
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/512;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;20, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;20; i++) {
</span><span class="line">                size_t x = 512.0*random()/RAND_MAX;
</span><span class="line">                indices[i] = x*stride;
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 7; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;20; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The results: 1.13e10 cycles for 4KB pages, 1.60e10 for 2MB and 1.56e10
for 1GB. That’s only 40% more cycles… it’s bad, but not horrible. The
reason is that the data vector spans only 24x1GB, so 1/6th of the
random lookups will hit the 1GB TLB. Instead, let’s try and load from
each of these 24 pages, in random order. 24 pages will easily fit in
the L1DTLB for 4KB pages, but not in the 4 slots for 1GB pages.</p>

<h1 id="takes-two-to-six">Takes two to six</h1>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define NCHUNKS 24
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;20, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;20; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                indices[i] = (x*stride) % (24ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 7; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;20; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The results are even worse (better)! 4.82e9 cycles for 4KB pages,
versus 3.96e9 and 2.84e9 for 2MB and 1GB pages!</p>

<p>The problem is aliasing. The TLB on my E5 has limited way-ness (4-way,
I believe), so, by aligning everything to a 1GB boundary, the
effective size of the 4KB page TLB is 4 entries (same for 2MB). In a
way, this highlights the effect of page size when TLBs are useless
(random accesses to dozens or hundreds of GBs): 2MB pages shave 18%
off the runtime, and 1GB pages another 30%, for a total of 60% as much
time to handle a 1GB TLB miss versus 4KB.</p>

<p>Let’s try again, with <code>indices[i] = (x*stride + (x*4096)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);</code> on line 19.  I now find 1.14e9, 6.18e9 and 2.65e9 cycles. Much better!</p>

<p>For fun, I also tried to offset by 2MB increments, with <code>indices[i] =
(x*stride + (x&lt;&lt;21)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);</code>, and found 2.76e9,
1.30e9, and 2.85e9 cycles.</p>

<p>Finally, I tried</p>

<pre><code>            size_t offset = 4096 + (1ul&lt;&lt;21);
            indices[i] = (x*stride + (x*offset)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);
</code></pre>

<p>so that neither 4KB nor 2MB pages would alias, and got 1.13e9, 1.08e9
and 2.65e9 cycles. That’s 234% as much time for 1GB pages as for 4KB.</p>

<p>We’re close: this setup is such that 1GB pages cause a lot of TLB
misses, but neither 4KB nor 2MB pages do.  However, <code>perf stat</code> shows
there’s a lot of cache misses, and that probably reduces the
difference between 4KB and 1GB pages.</p>

<p>Let’s try one last thing, with <code>size_t offset = 4096 + (1ul&lt;&lt;21) +
64;</code> (to avoid aliasing at the data cache level), and a smaller index
vector that fits in cache.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class=""><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;10, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;10; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                size_t offset = 4096 + (1ul&lt;&lt;21) + 64;
</span><span class="line">                indices[i] = (x*stride + ((x*offset)%(1ul&lt;&lt;30))) % (24ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 17; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;10; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We get 1.06e9, 9.94e8, and 2.62e9 cycles, i.e., 250% as much time with
1GB pages than 4KB ones.</p>

<p>We can easily turn this around: we just have to loop over more than 4
4KB-aligned locations in a 4GB space. For example, with</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define NCHUNKS 4096
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (4ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 4ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 4ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;10, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;10; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                size_t offset = 64;
</span><span class="line">                indices[i] = (x*stride + ((x*offset)%(1ul&lt;&lt;30))) % (4ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 17; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;10; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With the above, I find 7.55e9 cycles for 4KB pages, 3.35e9 for 2MB and
1.09e9 for 1GB pages. Here, 4KB pages are almost 7x as slow as 1GB
pages.  If I instead let <code>size_t offset = 4096 + 64;</code> (to avoid
aliasing in the 4KB TLB), I get 4.72e9 cycles for 4KB pages, so still
433% as much time.</p>

<p>We can also play the same trick over 32*2MB = 64MB.  On my E5, I find
3.23e9 cycles for 4KB pages, versus 1.09e9 for 2MB and 1GB pages.
Eliminating page-level aliasing only brings the 4KB case down to
3.02e9 cycles, and doesn’t affect the other two cases.</p>

<h1 id="so-are-1gb-pages-generally-useful">So, are 1GB pages generally useful?</h1>

<p>The following table summarises the runtimes of all the variations above with
2MB and 1GB pages (as a fraction of the number of cycles for 4KB
pages).</p>

<center>
<table style="border-collapse: collapse;">
<col style="border:1px solid #000000;" />
<col style="border:1px solid #000000;" />
<tr><td>2MB/4KB&nbsp;</td> <td>&nbsp;1GB/4KB</td></tr>
<tr><td>1.42</td> <td>1.38</td></tr>
<tr><td>0.82</td> <td>0.59</td></tr>
<tr><td>5.42</td> <td>2.32</td></tr>
<tr><td>0.47</td> <td>1.03</td></tr>
<tr><td>0.96</td> <td>2.34</td></tr>
<tr><td>0.94</td> <td>2.47</td></tr>
<tr><td>0.44</td> <td>0.14</td></tr>
<tr><td>0.72</td> <td>0.23</td></tr>
<tr><td>0.34</td> <td>0.34</td></tr>
<tr><td>0.36</td> <td>0.36</td></tr>
</table>
</center>

<p>Overall, I think that I wouldn’t automatically switch to 2MB pages,
but that 1GB pages are a solid choice for machines that basically run
a single process at a time. When the data fits in 4GB, 1GB pages
completely eliminate TLB misses. When the data is even larger, 2MB and
1GB pages make page table walks quicker (by 18% and 40%,
respectively). It takes a very contrived situation – in which a
program keeps hitting fewer than 512 4KB-pages that are spread out
across multiple GBs – for smaller pages to be preferable.  The worst
I managed was 250% as much time for 1GB pages vs 4KB; in the other
direction, I achieved 693% as much time for 4KB pages versus 1GB, and
433% with a realistic situation (e.g., repeated lookups in a 4GB hash
table). Plus, there’s another interesting benefits from larger pages
that did not show up in this post: we get more control over aliasing
in data caches.</p>

<p>With multiple processes in play, there are fragmentation issues, and
things aren’t as clear-cut… especially given that 1GB pages must
currently be allocated at boot-time, on Linux.</p>

<p>I’m also still unsure how 1GB pages interact with NUMA. I’m
particularly worried about interleaving: interleaving at a 1GB
granularity seems unlikely to smooth out the ratio of local:remote
accesses as much as doing it at a 4KB granularity.</p>
]]></content>
  </entry>
  
</feed>
