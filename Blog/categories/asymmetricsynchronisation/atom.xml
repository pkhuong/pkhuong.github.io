<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: AsymmetricSynchronisation | Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/Blog/categories/asymmetricsynchronisation/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2021-06-07T00:37:06-04:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Flatter wait-free hazard pointers]]></title>
    <link href="https://www.pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers/"/>
    <updated>2020-07-07T14:30:26-04:00</updated>
    <id>https://www.pvk.ca/Blog/2020/07/07/flatter-wait-free-hazard-pointers</id>
    <content type="html"><![CDATA[<p><small>2020-07-09: we can fix <a href="#hp_read_swf-relaxed">time-travel in <code>hp_read_swf</code></a> without changing the fast path. See <a href="#addendum-2020-07-09">the addendum</a>.</small></p>

<p>Back in February 2020, Blelloch and Wei submitted this cool preprint: <a href="https://arxiv.org/abs/2002.07053">Concurrent Reference Counting and Resource Management in Wait-free Constant Time</a>.
Their work mostly caught my attention because they propose a wait-free implementation of hazard pointers for safe memory reclamation.<sup id="fnref:but-also" role="doc-noteref"><a href="#fn:but-also" class="footnote" rel="footnote">1</a></sup>
<a href="http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf">Safe memory reclamation (PDF)</a> is a key component in lock-free algorithms when garbage collection isn’t an option,<sup id="fnref:it-is-gc" role="doc-noteref"><a href="#fn:it-is-gc" class="footnote" rel="footnote">2</a></sup>
and <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">hazard pointers (PDF)</a> let us bound the amount of resources stranded by delayed cleanups much more tightly than, e.g., <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation (PDF)</a>.
However the usual implementation has a <em>loop</em> in its <a href="https://www.iecc.com/gclist/GC-algorithms.html">read barriers (in the garbage collection sense)</a>,
which can be annoying for code generation and bad for worst-case time bounds.</p>

<p>Blelloch and Wei’s wait-free algorithm eliminates that loop… with a construction that stacks <a href="https://arxiv.org/abs/1911.09671">two emulated primitives—strong LL/SC, and atomic copy, implemented with the former—</a>on top of what real hardware offers.
I see the real value of the construction in proving that wait-freedom is achievable,<sup id="fnref:not-obvious" role="doc-noteref"><a href="#fn:not-obvious" class="footnote" rel="footnote">3</a></sup>
and that the key is atomic memory-memory copies.</p>

<p>In this post, I’ll show how to flatten down that abstraction tower into something practical with a bit of engineering elbow grease,
and come up with wait-free alternatives to the usual lock-free hazard pointers
that are competitive in the best case.
Blelloch and Wei’s insight that hazard pointers can use any wait-free atomic memory-memory copy lets us improve the worst case
without impacting the common case!</p>

<p>But first, what are hazard pointers?</p>

<h2 id="hazard-pointers-and-the-safe-memory-reclamation-problem">Hazard pointers and the safe memory reclamation problem</h2>

<p>Hazard pointers were introduced by <a href="https://dblp.uni-trier.de/pers/m/Michael:Maged_M=.html">Maged Michael</a>
in <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">Hazard Pointers: Safe Memory Reclamation for Lock-Free Objects (2005, PDF)</a>,
as the first solution to reclamation races in lock-free code.
The introduction includes a concise explanation of the safe memory reclamation (SMR) problem.</p>

<blockquote>
  <p>When a thread removes a node, it is possible that some other contending thread—in the course of its lock-free operation—has earlier read a reference to that node, and is about to access its contents. If the removing thread were to reclaim the removed node for arbitrary reuse, the contending thread might corrupt the object or some other object that happens to occupy the space of the freed node, return the wrong result, or suffer an access error by dereferencing an invalid pointer value. […] Simply put, the memory reclamation problem is how to allow the memory of removed nodes to be freed (i.e., reused arbitrarily or returned to the OS), while guaranteeing that no thread accesses free memory, and how to do so in a lock-free manner.</p>
</blockquote>

<p>In other words, a solution to the SMR problem lets us know when it’s safe to
<em>physically release</em> resources that used to be owned by a linked data structure,
once all links to these resources have been removed from that data structure (<a href="http://concurrencykit.org/presentations/ebr.pdf#page=8">after “logical deletion”</a>).
The problem makes intuitive sense for dynamically managed memory,
but it applies equally well to any resource (e.g., file descriptors),
and its solutions can even be seen as extremely read-optimised reader/writer locks.<sup id="fnref:RCU" role="doc-noteref"><a href="#fn:RCU" class="footnote" rel="footnote">4</a></sup></p>

<p>The basic idea behind Hazard Pointers is to have
each thread publish to permanently allocated<sup id="fnref:stable-alloc" role="doc-noteref"><a href="#fn:stable-alloc" class="footnote" rel="footnote">5</a></sup> hazard pointer records (HP records) the set of resources (pointers) it’s temporarily borrowing from a lock-free data structure.
That’s enough information for a background thread to snapshot the current list of resources that have been logically deleted but not yet physically released (the limbo list),
scan all records for all threads,
and physically release all resources in the snapshot that aren’t in any HP record.</p>

<p>With just enough batching of the limbo list, this scheme can be practical:
in practice, lock-free algorithms only need to pin a few (usually one or two) nodes at a time to ensure memory safety.  As long as we avoid running arbitrary code while holding hazardous references, we can bound the number of records each thread may need at any one time.
Scanning the records thus takes time roughly linear in the number of active threads, and we can amortise that to constant time per deleted item by waiting until the size of the limbo list is greater than a multiple of the number of active threads.<sup id="fnref:even-with-pinned-nodes" role="doc-noteref"><a href="#fn:even-with-pinned-nodes" class="footnote" rel="footnote">6</a></sup></p>

<p>The tricky bit is figuring out how to reliably publish to a HP record without locking.
Hazard pointers simplify that challenge with three observations:</p>
<ol>
  <li>It’s OK to have arbitrary garbage in a record (let’s disregard language-level<sup id="fnref:or-hw-level" role="doc-noteref"><a href="#fn:or-hw-level" class="footnote" rel="footnote">7</a></sup> undefined behaviour), since protected values are only ever subtracted from the limbo list: a garbage record simply doesn’t protect anything.</li>
  <li>It’s also OK to leave a false positive in a record: correctness arguments for hazard pointers already assume each record keeps a different node (resource) alive, and that’s the worst case.</li>
  <li>1 and 2 mean it doesn’t matter what pinned value we read in a record whose last update was started after we snapshotted the limbo list: resources in the limbo list are unreachable, so freshly pinned resources can’t refer to anything in the snapshot.</li>
</ol>

<p>This is where the clever bit of hazard pointers comes in:
we must make sure that any resource (pointer to a node, etc.) we borrow from a lock-free data structure is immediately protected by a HP record.
We can’t make two things happen atomically without locking.
Instead, we’ll <em>guess</em><sup id="fnref:guessing-is-fine" role="doc-noteref"><a href="#fn:guessing-is-fine" class="footnote" rel="footnote">8</a></sup> what resource we will borrow,
publish that guess,
and then actually borrow the resource.
If we guessed correctly, we can immediately use the borrowed resource;
if we were wrong, we must try again.</p>

<p>On an ideal <a href="https://en.wikipedia.org/wiki/Sequential_consistency">sequentially consistent</a> machine,
the pseudocode looks like the following.  The <code>cell</code> argument points to the resource we wish to acquire
(e.g., it’s a reference to the <code>next</code> field in a linked list node), and <code>record</code> is the hazard pointer
record that will protect the value borrowed from <code>cell</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_sc.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_sc</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="s2">&quot;Reads the resource (pointer) in `cell`, and protects it through `record`.&quot;</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span> <span class="o">=</span> <span class="n">guess</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load</span><span class="p">():</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>In practice, we must make sure that our write to <code>record.pin</code> is visible before re-reading the <code>cell</code>’s value, and we should also make sure the pointer read is ordered with respect to the rest of the calling read-side code.<sup id="fnref:beware-accidental-success" role="doc-noteref"><a href="#fn:beware-accidental-success" class="footnote" rel="footnote">9</a></sup></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_explicit.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_explicit</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">        <span class="n">fence_store_load</span><span class="p">()</span>  <span class="c1"># R1</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">():</span> <span class="c1"># R2</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We need a store/load fence in <code>R1</code> to make sure the store to the record (just above <code>R1</code>) is visible by the time the second read (<code>R2</code>) executes.  Under the <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/x86tso-paper.tphols.pdf">TSO memory model implemented by x86 chips (PDF)</a>,
this fence is the only one that isn’t implicitly satisfied by the hardware.
It also happens that fences are best implemented with atomic operations
on x86oids,
so we can eliminate the fence in <code>R1</code>
by replacing the store just before <code>R1</code> with an atomic exchange (fetch-and-set).</p>

<p>The slow cleanup path has its own fence that matches <code>R1</code> (the one in <code>R2</code>
matches the mutators’ writes to <code>cell</code>).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_explicit.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_explicit</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We must make sure all the values in the limbo list we grab in <code>C1</code>
were added to the list (and thus logically deleted) before we read any
of the records in <code>C2</code>, with the  acquire read in <code>C1</code>
matching the store-load fence in <code>R1</code>.</p>

<p>It’s important to note that the cleanup loop does not implement
anything like an atomic snapshot of all the records.  The reclamation
logic is correct as long as we scan the correct value for records that
have had the same pinned value since before <code>C1</code>: we assume that a
resource only enters the limbo list once <em>all</em> its persistent
references have been cleared (in particular, this means circular
backreferences must be broken before scheduling a node for
destruction), so any newly pinned value cannot refer to any resource
awaiting destruction in the limbo list.<sup id="fnref:DEBRA-plus" role="doc-noteref"><a href="#fn:DEBRA-plus" class="footnote" rel="footnote">10</a></sup></p>

<p>The following sequence diagrams shows how the fencing guarantees that any
iteration of <code>hp_read_explicit</code> will fail if it starts before <code>C1</code>
and observes a stale value.
If the read succeeds, the ordering between <code>R1</code> and <code>C1</code> instead
guarantees that the cleanup loop will observed the pinned value
when it reads the record in <code>C2</code>.</p>

<p><a href="https://sequencediagram.org/index.html#initialData=C4S2BsFMAIDFIHYGNIBNoAsCGAvLAndABwHsQFhJ8Aoaog0JEei6AJUi1SutS2CwAjLAGcYAWUgBbEvgCedBiCYtg0AOr4wVAPQBhKFgQBXIrU3b8+wyaIBaAHySZ8gFzQAqkT6VoKcODQAG5Y4MaQAPzUHFxUADx2zrJy7gDi4SIifpABwaHh0Zzc+I5JbtAACsaC4CAiGNBE5Aho0ADmGVnAJND4kEiyqNQWlFYGnLal0snuXj4w-oEhYZHUZXKOI7rjRqZp+ELQtVKCPbUiwNQAvFfwyDBsAIzQOtB6jzeFsfgJ6+4A8oIxPgggscoFTPMADS9SDAeTQEgIaCoEAAMzRa2m8k2WlG1gme2ggOBoOIzVaywKQA">
    <img src="/images/2020-07-07-flatter-wait-free-hazard-pointers/fenced-hp.png" />
</a></p>

<p>This all works, but it’s slow:<sup id="fnref:travis-says-its-fine" role="doc-noteref"><a href="#fn:travis-says-its-fine" class="footnote" rel="footnote">11</a></sup> we added an <em>atomic</em> write instruction (or worse, a fence) to a read-only operation.  We can do better with a little help from our operating system.</p>

<h2 id="injecting-fences-with-os-support">Injecting fences with OS support</h2>

<p>When we use fences or memory ordering correctly, there should be
an implicit pairing between fences or ordered operations: we use fencing to
enforce an ordering (one must fully execute before or after another,
overlap is forbidden) between pairs of instructions in different
threads.  For example, the pseudocode for hazard pointers with
explicit fencing and memory ordering paired the store-load fence in
<code>R1</code> with the acquisition of the limbo list in <code>C1</code>.</p>

<p>We only need that pairing very rarely, when a thread actually executes the
cleanup function.  The amortisation strategy guarantees we don’t scan records all the time, and we can always increase the amortisation factor if we’re
generating tiny amounts of garbage very quickly.</p>

<p>It kind of sucks that we have to incur a full fence on the fast read
path, when it only matches reads in the cleanup loop maybe as rarely
as, e.g., once a second.  If we waited long enough on the slow path, we could
<a href="https://pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering">rely on events like preemption or other interrupts to insert a barrier</a>
in all threads that are executing the read-side.</p>

<p>How long is “enough?”
Linux has the <a href="https://man7.org/linux/man-pages/man2/membarrier.2.html"><code>membarrier</code> syscall</a>
to block the calling thread until (more than) long enough has elapsed,
Windows has <a href="https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-flushprocesswritebuffers">the similar <code>FlushProcessWriteBuffers</code></a>, and
on other operating systems, we can probably <a href="https://github.com/pkhuong/barrierd">do something useful with scheduler statistics</a> or ask for a new syscall.</p>

<p>Armed with these new blocking system calls, we can replace the store-load fence in <code>R1</code> with a compiler barrier, and execute a slow <code>membarrier</code>/<code>FlushProcessWriteBuffers</code> after <code>C1</code>.
The cleanup function will then wait long enough<sup id="fnref:arguably-lock-ful" role="doc-noteref"><a href="#fn:arguably-lock-ful" class="footnote" rel="footnote">12</a></sup> to ensure that any
read-side operation that had executed before <code>R1</code> at the time we read the limbo list in <code>C1</code> will be visible (e.g., because the operating system knows a preemption interrupt executed at least once on each core).</p>

<p>The pseudocode for this asymmetric strategy follows.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_membarrier.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_membarrier</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">        <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># R1</span>
</span><span class="line">        <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">():</span> <span class="c1"># R2</span>
</span><span class="line">            <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_membarrier.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_membarrier</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We’ve replaced a fence on the fast read path with a compiler barrier, at the expense of executing a heavy syscall on the slow path.  That’s usually an advantageous trade-off, and is the <a href="https://github.com/facebook/folly/blob/master/folly/synchronization/Hazptr.h">preferred implementation strategy for Folly’s hazard pointers</a>.</p>

<p>The ability to pair mere <em>compiler</em> barriers with <code>membarrier</code>
syscalls opens the door for many more “atomic enough” operations, not
just the fenced stores and loads we used until now:
similarly to the key idea in <a href="https://github.com/concurrencykit/ck/blob/master/include/ck_ec.h">Concurrency Kit’s atomic-free SPMC event count</a>,
we can use non-interlocked read-modify-write instructions,
since any interrupt (please don’t mention imprecise interrupts) will happen before or after any such instruction,
and never in the middle of an instruction.</p>

<p>Let’s use that to simplify wait-free hazard pointers.</p>

<h2 id="wait-free-hazard-pointers-with-interrupt-atomic-instructions">Wait-free hazard pointers with interrupt-atomic instructions</h2>

<p>The key insight that lets <a href="https://arxiv.org/abs/2002.07053">Blelloch and Wei</a>
achieve wait-freedom in hazard pointer is that the combination
of publishing a guess and confirming that the guess is correct in <code>hp_read</code>
emulates an atomic memory-memory copy.  Given such an atomic copy primitive, the read-side becomes trivial.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_blelloch_wei.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_membarrier</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">atomic_copy</span><span class="p">(</span><span class="n">cell</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The “only” problem is that atomic copies (which would look like locking all other cores out of memory accesses, copying the <code>cell</code>’s word-sized contents to <code>record.pin</code>, and releasing the lock) don’t exist in contemporary hardware.</p>

<p>However, we’ve already noted that syscalls like <code>membarrier</code> mean we can weaken our requirements to interrupt atomicity. In other words, individual non-atomic instructions work since we’re assuming precise interrupts… and <code>x86</code> and <code>amd64</code> do have an instruction for memory-memory copies!</p>

<p>The <a href="https://www.felixcloutier.com/x86/movs:movsb:movsw:movsd:movsq"><code>MOVS</code> instructions</a> are typically only used with a <code>REP</code> prefix.  However, they can also be executed without any prefix, to execute one iteration of the copy loop.  Executing a <code>REP</code>-free <code>MOVSQ</code> instruction copies one quadword (8 bytes) from the memory address in the source register <code>[RSI]</code> to the address in the destination register <code>[RDI]</code>, and advances both registers by 8 bytes… and all this stuff happens in one instruction, so will never be split by an interrupt.
That’s an <em>interrupt</em>-atomic copy, which we can slot in place
of the software atomic copy in Blelloch and Wei’s proposal!</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_movs</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">x86</span><span class="o">.</span><span class="n">movs</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span>  <span class="c1"># R1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, the <code>MOVS</code> instruction is not atomic, but will be ordered with
respect to the <code>membarrier</code> syscall in <code>hp_cleanup_membarrier</code>: either
the copy fully executes before the <code>membarrier</code> in <code>C1</code>, in which case the
pinned value will be visible to the cleanup loop, or it executes after
the <code>membarrier</code>, which guarantees the copy will not observe a stale value
that’s waiting in the limbo list.</p>

<p>That’s just one instruction, but instructions aren’t all created
equal. <a href="https://uops.info/html-instr/MOVSQ.html"><code>MOVS</code> is on the heavy side</a>: in order to read from memory, write to memory, and increment two registers,
a modern Intel chip has to execute 5 micro-ops in at least ~5 cycles.
That’s not exactly fast; definitely better than an atomic (<code>LOCK</code>ed)
instruction, but not fast.</p>

<p>We can improve that with a trick from side-channel attacks, and
preserve wait-freedom.  We can usually guess what value we’ll find in
<code>record.pin</code>, simply by reading <code>cell</code> with a regular relaxed load.
Unless we’re extremely unlucky (realistically, as long as the reader
thread isn’t interrupted), <code>MOVSQ</code> will copy the same value we just
guessed.  That’s enough to exploit branch prediction and turn a data
dependency on <code>MOVSQ</code> (a high latency instruction) into a data
dependency on a regular load <code>MOV</code> (low latency), and a highly
predictable control dependency.  In very low level pseudo code, this
“speculative” version of the <code>MOVS</code> read-side might look like:</p>

<div id="hp_read_movs_spec">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs_spec.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_movs_spec</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">    <span class="n">x86</span><span class="o">.</span><span class="n">movs</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span>  <span class="c1"># R1</span>
</span><span class="line">    <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="p">:</span>  <span class="c1"># Likely</span>
</span><span class="line">        <span class="k">return</span> <span class="n">guess</span>
</span><span class="line">    <span class="k">return</span> <span class="n">record</span><span class="o">.</span><span class="n">pin</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

At this point though, we might as well just read assembly.

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_movs_spec.s </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="s"><span class="line"><span></span>    <span class="c1"># rsi: cell, rdi: record.pin</span>
</span><span class="line">    <span class="c1"># rax: guess</span>
</span><span class="line">    mov    <span class="p">(</span><span class="o">%rsi),%</span>rax            <span class="c1"># guess = cell.load_relaxed()</span>
</span><span class="line">    movsq  <span class="o">%ds:(%</span>rsi<span class="p">),</span><span class="o">%es:(%</span>rdi<span class="p">)</span>  <span class="c1"># MOVS cell -&gt; record.pin</span>
</span><span class="line">    cmp    <span class="o">%rax,-0x8(%</span>rdi<span class="p">)</span>        <span class="c1"># guess == record.pin ?</span>
</span><span class="line">    jne    slow                   <span class="c1"># if !=, goto slow</span>
</span><span class="line">    retq                          <span class="c1"># return guess</span>
</span><span class="line">slow<span class="o">:</span>
</span><span class="line">    mov    <span class="m">-0</span>x8<span class="p">(</span><span class="o">%rdi),%</span>rax        <span class="c1"># ret = record.pin</span>
</span><span class="line">    retq                          <span class="c1"># return ret</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>We’ll see that, in reasonable circumstances, this wait-free
code sequence is faster than the usual membarrier-based lock-free
read side.  But first, let’s see how we can achieve wait-freedom
when CISCy instructions like <code>MOVSQ</code> aren’t available, with an asymmetric “helping” scheme.</p>

<h2 id="interrupt-atomic-copy-with-some-help">Interrupt-atomic copy, with some help</h2>

<p>Blelloch and Wei’s wait-free atomic copy primitive builds on the usual
trick for wait-free algorithms: when a thread would wait for an
operation to complete, it helps that operation make progress instead
of blocking.  In this specific case, a thread initiates an atomic copy
by acquiring a fresh hazard pointer record, setting that descriptor’s
<code>pin</code> field to \(\bot\), publishing the address it wants to
copy from, and then performing the actual copy.  When another thread
enters the cleanup loop and wishes to read the record’s <code>pin</code> field , it may either find a value, or
\(\bot\); in the latter case, the cleanup thread has to help the
hazard pointer descriptor forward, by attempting to update the
descriptor’s <code>pin</code> field.</p>

<p>This strategy has the marked advantage of working.  However, it’s
also symmetric between the common case (the thread that initiated
the operation quickly completes it), and the worst case (a cleanup
thread notices the initiating thread got stuck and moves the
operation along).
This forces the common case to use atomic operations, similarly to the way cleanup threads would.
We pessimised the common case in order to eliminate blocking in the worst case, a frequent and unfortunate pattern in wait-free algorithms.</p>

<p>The source of that symmetry is our specification of an atomic copy
from the source field to a single destination <code>pin</code> field, which must
be written exactly once by the thread that initiated the copy
(the hazard pointer reader), or any concurrent helper (the cleanup loop).</p>

<p>We can relax that requirement, since we know that the hazard pointer
scanning loop can handle spurious or garbage pinned values.  Rather
than forcing both the read sequence (fast path) and the cleanup loop (slow path) to write to the same
<code>pin</code> field, we will give each HP record <em>two</em> pin fields: a
single-writer one for the fast path, and a multi-writer one for all
helpers (all threads in cleanup code).</p>

<p>The read-side sequence will have to first write to the HP record to
publish the cell it’s reading from, read and publish the cell’s pinned
value, and then check if a cleanup thread helped the record along.  If
the record was helped, the read-side sequence must use the value
written by the helping cleanup thread.  This means cleanup threads can
detect when a HP record is missing its pinned value, and help it along
with the cell’s current value.  Cleanup threads may later observe two
pinned values (both the reader and a cleanup thread wrote a pinned
value); in that case, both values are conservatively protected from
physical destruction.</p>

<p>Until now a hazard pointer record has only had one field, the “pinned”
value.  We must add some complexity to make this asymmetric helping
scheme work: in order for cleanup threads to be able to help, we must publish
the cell we are reading, and we need somewhere for cleanup threads to write
the pinned value they read.  We also need some sort of <a href="https://en.wikipedia.org/wiki/ABA_problem">ABA protection</a>
to make sure slow cleanup threads don’t overwrite a fresher pinned value
with a stale one, when the <em>cleanup thread</em> gets stuck (preempted).</p>

<p>Concretely, the HP record still has a <code>pin</code> field, which is only
written by the reader that owns the record, and read by cleanup
threads.  The <code>help</code> subrecord is written by both the owner of the
record and any cleanup thread that might want to move a reader along.  The
reader will first write the address of the pointer it wants to read
and protect in <code>cell</code>, generate a new unique generation id by incrementing
<code>gen_sequence</code>, and write that to <code>pin_or_gen</code>.  We’ll tag
generation ids with their sign: negative
values are generation ids, positive ones are addresses.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_record_wf.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="kt">intptr_t</span> <span class="n">gen_sequence</span> <span class="o">=</span> <span class="n">INTPTR_MIN</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">struct</span> <span class="n">hp_record_wf</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">void</span> <span class="o">*</span><span class="n">pin</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">void</span> <span class="o">**</span><span class="k">volatile</span> <span class="n">cell</span><span class="p">;</span>
</span><span class="line">                <span class="cm">/* -ve are gen, +ve are pinned pointers. */</span>
</span><span class="line">                <span class="k">volatile</span> <span class="kt">intptr_t</span> <span class="n">pin_or_gen</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span> <span class="n">help</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>At this point, any cleanup thread should be able to notice that the
<code>help.pin_or_gen</code> is a generation value, and find a valid cell address
in <code>help.cell</code>.  That’s all the information a cleanup threads needs to 
attempt to help the record move forward.  It can read the cell’s value, and publish
the pinned value it just read with an atomic compare-and-swap (CAS) of
<code>pin_or_gen</code>; if the CAS fails, another cleanup thread got there first, or
the reader has already moved on to a new target cell.  In the latter
case, any in-flight hazard pointer read sequence started before we
started reclaiming the limbo list, and it doesn’t matter what pinned
value we extract from the record.</p>

<p>Having populated the <code>help</code> subrecord, a reader can now publish a
value in <code>pin</code>, and then look for a pinned value in
<code>help.pin_or_gen</code>: if a cleanup thread published a pinned value there, the
reader must use it, and not the potentially stale (already destroyed)
value the reader wrote to <code>pin</code>.</p>

<p>On the read side, we obtain plain wait-freedom, with standard operations.
All we need are two compiler barriers to let
membarriers guarantee writes to the <code>help</code> subrecord are visible
before we start reading from the target cell, and to guarantee
that any cleanup thread’s write to <code>record.help.pin_or_gen</code> is visible
before we compare <code>record.help.pin_or_gen</code> against <code>gen</code>:</p>

<div id="hp_read_wf">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_wf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_wf</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">gen</span> <span class="o">=</span> <span class="n">gen_sequence</span>
</span><span class="line">    <span class="n">gen_sequence</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">cell</span><span class="p">)</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">store_release</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RA</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>  <span class="c1"># R2</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RB</span>
</span><span class="line">    <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span> <span class="o">!=</span> <span class="n">gen</span><span class="p">:</span>  <span class="c1"># Unlikely</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span>
</span><span class="line">    <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>On the cleanup side, we will consume the limbo list, issue a
membarrier to catch any read-side critical section that wrote to
<code>pin_or_gen</code> before we consumed the list, help these sections
along, issue another membarrier to guarantee that either the readers’
writes to <code>record.pin</code> are visible, or our writes to
<code>record.help.pin_or_gen</code> are visible to readers, and finally scan the
records while remembering to pin the union of <code>record.pin</code> and
<code>record.help.pin_or_gen</code> if the latter holds a pinned value.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_wf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_wf</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">gen</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="c1"># Help this record by populating its helped value.</span>
</span><span class="line">        <span class="k">if</span> <span class="n">gen</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="c1"># XXX: How do we know this is safe?!</span>
</span><span class="line">            <span class="n">value</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>
</span><span class="line">            <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">pin_or_gen</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">helped</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The membarrier in <code>C1</code> matches the compiler barrier in <code>RA</code>: if a
read-side section executed <code>R2</code> before we consumed the limbo list, its
writes to <code>record.help</code> must be visible.  The second membarrier in
<code>C2</code> matches the compiler barrier in <code>RB</code>: if the read-side section
has written to <code>record.pin</code>, that write must be visible, otherwise,
the cleanup threads’s write to <code>help.pin_or_gen</code> must be visible to the reader.
Finally, when scanning for pinned values, we can’t determine whether
the reader used its own value or the one we published, so we must
conservatively add both to the pinned set.</p>

<p>That’s a couple more instructions on the read-side than the
speculative <code>MOVSQ</code> implementation.  However, the instructions are
simpler, and the portable wait-free implementation benefits even more
from speculative execution: the final branch is equally predictable,
and now depends only on a read of <code>record.help.pin_or_gen</code>, which can
be satisfied by forwarding the reader’s own write to that same field.</p>

<p>The end result is that, in my microbenchmarks, this portable wait-free
implementation does slightly <em>better</em> than the speculative <code>MOVSQ</code> code.
We make this even tighter, by further specialising the code.  The cleanup
path is already slow.  What if we also assumed mutual exclusion; what if,
for each record, only one cleanup at a time could be in flight?</p>

<h2 id="interrupt-atomic-copy-with-at-most-one-helper">Interrupt-atomic copy, with at most one helper</h2>

<p>Once we may assume mutual exclusion between cleanup loops–more specifically, the “help” loop, the only part that writes to records–we don’t
have to worry about ABA protection anymore.  Hazard pointer records
can lose some weight.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_record_swf.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">hp_record_swf</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">void</span> <span class="o">*</span><span class="n">pin</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="p">{</span>
</span><span class="line">                <span class="k">volatile</span> <span class="kt">intptr_t</span> <span class="n">cell_or_pin</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span> <span class="n">help</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We’ll also use tagging with negative or positive values, this time to
distinguish target cell addresses (positive) from pinned values
(negative).  Now that the read side doesn’t have to update a
generation counter to obtain unique sequence values, it’s even simpler.</p>

<div id="hp_read_swf">
<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_read_swf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_read_swf</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">as_int</span><span class="p">())</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RA</span>
</span><span class="line">    <span class="n">guess</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span>  <span class="c1"># R2</span>
</span><span class="line">    <span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">store_relaxed</span><span class="p">(</span><span class="n">guess</span><span class="p">)</span>
</span><span class="line">    <span class="n">compiler_barrier</span><span class="p">()</span>  <span class="c1"># RB</span>
</span><span class="line">    <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Unlikely; &lt; cell.as_int() also works.</span>
</span><span class="line">        <span class="n">guess</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">())</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span>
</span><span class="line">    <span class="k">return</span> <span class="n">guess</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>

<p>The cleanup function isn’t particularly different, except for the new
encoding scheme.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_swf.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_swf</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="k">with</span> <span class="n">cleanup_lock</span><span class="p">:</span>
</span><span class="line">        <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">            <span class="n">cell</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">            <span class="k">if</span> <span class="n">cell</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="c1"># XXX: How do we know this is safe?!</span>
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">cell</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
</span><span class="line">                <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="o">-</span><span class="n">helped</span><span class="p">)</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p><a href="https://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUQHYHMoFoDGUCGCCuAHaAdyzFQDMAnSGACywC8tKATafAe0WEkoCg++ZqHQghCYNABKkLC158WWYFgBGWAM4wAspAC2HSgE9BwkKPGSA6pTC8A9AGFsefAJt3KTlwVQA+XQNjAC5oAFV8JR5odEhwcGgANyxwXEg+GTlef0DDI1CABVxVcBANWmgVSiRISVj4vlzjfw8eL2dZV1CAcUo1aFK9VQ4BsuA+AF4JgDFIBFjpAEFoe2hHAEYpjNl5SgAeVCb86R2YuITk1PTM3Zz9PMLi0vL2RARINku0xvvjA9aHB0cARQgAZDhyM7xAD6hmh+EQfAB7R8+DuQWOjkWsCh4FhlHhiGg5EMJFYkxmcwWUgAQis1gAmLY3XgHI6hMJaXH4wkIaAgcj84AAcg00CwrwQPwxLVsbW8nRB0AA8qotJREh9Je9Pik0ho+EA">
    <img src="/images/2020-07-07-flatter-wait-free-hazard-pointers/swf-hp.png" />
</a></p>

<p>Again, <code>RA</code> matches <code>C1</code>, and <code>RB</code> <code>C2</code>.  This new implementation is simpler than <code>hp_read_wf</code> on the read side, and needs even fewer instructions.</p>

<h2 id="qualitative-differences-between-hp-implementations">Qualitative differences between HP implementations</h2>

<p>A key attribute for hazard pointers is how much they slow down pointer
traversal in the common case.  However, there are other qualitative
factors that should impact our choice of implementation.</p>

<p>The <a href="https://github.com/concurrencykit/ck/blob/master/include/ck_hp.h">classic fenced (<code>hp_read_explicit</code>) implementation</a>
needs one atomic or fence instruction per read, but does not require any
exotic OS operation.</p>

<p>A <a href="https://github.com/facebook/folly/blob/master/folly/synchronization/Hazptr.h">simple membarrier implementation (<code>hp_read_membarrier</code>)</a>
is ABI-compatible with the fenced implementations, but lets the read side
replace the fence with a compiler barrier, as long as the
slow cleanup path can issue <a href="https://man7.org/linux/man-pages/man2/membarrier.2.html"><code>membarrier</code> syscalls</a>
on Linux, or <a href="https://docs.microsoft.com/en-us/windows/win32/api/processthreadsapi/nf-processthreadsapi-flushprocesswritebuffers">the similar <code>FlushProcessWriteBuffers</code></a>
on Windows.  All the remaining implementations (we won’t mention the much more complex wait-free implementation of <a href="https://arxiv.org/abs/2002.07053">Blelloch and Wei</a>)
also rely on the same syscalls to avoid fences or atomic instructions
on the read side, while additionally providing wait-freedom (constant
execution time) for readers, rather than mere lock-freedom.</p>

<p>The simple <code>MOVSQ</code>-based implementations (<code>hp_read_movs</code>) is fully
compatible with <code>hp_read_membarrier</code>, wait-free, and usually compiles
down to fewer instruction bytes, but is slightly slower.  Adding
speculation (<code>hp_read_movs_spec</code>) retains compatibility and closes the
performance gap, with a number of instruction bytes comparable to the
lock-free membarrier implementation.  In both cases, we rely on
<code>MOVSQ</code>, an instruction that only exists on <code>x86</code> and <code>amd64</code>.</p>

<p>However, we can also provide portable wait-freedom, once we modify the
cleanup code to help the read side sections forward.  The basic
implementation <code>hp_read_wf</code> compiles to many more instructions than
the other read-side implementations, but those instructions are mostly
upstream of the protected pointer read; in microbenchmarks, the result
can even be faster than the simple <code>hp_read_membarrier</code> or the
speculative <code>hp_read_movs_spec</code>.  The downside is that instruction
bytes tend to hurt much more in real code than in microbenchmarks.
We also rely on pointer tagging, which could make the code less widely applicable.</p>

<p>We can simplify and shrink the portable wait-free code by assuming
mutual exclusion on the cleanup path (<code>hp_read_swf</code>).  Performance is
improved or roughly the same, and instruction bytes comparable to
<code>hp_read_membarrier</code>.  However, we’ve introduced more opportunities
for reclamation hiccups.</p>

<p>More importantly, achieving wait-freedom with concurrent help suffers
from a fundamental issue: helpers don’t know that the pointer read they’re
helping move forward is stale until they (fail to) CAS into
place the value they just read.  This means they must be able to safely read potentially stale
pointers without crashing.  One might think mutual exclusion in the
cleanup function fixes that, but programs often mix and match
different reclamation schemes, as well as lock-free and lock-ful code.
On Linux, we could
abuse <a href="https://man7.org/linux/man-pages/man2/process_vm_readv.2.html">the <code>process_vm_readv</code> syscall</a>;<sup id="fnref:no-guarantees" role="doc-noteref"><a href="#fn:no-guarantees" class="footnote" rel="footnote">13</a></sup>
in general I suppose we could install signal handlers to catch <code>SIGSEGV</code> and <code>SIGBUS</code>.</p>

<div id="hp_read_swf-relaxed"></div>

<p>The stale read problem is even worse for the single-cleanup <code>hp_read_swf</code> read sequence:
there’s no ABA protection, so a cleanup helper can pin an
old value in <code>record.help.cell_or_pin</code>.  This could happen if a
read-side sequence is initiated before <code>hp_cleanup_swf</code>’s <code>membarrier</code>
in <code>C1</code>, and the associated incomplete record is noticed by the
helper, at which point the helper is preempted.  The read-side
sequence completes, and later uses the same record to read from the
same address… and that’s when the helper resumes execution, with a
<code>compare_exchange</code> that succeeds.</p>

<p>The pinned value “helped in” by <code>hp_cleanup_swf</code> is still valid—the
call to <code>hp_cleanup_swf</code> hasn’t physically destroyed anything yet—so
the hazard pointer implementation is technically correct.  However,
this scenario shows that <code>hp_read_swf</code> can violate memory ordering and
causality, and even let long-overwritten values time travel into the future.  The
simpler read-side code sequence comes at a cost: its load is extremely
relaxed, much more so than any intuitive mental model might allow.<sup id="fnref:hybrid-swf" role="doc-noteref"><a href="#fn:hybrid-swf" class="footnote" rel="footnote">14</a></sup></p>

<p>EDIT 2020-07-09: However, see <a href="#addendum-2020-07-09">this addendum</a> for a way to fix that race
without affecting the fast (read) path.</p>

<p>Having to help readers forward also loses a nice practical property of
hazard pointers: it’s always safe to spuriously consider arbitrary
(readable) memory as a hazard pointer record, it only costs us
additional conservatism in reclamation.  That’s not the case anymore,
once the cleanup thread has to help readers, and thus must write to HP
records.  This downside does not impact plain implementations of
hazard pointers, but does make it harder to improve record management
overhead by taking inspiration from managed language runtimes.</p>

<h2 id="some-microbenchmarks">Some microbenchmarks</h2>

<p>The overhead of hazard pointers only matters in code that traverse a
lot of pointers, especially pointer chains.  That’s why I’ll focus on
microbenchmarking a loop that traverses a pseudo-randomly shuffled
circular linked list (embedded in an array of 1024 nodes, at 16
bytes per node) for a fixed number of pointer chasing hops.  You can
find the <a href="https://gist.github.com/pkhuong/5f3acc53b4f046f2717e645ecd504f7b">code to replicate the results in this gist</a>.</p>

<p>The unprotected (baseline) inner loop follows.  Note the <code>NULL</code>
end-of-list check, for realism; the list is circular, so the loop
never breaks early.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_baseline.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line"><span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>  <span class="err">#</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">=</span> <span class="n">head</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>There’s clearly a dependency chain between each read of <code>head-&gt;next</code>.
The call to <code>frob_ptr</code> lets us introduce work in the dependency chain,
which more closely represents realistic use cases.  For example, when
using hazard pointers to protect a binary search tree traversal, we
must perform a small amount of work to determine whether we want to go
down the left or the right subtree.</p>

<p>A hazard pointer-ed implementation of this loop would probably unroll
the loop body twice, to more easily implement hand-over-hand locking.
That’s why I also include an unrolled version of this inner loop in
the microbenchmark: we avoid discovering that hazard pointer
protection improves performance because it’s also associated with
unrolling, and gives us an idea of how much variation we can expect from
small code generation changes.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_unrolled.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line"><span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">||</span> <span class="o">++</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_hops</span><span class="p">)</span>
</span><span class="line">                <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">;</span>
</span><span class="line">        <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The hazard pointer inner loops are just like the above, except that
<code>head = head-&gt;next</code> is replaced with calls to an inline function.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>traverse_hp.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="kr">inline</span> <span class="nf">__attribute__</span><span class="p">((</span><span class="n">__always_inline__</span><span class="p">,</span> <span class="n">__flatten__</span><span class="p">))</span> <span class="kt">uint64_t</span>
</span><span class="line"><span class="n">traverse_hp_generic</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">(</span><span class="o">*</span><span class="n">hp_read</span><span class="p">)(</span><span class="k">struct</span> <span class="n">hp_record</span> <span class="o">*</span><span class="p">,</span> <span class="kt">void</span> <span class="o">**</span><span class="p">),</span>
</span><span class="line">    <span class="kt">size_t</span> <span class="n">num_hops</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">start</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">static</span> <span class="k">struct</span> <span class="n">hp_record</span> <span class="n">backing</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">hp_record</span> <span class="o">*</span><span class="n">records</span> <span class="o">=</span> <span class="n">backing</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">node</span> <span class="o">*</span><span class="n">head</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">nodes</span><span class="p">[</span><span class="n">start</span><span class="p">];</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Let&#39;s pretend we published these records. */</span>
</span><span class="line">        <span class="k">asm</span> <span class="k">volatile</span><span class="p">(</span><span class="s">&quot;&quot;</span> <span class="o">:</span> <span class="s">&quot;+r&quot;</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">::</span> <span class="s">&quot;memory&quot;</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_hops</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span>
</span><span class="line">                        <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">hp_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">records</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">||</span> <span class="o">++</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">num_hops</span><span class="p">)</span>
</span><span class="line">                        <span class="k">break</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="n">head</span><span class="o">-&gt;</span><span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">^=</span> <span class="n">value</span><span class="p">;</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">hp_read</span><span class="p">(</span><span class="o">&amp;</span><span class="n">records</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">head</span><span class="o">-&gt;</span><span class="n">next</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">frob_ptr</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="n">acc</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The dependency chain is pretty obvious; we can measure the sum of
latencies for 1000 pointer dereferences by running the loop 1000
times.  I’m using a large iteration count to absorb noise from the
timing harness (roughly 50 cycles per call), as well as any boundary
effect around the first and last few loop iterations.</p>

<p>All the cycle measurements here are from my unloaded E5-4617, running
at 2.9 GHz without Turbo Boost.  First, let’s see what happens with a
pure traversal, where <code>frob_ptr</code> is an inline no-op function
that simply return its first argument.  This microbenchmark is far
from realistic (if I found an inner loop that <em>only</em> traversed a
singly linked list, I’d consider a different data structure), but helps
establish an upper bound on the overhead from different hazard pointer
read sides.  I would usually show a faceted graph of the latency
distribution for the various methods… but the results are so stable<sup id="fnref:did-not-randomize-codegen" role="doc-noteref"><a href="#fn:did-not-randomize-codegen" class="footnote" rel="footnote">15</a></sup>
that I doubt there’s any additional information to be found in the graphs,
compared to the tables below.</p>

<p>The following table shows the cycle counts for following 1000 pointers in a
circular linked list, with various hazard pointer schemes and <em>no work to find the next node</em>, on an unloaded E5-4617 @ 2.9 GHz, without Turbo Boost.</p>

<pre><code>| Method * 1000 iterations  | p00.1 latency | median latency | p99.9 latency |
|---------------------------|---------------|----------------|---------------|
|  noop                     |            52 |             56 |            56 |
|  baseline                 |          4056 |           4060 |          4080 |
|  unrolled                 |          4056 |           4060 |          4080 |
|  hp_read_explicit         |         20136 |          20160 |         24740 |
|  hp_read_membarrier       |          5080 |           5092 |          5164 |
|  hp_read_movs             |         10060 |          10076 |         10348 |
|  hp_read_movs_spec        |          8568 |           8568 |          8572 |
|  hp_read_wf               |          6572 |           7620 |          8140 |
|  hp_read_swf              |          4268 |           4304 |          4368 |
</code></pre>

<p>The table above reports quantiles for the total runtime of 1000
pointer dereferences, after one million repetitions.</p>

<p>We’re looking at a baseline of 4 cycles/pointer dereference (the L1 cache
latency), regardless of unrolling.  The only implementation with an
actual fence or atomic operation, <code>hp_read_explicit</code> fares pretty
badly, at more than 5x the latency.  Replacing that fence with a
compiler barrier in <code>hp_read_membarrier</code> reduces the overhead to ~1
cycle per pointer dereference.  Our first wait-free implementation,
<code>hp_read_movs</code> (based on a raw <code>MOVSQ</code>) doesn’t do too great, with a
~6 cycle (150%) overhead for each pointer dereference.  However,
speculation (<code>hp_read_movs_spec</code>) does help shave that to ~4.5 cycles
(110%).  The portable wait-free implementation <code>hp_read_wf</code> does
slightly better, and its single-cleanup version <code>hp_read_swf</code> takes
the crown, by adding around 0.2 cycle/dereference.</p>

<p>These results are stable and repeatable, but still fragile, in a way:
except for <code>hp_read_explicit</code>, which is massively slowed down by its
atomic operation, and for <code>hp_read_movs</code>, which adds a known latency bump on the hot path, the other slowdowns mostly reflect contention for
execution resources.  In real life, such contention usually only occurs in
heavily tuned code, and the actual execution units (ports) in high
demand will vary from one inner loop to another.</p>

<p>Let’s see what happens when we insert a ~4-cycle latency slowdown
(three for the multiplication, and one more for the increment) in the
hot path, by redefining <code>frob_ptr</code>.  The result of the integer
multiplication by 0 is always 0, but adds a (non speculated) data
dependency on the node value and on the multiplication to the pointer
chasing dependency chain.  Only four cycles of work to decide which
pointer to traverse is on the low end of my practical experience, but
suffices to equalise away most of the difference between the hazard
pointer implementations.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>frob_ptr.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span> <span class="o">*</span>
</span><span class="line"><span class="nf">frob_ptr</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">ptr</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">uintptr_t</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Make sure we get an actual MUL. */</span>
</span><span class="line">        <span class="k">asm</span><span class="p">(</span><span class="s">&quot;&quot;</span> <span class="o">:</span> <span class="s">&quot;+r&quot;</span><span class="p">(</span><span class="n">y</span><span class="p">));</span>
</span><span class="line">        <span class="k">return</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">ptr</span> <span class="o">+</span> <span class="p">(</span><span class="n">value</span> <span class="o">*</span> <span class="n">y</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Let’s again look at the quantiles for the cycle count for one million
loops of 1000 pointer dereferences, on an unloaded E5-4617 @ 2.9 GHz.</p>

<pre><code>| Method * 1000 iterations  | p00.1 latency | median latency | p99.9 latency |
|---------------------------|---------------|----------------|---------------|
|  noop                     |            52 |             56 |            56 |
|  baseline                 |         10260 |          10320 |         10572 |
|  unrolled                 |          9056 |           9060 |          9180 |
|  hp_read_explicit         |         22124 |          22156 |         26768 |
|  hp_read_membarrier       |         10052 |          10084 |         10264 |
|  hp_read_movs             |         12084 |          12112 |         15896 |
|  hp_read_movs_spec        |          9888 |           9940 |         10152 |
|  hp_read_wf               |          9380 |           9420 |          9672 |
|  hp_read_swf              |         10112 |          10136 |         10360 |
</code></pre>

<p>The difference between <code>unrolled</code> in this table and in the previous
one shows we actually added around 5 cycles of latency per iteration
with the multiplication in <code>frob_ptr</code>.  This dominates the overhead we
estimated earlier for all the hazard pointer schemes except for the
remarkably slow <code>hp_read_explicit</code> and <code>hp_read_movs</code>.  It’s thus not
surprising that all hazard pointer implementations but the latter
two are on par with the unprotected traversal loops (within 1.1 cycle
per pointer dereference, less than the impact of unrolling the loop
without unlocking any further rewrite).</p>

<p>The relative speed of the methods has changed, compared
to the previous table.  The speculative wait-free implementation
<code>hp_read_movs_spec</code> was slower than <code>hp_read_membarrier</code> and much
slower than <code>hp_read_swf</code>; it’s now slightly faster than both.
The simple portable wait-free implementation <code>hp_read_wf</code> was slower than
<code>hp_read_membarrier</code> and <code>hp_read_swf</code>; it’s now the fastest implementation.</p>

<p>I wouldn’t read too much into the relative rankings of
<code>hp_read_membarrier</code>, <code>hp_read_movs_spec</code>, <code>hp_read_wf</code>, and
<code>hp_read_swf</code>.  They only differ by fractions of a cycle per
dereference (all between 9.5 and 10.1 cycle/deref), and the exact values are a function of the
specific mix of micro-ops in the inner loop, and of the
near-unpredictable impact of instruction ordering on the chip’s
scheduling logic.  What really matters is that their impact
on traversal latency is negligible once the pointer chasing loop does <em>some</em>
work to find the next node.</p>

<h2 id="whats-the-best-hazard-pointer-implementation">What’s the best hazard pointer implementation?</h2>

<p>I hope I’ve made a convincing case that hazard pointers can be
<em>wait-free and efficient</em> on the read-side, as long as we have access
to something like <code>membarrier</code> or <code>FlushProcessWriteBuffers</code> on the
slow cleanup (reclamation) path.  If one were to look at the
microbenchmarks alone, one would probably pick <code>hp_read_swf</code>.</p>

<p>However, the real world is more complex than microbenchmarks.  When I
have to extrapolate from microbenchmarks, I usually worry about the
hidden impact of instruction bytes or cold branches, since
microbenchmarks tend to fail at surfacing these things.  I’m not as
worried for <code>hp_read_movs_spec</code>, and <code>hp_read_swf</code>: they both compile
down to roughly as many instructions as the incumbent, <code>hp_read_membarrier</code>,
and their forward untaken branch would be handled fine by a static predictor.</p>

<p>What I would take into account is the ability to transparently use
<code>hp_read_movs_spec</code> in code that already uses <code>hp_read_membarrier</code>,
and the added requirements of <code>hp_read_swf</code>.  In
addition to relying on <code>membarrier</code> for correctness, <code>hp_read_swf</code>
needs a pointer tagging scheme to distinguish target pointers from pinned ones, a way for cleanup threads to read stale pointers without
crashing, and also imposes mutual exclusion around the scanning of (sets
of) hazard pointer records.  These additional requirements don’t seem
impractical, but I can imagine code bases where they would constitute
hard blockers (e.g., library code, or when protecting arbitrary integers).
Finally, <code>hp_read_swf</code> can let protected values time travel in the future,
with read sequences returning values so long after they were overwritten
that the result violates pretty much any memory ordering model… unless
you implement the addendum below.</p>

<p>TL;DR: <a href="#hp_read_swf">Use <code>hp_read_swf</code></a> if you’re willing to sacrifice wait-freedom on the reclamation path <a href="#addendum-2020-07-09">and remember to implement the cleanup function with time travel protection</a>.  When targeting <code>x86</code> and <code>amd64</code>, <a href="#hp_read_movs_spec"><code>hp_read_movs_spec</code> is a well rounded option</a>, and still wait-free.  Otherwise, <a href="#hp_read_wf"><code>hp_read_wf</code> uses standard operations</a>, but compiles down to more code.</p>

<p>P.S., <a href="https://travisdowns.github.io/">Travis Downs</a> notes that mem-mem <code>PUSH</code> might be an alternative to <code>MOVSQ</code>, but that requires either pointing <code>RSP</code> to arbitrary memory, or allocating hazard pointers on the stack (which isn’t necessarily a bad idea).  Another idea worthy of investigation!</p>

<p><small>Thank you, Travis, for deciphering and validating a much rougher draft when the preprint dropped, and Paul and Jacob, for helping me clarify this last iteration.</small></p>

<div id="addendum-2020-07-09"></div>

<h2 id="addendum-2020-07-09-fix-time-travel-in-hp_read_swf">Addendum 2020-07-09: fix time travel in <code>hp_read_swf</code></h2>

<p>There is one huge practical issue with
<a href="#hp_read_swf"><code>hp_read_swf</code>, our simple and wait-free hazard pointer read sequence</a>
that sacrifices lock-free reclamation to avoid x86-specific instructions:
<a href="#hp_read_swf-relaxed">when the cleanup loop must help a record forward, it can fill in old values in ways that violate causality</a>.</p>

<p>I noted that the reason for this hole is the lack of ABA protection in
HP records… and <a href="#hp_read_wf"><code>hp_read_wf</code> is what we’d get if we were to add full ABA protection</a>.</p>

<p>However, given mutual exclusion around the “help” loop in the cleanup
function, we don’t need full ABA protection.  What we really need to
know is whether a given in-flight record we’re about to CAS forward is
the same in-flight record for which we read the cell’s value, or was
overwritten by a read-side section.  We can encode that by
stealing one more bit from the target <code>cell</code> address in <code>cell_or_pin</code>.</p>

<p>We already steal the sign bit to distinguish the address of the <code>cell</code>
to read (positive), from pinned values (negative).  The split make
sense because 64 bit architectures tend to reserve high (negative)
addresses for kernel space.  I doubt we’ll see full 64 bit address
spaces for a while, so it seems safe to steal the next bit (bit 62) to
tag <code>cell</code> addresses.  The next table summarises the tagging scheme.</p>

<pre><code>0b00xxxx: untagged cell address
0b01xxxx: tagged cell address
0b1yyyyy: helped pinned value
</code></pre>

<p>At a high level, we’ll change the <code>hp_cleanup_swf</code> to tag
<code>cell_or_pin</code> before reading the value pointed by <code>cell</code>, and only CAS in the
new pinned value if the cell is still tagged.
Thanks to mutual exclusion, we know <code>cell_or_pin</code> can’t be re-tagged by another thread.
Only the <code>for record in records</code> block has to change.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hp_cleanup_swf_no_time_travel.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hp_cleanup_swf_no_time_travel</span><span class="p">(</span><span class="n">limbo</span><span class="p">,</span> <span class="n">records</span><span class="p">):</span>
</span><span class="line">    <span class="k">with</span> <span class="n">cleanup_lock</span><span class="p">:</span>
</span><span class="line">        <span class="n">to_reclaim</span> <span class="o">=</span> <span class="n">limbo</span><span class="o">.</span><span class="n">consume_snapshot_acquire</span><span class="p">()</span>
</span><span class="line">        <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C1</span>
</span><span class="line">        <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">            <span class="n">cell</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">            <span class="k">if</span> <span class="n">cell</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="k">continue</span>
</span><span class="line">            <span class="c1"># We assume valid addresses do not have that bit set.</span>
</span><span class="line">            <span class="k">if</span> <span class="p">(</span><span class="n">cell</span> <span class="o">&amp;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">62</span><span class="p">))</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="k">continue</span>
</span><span class="line">            <span class="n">tagged</span> <span class="o">=</span> <span class="n">cell</span> <span class="o">|</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="mi">62</span><span class="p">)</span>
</span><span class="line">            <span class="k">if</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">tagged</span><span class="p">):</span>
</span><span class="line">                <span class="c1"># Compare exchange should act as a store-load barrier.</span>
</span><span class="line">                <span class="c1"># XXX: How do we know this load is safe?!</span>
</span><span class="line">                <span class="n">value</span> <span class="o">=</span> <span class="o">-</span><span class="n">cell</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">()</span><span class="o">.</span><span class="n">load_acquire</span><span class="p">()</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
</span><span class="line">                <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">compare_exchange</span><span class="p">(</span><span class="n">tagged</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line">    <span class="n">os</span><span class="o">.</span><span class="n">membarrier</span><span class="p">()</span>  <span class="c1"># C2</span>
</span><span class="line">    <span class="n">pinned</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">records</span><span class="p">:</span>
</span><span class="line">        <span class="n">helped</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">help</span><span class="o">.</span><span class="n">cell_or_pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">()</span>
</span><span class="line">        <span class="k">if</span> <span class="n">helped</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">            <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="o">-</span><span class="n">helped</span><span class="p">)</span><span class="o">.</span><span class="n">as_ptr</span><span class="p">())</span>
</span><span class="line">        <span class="n">pinned</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">pin</span><span class="o">.</span><span class="n">load_relaxed</span><span class="p">())</span>
</span><span class="line">    <span class="k">for</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">to_reclaim</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">resource</span> <span class="ow">in</span> <span class="n">pinned</span><span class="p">:</span>
</span><span class="line">            <span class="n">limbo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">resource</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">resource</span><span class="o">.</span><span class="n">destroy</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We doubled the number of atomic operations in the helping loop, but
that’s acceptable on the slow path.  We also rely on strong
<code>compare_exchange</code> (compare-and-swap) acting as a store-load fence.
If that doesn’t come for free, we could also tag records in one pass,
issue a store-load fence, and help tagged records in a
second pass.</p>

<p>Another practical issue with the <code>swf</code>/<code>wf</code> cleanup approach is that
they require <em>two</em> membarriers, and
<a href="/Blog/2019/01/09/preemption-is-gc-for-memory-reordering/#loaded-preemption-latency">OS-provided implementations can be slow, especially under load</a>.
This is particularly important for the <code>swf</code> approach, since mutual
exclusion means that one slow <code>membarrier</code> delays the physical destruction of everything on the limbo list.</p>

<p>I don’t think we can get rid of mutual exclusion, and, while
<a href="https://github.com/pkhuong/barrierd">we can improve membarrier latency</a>,
reducing the number of membarriers on the reclamation path is always good.</p>

<p>We can software pipeline calls to the cleanup function,
and use the same <code>membarrier</code> for <code>C1</code> and <code>C2</code> in two consecutive
cleanup calls.  Overlapping cleanups decreases the worst-case reclaim
latency from 4 membarriers to 3, and that’s not negligible when each
<code>membarrier</code> can block for 30ms.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:but-also" role="doc-endnote">
      <p>I also tend to read anything by <a href="https://dblp.uni-trier.de/pers/b/Blelloch:Guy_E=.html">Guy Blelloch</a>. <a href="#fnref:but-also" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:it-is-gc" role="doc-endnote">
      <p>In fact, I’ve often argued that SMR <em>is</em> garbage collection, just not fully tracing GC.  Hazard pointers in particular look a lot like deferred reference counting, <a href="https://web.eecs.umich.edu/~weimerw/2012-4610/reading/bacon-garbage.pdf">a form of tracing GC</a>. <a href="#fnref:it-is-gc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-obvious" role="doc-endnote">
      <p>Something that wasn’t necessarily obvious until then. See, for example, <a href="https://arxiv.org/abs/2001.01999">this article presented at PPoPP 2020</a>, which conjectures that “making the original Hazard Pointers scheme or epoch-based reclamation completely wait-free seems infeasible;” Blelloch was in attendance, so this must have been a fun session. <a href="#fnref:not-obvious" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:RCU" role="doc-endnote">
      <p>The SMR problem is essentially the same problem as determining when it’s safe to return from <code>rcu_synchronize</code> or execute a <code>rcu_call</code> callback.  Hence, the same <a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix03/tech/freenix03/full_papers/arcangeli/arcangeli.pdf#page=5">reader-writer lock analogy</a> holds. <a href="#fnref:RCU" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:stable-alloc" role="doc-endnote">
      <p>Hazard pointer records must still be managed separately, e.g., with a type stable allocator, but we can bootstrap everything else once we have a few records per thread. <a href="#fnref:stable-alloc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:even-with-pinned-nodes" role="doc-endnote">
      <p>We can even do that without keeping track of the number of nodes that were previously pinned by hazard pointer records and kept in the limbo list: each record can only pin at most one node, so we can wait until the limbo list is, e.g., twice the size of the record set. <a href="#fnref:even-with-pinned-nodes" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:or-hw-level" role="doc-endnote">
      <p>Let’s also hope efforts like <a href="https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/">CHERI</a> don’t have to break lock-free code. <a href="#fnref:or-hw-level" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:guessing-is-fine" role="doc-endnote">
      <p>The scheme is correct with any actual guess; we could even use a random number generator. However, performance is ideal (the loop exits) when we “guess” by reading current value of the pointer we want to borrow safely. <a href="#fnref:guessing-is-fine" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:beware-accidental-success" role="doc-endnote">
      <p>I tend to implement lock-free algorithms with a heavy dose of inline assembly, or <a href="http://concurrencykit.org/">Concurrency Kit</a>’s wrappers: it’s far too easy to run into subtle undefined behaviour.  For example, comparing a pointer after it has been freed is UB in C and C++, even if we don’t access the pointee.  Even if we compare as <code>uintptr_t</code>, it’s apparently debatable whether the code is well defined when the comparison happens to succeed because the pointee was freed, then recycled in an allocation and published to <code>cell</code> again. <a href="#fnref:beware-accidental-success" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:DEBRA-plus" role="doc-endnote">
      <p>In <a href="http://www.cs.utoronto.ca/~tabrown/debra/fullpaper.pdf">Reclaiming Memory for Lock-Free Data Structures: There has to be a Better Way</a>, Trevor Brown argues this requirement is a serious flaw in all hazard pointer schemes.  I think it mostly means applications should be careful to coarsen the definition of resource in order to ensure the resulting condensed heap graph is a DAG.  In extreme cases, we end up proxy collecting an epoch, but we can usually do much better. <a href="#fnref:DEBRA-plus" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:travis-says-its-fine" role="doc-endnote">
      <p>That’s what <a href="https://travisdowns.github.io/blog/2020/07/06/concurrency-costs.html">Travis Downs classifies as a Level 1 concurrency cost</a>, which is usually fine for writes, but adds a sizable overhead to simple read-only code. <a href="#fnref:travis-says-its-fine" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:arguably-lock-ful" role="doc-endnote">
      <p>I suppose this means the reclamation path isn’t wait-free, or even lock-free, anymore, in the strict sense of the words. In practice, we’re simply waiting for periodic events that would occur regardless of the syscalls we issue.  People who really know what they’re doing might have fully isolated cores.  If they do, they most likely have a watchdog on their isolated and latency-sensitive tasks, so we can still rely on running some code periodically, potentially after some instrumentation: if an isolated task fails to check in for a short while, the whole box will probably be presumed wedged and taken offline. <a href="#fnref:arguably-lock-ful" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:no-guarantees" role="doc-endnote">
      <p>With the caveat that the public documentation for <code>process_vm_readv</code> does not mention any atomic load guarantee. In practice, I saw a long-by-long copy loop the last time I looked at the code, and I’m pretty sure the kernel’s build flags prevent GCC/clang from converting it to <code>memcpy</code>. We could rely on the strong “don’t break userspace” culture, but it’s probably a better idea to try and get that guarantee in writing. <a href="#fnref:no-guarantees" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:hybrid-swf" role="doc-endnote">
      <p>This problem feels like something we could address with a coarse epoch-based versioning scheme.  It’s however not clear to me that the result would be much simpler than <code>hp_read_wf</code>, and we’d have to steal even more bits (2 bits, I expect) from <code>cell_or_pin</code> to make room for the epoch.  EDIT 2020-07-09: it turns out <a href="#addendum-2020-07-09">we only need to steal one bit</a>. <a href="#fnref:hybrid-swf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:did-not-randomize-codegen" role="doc-endnote">
      <p>Although I did compare several independent executions and confirmed the reported cycle counts were stable, I did not try to randomise code generation… mostly because I’m not looking for super fine differences as much as close enough runtimes.  Hopefully, aligning functions to 256 bytes leveled some bias away. <a href="#fnref:did-not-randomize-codegen" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Preemption is GC for memory reordering]]></title>
    <link href="https://www.pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering/"/>
    <updated>2019-01-09T16:29:37-05:00</updated>
    <id>https://www.pvk.ca/Blog/2019/01/09/preemption-is-gc-for-memory-reordering</id>
    <content type="html"><![CDATA[<p>I previously <a href="/Blog/2018/08/25/restartable-sequences-with-the-polysemic-null-segment-selector/">noted how preemption makes lock-free programming harder in userspace than in the kernel</a>.
I now believe that preemption ought to be treated as a sunk cost, like
garbage collection: we’re already paying for it, so we might as
well use it. Interrupt processing (returning from an interrupt
handler, actually) is fully serialising on x86, and on other
platforms, no doubt: any userspace instruction either fully executes
before the interrupt, or is (re-)executed from scratch some time after
the return back to userspace. That’s something we can abuse to
guarantee ordering between memory accesses, without explicit barriers.</p>

<p>This abuse of interrupts is complementary to <a href="https://www.cs.tau.ac.il/~mad/publications/asplos2014-ffwsq.pdf">Bounded TSO</a>.
Bounded TSO measures the hardware limit on the number of store instructions
that may concurrently be in-flight (and combines that with the
knowledge that instructions are retired in order) to guarantee
liveness without explicit barriers, with no overhead, and usually
marginal latency.  However, without worst-case execution
time information, it’s hard to map instruction counts to real time.
Tracking interrupts lets us determine when enough real time has
elapsed that earlier writes have definitely retired,
albeit after a more conservative delay than Bounded TSO’s typical
case.</p>

<p>I reached this position after working on two lock-free
synchronisation primitives—<a href="http://www.1024cores.net/home/lock-free-algorithms/eventcounts">event counts</a>,
and asymmetric flag flips as used in <a href="https://ieeexplore.ieee.org/document/1291819">hazard pointers</a> 
and <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation</a>—that
are similar in that a slow path waits for a sign of life from a fast
path, but differ in the way they handle “stuck” fast paths. I’ll cover
the event count and flag flip implementations that I came to on
Linux/x86[-64], which both rely on interrupts for ordering. Hopefully
that will convince you too that preemption is a useful source of
pre-paid barriers for lock-free code in userspace.</p>

<p><small>I’m writing this for readers who are already familiar with
<a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">lock-free programming</a>,
<a href="http://www.cs.utoronto.ca/~tomhart/papers/tomhart_thesis.pdf">safe memory reclamation techniques</a> in particular, and have some
experience reasoning with <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf">formal memory models</a>.
For more references, <a href="https://queue.acm.org/detail.cfm?id=2492433">Samy’s overview in the ACM Queue</a> is a good resource.
I already committed the code for 
<a href="https://github.com/concurrencykit/ck/commit/a16642f95c048c65d47107205a2cfc70d099dbd6">event counts in Concurrency Kit</a>,
and for <a href="https://github.com/pkhuong/barrierd">interrupt-based reverse barriers in my <code>barrierd</code> project</a>.
</small></p>

<h1 id="event-counts-with-x86-tso-and-futexes">Event counts with x86-TSO and futexes</h1>

<p>An <a href="http://www.1024cores.net/home/lock-free-algorithms/eventcounts">event count</a> 
is essentially a version counter that lets threads wait
until the current version differs from an arbitrary prior version.  A
trivial “wait” implementation could spin on the version counter.
However, the value of event counts is that they let lock-free code
integrate with OS-level blocking: waiters can grab the event count’s
current version <code>v0</code>, do what they want with the versioned data, and
wait for new data by <em>sleeping</em> rather than burning cycles until the
event count’s version differs from <code>v0</code>.  The event count is a common
synchronisation primitive that is often reinvented and goes by many
names (e.g.,
<a href="https://github.com/gwsystems/composite/issues/377">blockpoints</a>);
what matters is that writers can update the version counter, and
waiters can read the version, run arbitrary code, then efficiently
wait while the version counter is still equal to that previous
version.</p>

<p>The explicit version counter solves the lost wake-up issue associated
with misused condition variables, as in the pseudocode below.</p>

<pre><code>bad condition waiter:

while True:
    atomically read data
    if need to wait:
        WaitOnConditionVariable(cv)
    else:
        break
</code></pre>

<p>In order to work correctly, condition variables require waiters to
acquire a mutex that protects both data and the condition variable,
before checking that the wait condition still holds and then waiting
on the condition variable.</p>

<pre><code>good condition waiter:

while True:
    with(mutex):
        read data
        if need to wait:
	        WaitOnConditionVariable(cv, mutex)
	    else:
	        break Waiters must prevent writers from making changes to the data, otherwise the data change (and associated condition variable wake-up) could occur between checking the wait condition, and starting to wait on the condition variable.  The waiter would then have missed a wake-up and could end up sleeping forever, waiting for something that has already happened.

good condition waker:

with(mutex):
    update data
    SignalConditionVariable(cv)
</code></pre>

<p>The six diagrams below show the possible interleavings between the
signaler (writer) making changes to the data and waking waiters, and
a waiter observing the data and entering the queue to wait for
changes.  The two left-most diagrams don’t interleave anything; these
are the only scenarios allowed by correct locking.  The remaining four
actually interleave the waiter and signaler, and show that, while
three are accidentally correct (lucky), there is one case, <code>WSSW</code>,
where the waiter misses its wake-up.</p>

<div style="display: grid; grid-template-columns: repeat(3, 1fr); width: 150%; margin-left: -25%;">
<!-- SSWW (1, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQDKpGGANNAOID29AJgJQBQbADsgE6gDGIbvGDRSIAObxkUHmybJgyAEbJEMALKQAtvR4BPLrwFDkI6FjCQ5bcVJnWAtAD4tugwC5o-ABZmJMApKJACM7HbSsi5uevpeAO7IANYwiVY8iCQATOwxBo4ulsDWXvTK6jwAboGKyKHsRU6uOrFekCLW0GnA+IQwxDlAA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-ssww.png" />
    </a>
    </div>
<!-- SWSW (1, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQDKG5ANNADJ4DGA1gJ4CUAUOwA7IBOo9ED3jBopEAHN4yKL3YATZMGQAjZIhgBZSAFsA9r2bc+AochHQsYSHPbipMmwFoAfNv2GAXNHoALcxIwisokAIwc7gbMTq5WwDbeeioavABuQUrIYRz20rKukV7QAO7IjDCl1ryIJABMHHHObrpR3pAiNiXYwPiEMMT17EA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-swsw.png" />
    </a>
    </div>
<!-- SWWS (1, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQDKGGpANNADJ4DGA1gJ4CUAUBwA7IBOoBiF7xg0UiADm8ZFD4cAJsmDIARskQwAspAC2Aez4se-QcOSjoWMJHkcJ02bYC0APh0GjALmgMAFhaSMEoqJACMnB6GLM5u1sC2PvqqmnwAbsHKyOGc8S7uetE+kKK20ADu2MD4hDDEAEycDjJyblHeFchMMJU2fIgkjRxAA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-swws.png" />
    </a>
    </div>
<!-- WWSS (2, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQYYDK5ANNAOID29AJgJQBQbADsgE6gDGIbvGDRyIAObxkUHmybJgyAEbJEMALKQAtvR4BPLrwFDkI6FjCQ5bLboMBaBwD5LwawC5o9Zep4A3GAUlEgBGdjdrFzs9fS9IEWtoAHdsYHxCGGIAJnZxKRko5xiDL34ACzMJIMVkMLzJaVlonVivVIBrGFSrHkQSXLYgA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-wwss.png" />
    </a>
    </div>
<!-- WSWS (2, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQYDK5ANNADJ4DGA1gJ4CUAUOwA7IBOo9ED3jBoZEAHN4yKL3YATZMGQAjZIhgBZSAFsA9r2bc+AochHQsYSHPbb9hgLSOAfFeA2AXND0qNvADcYRWUSAEYOcSkZG1d7A2ZvegALcwlgpWRwjndYl3jDb0gRG2gAd2xgfEIYYgAmSMlpWTjdBO8KxhgK615EEgb2IA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-wsws.png" />
    </a>
    </div>
<!-- WSSW (2, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIFUDsDOBDAZjA6ssBHArpAdABQYDKZGANNAELIAmAlAFAsAOyATqAMYid4waGRABzeMihcWDZMGQAjZIhgBZSAFsA9lwCeHbnwHIh0LGEgyWGnfoC09gHwXgVgFzRti1VwBuMHIKJACMrKISUlbOtrp6nrwAFqZigfLIoeHiktIxWnGeAO7IANYwxZZciCQATKyu0U6x+p6QQlbQFcD4hDDEdSxAA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unsafe-waitqueue-wssw.png" />
    </a>
    </div>
</div>

<p>If any waiter can prevent writers from making progress, we don’t have
a lock-free protocol. Event counts let waiters detect when they would
have been woken up (the event count’s version counter has changed),
and thus patch up this window where waiters can miss wake-ups for data
changes they have yet to observe. Crucially, waiters detect lost
wake-ups, rather than preventing them by locking writers out. Event
counts thus preserve lock-freedom (and even wait-freedom!).</p>

<p>We could, for example, use an event count in a lock-free ring buffer:
rather than making consumers spin on the write pointer, the write
pointer could be encoded in an event count, and consumers would then
efficiently block on that, without burning CPU cycles to wait for new
messages.</p>

<p>The challenging part about implementing event counts isn’t making sure
to wake up sleepers, but to only do so when there are sleepers to
wake.  For some use cases, we don’t need to do any active wake-up,
because exponential backoff is good enough: if version updates
signal the arrival of a response in a request/response communication
pattern, exponential backoff, e.g., with a <code>1.1x</code> backoff factor,
could bound the increase in response latency caused by the blind sleep
during backoff, e.g., to 10%.</p>

<p>Unfortunately, that’s not always applicable.  In general, we can’t
assume that signals corresponds to responses for prior requests, and
we must support the case where progress is usually fast enough that
waiters only spin for a short while before grabbing more work.  The
latter expectation means we can’t “just” unconditionally execute a
syscall to wake up sleepers whenever we increment the version counter:
that would be too slow.  This problem isn’t new, and has a solution
similar to the one deployed in adaptive spin locks.</p>

<p>The solution pattern for adaptive locks relies on tight integration
with an OS primitive, e.g., <a href="https://www.akkadia.org/drepper/futex.pdf">futexes</a>.
The control word, the machine word on which waiters spin, encodes its
usual data (in our case, a version counter), as well as a new flag to denote that
there are sleepers waiting to be woken up with an OS syscall. Every
write to the control word uses atomic read-modify-write instructions,
and before sleeping, waiters ensure the “sleepers are present” flag is
set, <em>then make a syscall to sleep only if the control word
is still what they expect, with the sleepers flag set</em>.</p>

<p><a href="https://github.com/openbsd/src/blob/dbb9e73f5c4a3032c7e16db983dfa4f7c022e352/sys/kern/sys_futex.c#L109">OpenBSD’s compatibility shim</a> for Linux’s futexes
is about as simple an implementation of the futex calls as it gets.  The
OS code for futex wake and wait is identical to what userspace would do
with mutexes and condition variables (waitqueues). Waiters lock out
wakers for the futex word or a coarser superset, check that the futex
word’s value is as expected, and enters the futex’s waitqueue. Wakers
acquire the futex word for writes, and wake up the waitqueue.  The
difference is that all of this happens in the kernel, which, unlike
userspace, can force the scheduler to be helpful.  Futex code can run
in the kernel because, unlike arbitrary mutex/condition variable
pairs, the protected data is always a single machine integer, and
the wait condition an equality test. This setup is simple enough to
fully implement in the kernel, yet general enough to be useful.</p>

<p>OS-assisted conditional blocking is straightforward enough to adapt to
event counts.  The control word is the event count’s version counter,
with one bit stolen for the “sleepers are present” flag (sleepers
flag).</p>

<p>Incrementing the version counter can use a regular atomic increment;
we only need to make sure we can tell whether the sleepers flag might
have been set before the increment. If the sleepers flag was set, we
clear it (with an atomic bit reset), and wake up any OS thread blocked
on the control word.</p>

<pre><code>increment event count:

old &lt;- fetch_and_add(event_count.counter, 2)  # flag is in the low bit
if (old &amp; 1):
    atomic_and(event_count.counter, -2)
    signal waiters on event_count.counter
</code></pre>

<p>Waiters can spin for a while, waiting for the version counter to
change. At some point, a waiter determines that it’s time to stop
wasting CPU time. The waiter then sets the sleepers flag with a
compare-and-swap: the CAS (compare-and-swap) can only fail because the
counter’s value has changed or because the flag is already set. In the
former failure case, it’s finally time to stop waiting.  In the latter
failure care, or if the CAS succeeded, the flag is now set.  The
waiter can then make a syscall to block on the control word, but only
if the control word still has the sleepers flag set and contains the
same expected (old) version counter.</p>

<pre><code>wait until event count differs from prev:

repeat k times:
    if (event_count.counter / 2) != prev:  # flag is in low bit.
        return
compare_and_swap(event_count.counter, prev * 2, prev * 2 + 1)
if cas_failed and cas_old_value != (prev * 2 + 1):
    return
repeat k times:
    if (event_count.counter / 2) != prev:
        return
sleep_if(event_count.center == prev * 2 + 1)
</code></pre>

<p>This scheme works, and offers decent performance.  In fact, it’s good
enough for <a href="https://github.com/facebook/folly/blob/master/folly/experimental/EventCount.h">Facebook’s Folly</a>.<br />
I certainly don’t see how we can improve on that if there are
concurrent writers (incrementing threads).</p>

<p>However, if we go back to the ring buffer example, there is often only
one writer per ring.  Enqueueing an item in a 
<a href="https://github.com/concurrencykit/ck/blob/master/include/ck_ring.h#L110">single-producer ring buffer incurs no atomic, only a <code>release</code> store</a>:
the write pointer increment only has to be visible after the data
write, which is always the case under the TSO memory model (including
x86).  Replacing the write pointer in a single-producer ring buffer
with an event count where each increment incurs an atomic operation is
far from a no-brainer. Can we do better, when there is only one
incrementer?</p>

<p>On x86 (or any of the zero other architectures with non-atomic
read-modify-write instructions and TSO), we can… but we
must accept some weirdness.</p>

<p>The operation that must really be fast is incrementing the event
counter, especially when the sleepers flag is not set.  Setting the
sleepers flag on the other hand, may be slower and use atomic
instructions, since it only happens when the executing thread is
waiting for fresh data.</p>

<p>I suggest that we perform the former, the increment on the fast path,
with a non-atomic read-modify-write instruction, either <code>inc mem</code> or
<code>xadd mem, reg</code>.  If the sleepers flag is in the sign bit, we can
detect it (modulo a false positive on wrap-around) in the condition
codes computed by <code>inc</code>; otherwise, we must use <code>xadd</code> (fetch-and-add)
and look at the flag bit in the fetched value.</p>

<p>The usual ordering-based arguments are no help in this kind of
asymmetric synchronisation pattern.  Instead, we must go directly to
the <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf">x86-TSO</a>
memory model.  All atomic (<code>LOCK</code> prefixed) instructions conceptually
flush the executing core’s store buffer, grab an exclusive lock on
memory, and perform the read-modify-write operation with that lock
held.  Thus, manipulating the sleepers flag can’t lose updates that
are already visible in memory, or on their way from the store buffer.
The RMW increment will also always see the latest version update
(either in global memory, or in the only incrementer’s store buffer),
so won’t lose version updates either.  Finally, scheduling and thread
migration must always guarantee that the incrementer thread sees its
own writes, so that won’t lose version updates.</p>

<pre><code>increment event count without atomics in the common case:

old &lt;- non_atomic_fetch_and_add(event_count.counter, 2)
if (old &amp; 1):
    atomic_and(event_count.counter, -2)
    signal waiters on event_count.counter
</code></pre>

<p>The only thing that might be silently overwritten is the sleepers
flag: a waiter might set that flag in memory just after the
increment’s load from memory, or while the increment reads a value
with the flag unset from the local store buffer.  The question is then
how long waiters must spin before either observing an increment, or
knowing that the flag flip will be observed by the next increment.
That question can’t be answered with the memory model, and worst-case
execution time bounds are a joke on contemporary x86.</p>

<p>I found an answer by remembering that <code>IRET</code>, the instruction used to
return from interrupt handlers, <a href="https://www.felixcloutier.com/x86/IRET:IRETD.html">is a full barrier</a>.<sup id="fnref:model-ooe" role="doc-noteref"><a href="#fn:model-ooe" class="footnote" rel="footnote">1</a></sup>
We also know that interrupts happen at frequent and regular intervals,
if only for the preemption timer (every 4-10ms on stock Linux/x86oid).</p>

<p>Regardless of the bound on store visibility, a waiter can flip the
sleepers-are-present flag, spin on the control word for a while, and
then start sleeping for short amounts of time (e.g., a millisecond or
two at first, then 10 ms, etc.): the spin time is long enough in the
vast majority of cases, but could still, very rarely, be too short.</p>

<p>At some point, we’d like to know for sure that, since we have yet to
observe a silent overwrite of the sleepers flag or any activity on the
counter, the flag will always be observed and it is now safe to sleep
forever. Again, I don’t think x86 offers any strict bound on this sort
of thing.  However, one second seems reasonable.  Even if a core could
stall for that long, interrupts fire on every core several times a
second, and returning from interrupt handlers acts as a full barrier.
No write can remain in the store buffer across interrupts, interrupts
that occur at least once per second.  It seems safe to assume that,
once no activity has been observed on the event count for one second,
the sleepers flag will be visible to the next increment.</p>

<p>That assumption is only safe if interrupts do fire at regular
intervals.  Some latency sensitive systems dedicate cores to specific
userspace threads, and move all interrupt processing and preemption
away from those cores.  A correctly isolated core running
<a href="https://lwn.net/Articles/549580/">Linux in tickless mode</a>, with a
single runnable process, might not process interrupts frequently
enough.  However, this kind of configuration does not happen by accident. I
expect that even a half-second stall in such a system
would be treated as a system error, and hopefully trigger a watchdog.
When we can’t count on interrupts to get us barriers for free, we can
instead rely on practical performance requirements to enforce a
hard bound on execution time.</p>

<p>Either way, waiters set the sleepers flag, but can’t rely on it being
observed until, very conservatively, one second later. Until that time
has passed, waiters spin on the control word, then block for short,
but growing, amounts of time. Finally, if the control word
(event count version and sleepers flag) has not changed in one second,
we assume the incrementer has no write in flight, and will observe the
sleepers flag; it is safe to block on the control word forever.</p>

<pre><code>wait until event count differs from prev:

repeat k times:
    if (event_count.counter / 2) != prev:
        return
compare_and_swap(event_count.counter, 2 * prev, 2 * prev + 1)
if cas_failed and cas_old_value != 2 * prev + 1:
    return
repeat k times:
    if event_count.counter != 2 * prev + 1:
        return
repeat for 1 second:
    sleep_if_until(event_count.center == 2 * prev + 1,
                   $exponential_backoff)
    if event_count.counter != 2 * prev + 1:
        return
sleep_if(event_count.center == prev * 2 + 1)
</code></pre>

<p>That’s the solution I implemented in this pull request for 
<a href="https://github.com/concurrencykit/ck/pull/133">SPMC and MPMC event counts in concurrency kit</a>.
The MP (multiple producer) implementation is the regular adaptive
logic, and matches Folly’s strategy. It needs about 30 cycles for an
uncontended increment with no waiter, and waking up sleepers adds
another 700 cycles on my E5-46xx (Linux 4.16).  The single producer
implementation is identical for the slow path, but only takes ~8
cycles per increment with no waiter, and, eschewing atomic
instruction, does not flush the pipeline (i.e., the out-of-order
execution engine is free to maximise throughput).  The additional
overhead for an increment without waiter, compared to a regular ring
buffer pointer update, is 3-4 cycles for a single predictable conditional branch or fused
<code>test</code> and branch, and the RMW’s load instead of a regular
add/store.  That’s closer to zero overhead, which makes it much easier
for coders to offer OS-assisted blocking in their lock-free
algorithms, without agonising over the penalty when no one needs to
block.</p>

<h1 id="asymmetric-flag-flip-with-interrupts-on-linux">Asymmetric flag flip with interrupts on Linux</h1>

<p><a href="https://ieeexplore.ieee.org/document/1291819">Hazard pointers</a> and
<a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation</a>.
Two different <a href="http://www.cs.utoronto.ca/~tomhart/papers/tomhart_thesis.pdf">memory reclamation</a>
technique, in which the fundamental complexity stems from nearly
identical synchronisation requirements: rarely, a cold code path
(which is allowed to be very slow) writes to memory, and must know
when another, much hotter, code path is guaranteed to observe the slow
path’s last write.</p>

<p>For hazard pointers, the cold code path waits until, having
overwritten an object’s last persistent reference in memory, it is
safe to destroy the pointee.  The hot path is the reader:</p>

<pre><code>1. read pointer value *(T **)x.
2. write pointer value to hazard pointer table
3. check that pointer value *(T **)x has not changed
</code></pre>

<p>Similarly, for epoch reclamation, a read-side section will grab the
current epoch value, mark itself as reading in that epoch, then
confirm that the epoch hasn’t become stale.</p>

<pre><code>1. $epoch &lt;- current epoch
2. publish self as entering a read-side section under $epoch
3. check that $epoch is still current, otherwise retry
</code></pre>

<p>Under a sequentially consistent (SC) memory model, the two sequences
are valid with regular (atomic) loads and stores.  The slow path can
always make its write, then scan every other thread’s single-writer
data to see if any thread has published something that proves it
executed step 2 before the slow path’s store (i.e., by publishing the
old pointer or epoch value).</p>

<p>The diagrams below show all possible interleavings.  In all cases,
once there is no evidence that a thread has failed to observe the slow
path’s new write, we can correctly assume that all threads will
observe the write.  I simplified the diagrams by not interleaving the first
read in step 1: its role is to provide a guess for the value that will
be re-read in step 3, so, at least with respect to correctness, that
initial read might as well be generating random values.  I also kept
the second “scan” step in the slow path abstract.  In practice, it’s a
non-snapshot read of all the epoch or hazard pointer tables for
threads that execute the fast path: the slow path can assume an epoch
or pointer will not be resurrected once the epoch or pointer is absent
from the scan.</p>

<div style="display: grid; grid-template-columns: repeat(3, 1fr); width: 150%; margin-left: -25%;">
<!-- FFSS (1, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaACgDFHZYBKAKA5qV1ANpIAdsGiNUonsAAWHACZJgSAEaoYAWUgYA9rjTde-QSLjhtAd2hTZHcSkmLpAHgC0LzTr0AuaACVISHLQAG5I4ACuMPQAjJx2DjIuAHweumg+AArhytQo0tAE4bi4kCb0AExxElaOru5aaT7w2kLEILgYIWGRDADMnBywZpbWyane0ACqNArAMKERUbGDwzUydePpcATCVtm50pBBFQNC2nPQ2sGQuKYWa9JejNrhQkH2uBTkULhAA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-ffss.png" />
    </a>
    </div>
<!-- FSFS (1, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaACgDFZmBKAKHZqV1ANqQA7YNEaoR3YAAt2AEyTAkAI1QwAspAwB7XGi48+A4XHBaA7tEkz2YlBIVSAPAFpnG7boBc0AEqQkstAAbkjgAK4w9ACMHLb20s4AfO46aN4ACmFK1ChS0ARhuLiQxvQATBywphZWSSle0ACqNPLAMCHhkTGccZYOLm6aqd7wWoLEILgYwaERDADMHOxV5n3SA-VpcARCllk5UpCB5Uso3AQwzgAs7IJabdBaQZC4ouJrUt4AIjiQBG2BOyhEqQFAodhnJAXaDOMq3e4wJ4vEyrKzeRhaMKCQF4CjkKC4IA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-fsfs.png" />
    </a>
    </div>
<!-- FSSF (1, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaACgDFZZGBKAKA5qV1ANpIAdsGiNUonsAAWHACZJgSAEaoYAWUgYA9rjTde-QSLjhtAd2hTZHcSkmLpAHgC0LzTr0AuaACVISHLQAG5I4ACuMPQAjJx2DjIuAHweumg+AArhytQo0tAE4bi4kCb0AEycsGaW1smp3tAAqjQKwDChEVGxHNUWVo6u7lppPrAEwlbZudKQQRWcthIDMkMN6dDw2kLEILgYIWGRDADMiyg8BDAuACwcQtrt0NrBkLim-dY+jNrhQkH2XAUchQXAcC5IK7QFzle6PGAvN5iZZfaAAERwkAI7QBSigQkgKBQQA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-fssf.png" />
    </a>
    </div>
<!-- SSFF (2, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaACllgDFmBKAKA5qV1ANpIAdsGjNUonsAAWHACZJgSAEaoYAWUgYA9rjTde-QSLjhtAd2hTZHcSkmLpAHgC0LzTr0AuaACVISHLQAG5I4ACuMPQAjJwcsGaW1i4AfB66aD4AqjQKwDChEVGx8YlWjq7uWhk+sATCVuHK1CjSkEH0AEycdg4yqene0AAKTS3S0AThuLiQJl09EuUylYOZ0PDaQsQguBghYZEMAMxxQtr50NrBkLhiS9Y+ACI4kAT5QfZhc5AoKEA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-ssff.png" />
    </a>
    </div>
<!-- SFSF (2, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaAClgDFmBKAKHZqV1ANqQA7YNCaoR3YAAt2AEyTAkAI1QwAspAwB7XGi48+A4XHBaA7tEkz2YlBIVSAPAFpnG7boBc0AEqQkstAAbkjgAK4w9ACMHOywphZWzgB87jpo3gCqNPLAMCHhkTE24pYOKWle0AAKYUrUKFLQBGG4uJDG9ABMHPHmZdIubprp3rAEQpZ1DVKQgd0ctvaDrpUZ0PBagsQguBjBoREMAMyxKNwEMM4ALOyCWnnQWkGQuCb9Vt5MWmGCgXa4CjkKC4O4PGDPV6iUqfaAAERwkAIeX+iiggkgKBQQA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-sfsf.png" />
    </a>
    </div>
<!-- SFFS (2, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIGUGFoEMDOBPAthywBOIBjaAM3CQHMTwQAHaAClgDEnYBKAKA5qV1ANpIAdsGhNUonsAAWHACZJgSAEaoYAWUgYA9rjTde-QSLjhtAd2hTZHcSkmLpAHgC0LzTr0AuaACVISHLQAG5I4ACuMPQAjJwcsGaW1i4AfB66aD4AqjQKwDChEVGxthJWjqnp3tAACuHK1CjS0AThuLiQJvQATJx2DjKu7loZPvDaQsQguBghYZEMAMycCRblg25VmXAEwlb1jdKQQT1xKDwEMC4ALBxC2vnQ2sGQuGJl1j4AIjiQBPlBexhTqQFAoO4PGDPV6mNafMTacJCQF4CjkKC4IA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/sc-flip-sffs.png" />
    </a>
    </div>
</div>

<p>No one implements SC in hardware. X86 and SPARC offer the strongest
practical memory model, <a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/">Total Store Ordering</a>, and that’s
still not enough to correctly execute the read-side critical sections
above without special annotations.  Under TSO, reads (e.g., step 3)
are allowed to execute before writes (e.g., step 2).
<a href="https://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf">X86-TSO</a> models
that as a buffer in which stores may be delayed, and that’s what the
scenarios below show, with steps 2 and 3 of the fast path reversed
(the slow path can always be instrumented to recover sequential order,
it’s meant to be slow).  The TSO interleavings only differ from the SC
ones when the fast path’s steps 2 and 3 are separated by something on
slow path’s: when the two steps are adjacent, their order relative to
the slow path’s steps is unaffected by TSO’s delayed stores. TSO is so
strong that we only have to fix one case, <code>FSSF</code>, where the slow path
executes in the middle of the fast path, with the reversal of store
and load order allowed by TSO.</p>

<div style="display: grid; grid-template-columns: repeat(3, 1fr); width: 150%; margin-left: -25%;">
<!-- FFSS (1, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAoAxZ+eAGmgHEB7XgCYBKAFAi6KfKCL0UAO2DRm6RROAALEQJTAUAI3QwAspCy98GcZOmyF0eOF4B3aGs0jlaVTvUAeALT+JmYWAFzQAEqQKALQAG4o4ACuMIwAjKKe3hr+AHyMAKxCweYY4QAKSXq0aOrQREn4+JB2jABMomgSRDD+AGweKq4+AUGmpeEAwrxypCD4WPGJKUwAzKIiDs7DObklYdAAqnTawDAJyakZm44ubqP7ZfZE8q5VNeqQse0bcrxn0F4cUg+Hstx26lCzF4STksS8+ColCg+CAA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-ffss.png" />
    </a>
    </div>
<!-- FSFS (1, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAoAxeVgGmgBkBXIgawwBKAFAi6KfKCL0UAO2DRm6RROAALEQBMUwFACN0MALKQsAe3wZxk6bIXR44cwHdoazSOVpVu9QB4AWkDTCysALmgAJUgULWgANxRwHhhGAEZRb18NQIA+RgAOIVDLDEiABR59WjR1aCIefHxIB0YAJlE0CSIYQIBOLxV3PyCQszLIgGFzOVIQfCxE5NSmAGZRJ1cR3LzSiOgAVTodYBgklLTMsS23DzH98scieXdq2vVIeI7RETlzM7QcwJSD4RzOO5+SLMcw8OTxHz4KiUKD4IA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-fsfs.png" />
    </a>
    </div>
<!-- FSSF (1, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAoAxeeZgGmgCEUATAJQAoIXRT5QReigB2waM3TyxwABZC+KYCgBG6GAFlIWAPb4Mo8ZOlzo8cCYDu0FeqGK0yraoA8AWj8jU3MALmgAJUh+aAA3FHAAVxhGAEZhDy81PwA+VIAmASCzDDCABQSdWjRVaCIE-HxIW0YCoTQxIhg-FLz3JRdvf0DjYrCAYRMZUhB8LFj4pKYAZmF7JwGs7KLQ6ABVOk1gGDjE5LShNedXIe2SuyJZFwqq1Ug+Jla2jq6AFiEZExHaAmGKQfAKfquMIGEBoNBvaCeeJNSBwgCE-0BMBBYLsDiu3mhsPh708+ColCg+DRQA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-fssf.png" />
    </a>
    </div>
<!-- SSFF (2, 1) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAp54AxVgGmgHEB7XgCYBKAFAi6KfKCL0UAO2DRW6RROAALEQJTAUAI3QwAspCy98GcZOmyF0eOF4B3aGs0jlaVTvUAeALT+JmYWAFzQAEqQKALQAG4o4ACuMIwAjKIOzq4+-gB8weYY4QCqdNrAMAnJqRkiWS5uAUGmReHwRPKuSXq0aOqQsYwATKKe3hr5jACsQoVh0AAKPX3q0ERJ+PiQdiOiaBJEMP4AbB4qORrN88XQAMK8cqQg+FjxiSlMAMyiInK8lWgvDikHwSgublCABE8JAiJVYl5EjtIGg0EA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-ssff.png" />
    </a>
    </div>
<!-- SFSF (2, 2) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAp4AxVgGmgBkBXIgawwBKAFAi6KfKCL0UAO2DQW6RROAALEQBMUwFACN0MALKQsAe3wZxk6bIXR44cwHdoazSOVpVu9QB4AWkDTCysALmgAJUgULWgANxRwHhhGAEZRJ1d3P0CAPlDLDEiAVTodYBgklLTMrxVcjQLGAE4hIojoAAUefVo0dWgiHnx8SAdGACZRNAkiGED0gAYGnyaA4M6S6ABhczlSEHwsROTUpgBmLOc3DyCQs2LI+CJ5dz6B9Uh46dEROYoBbQQIAFhEcnMVWg5gSkHwSkaHnCABE8JAiFV4j5khNIGg0CIgA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-sfsf.png" />
    </a>
    </div>
<!-- SFFS (2, 3) -->
    <div>
    <a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAp4AxF+AGmgHEB7XgCYBKAFAi6KfKCL0UAO2DQW6RROAALEQJTAUAI3QwAspCy98GcZOmyF0eOF4B3aGs0jlaVTvUAeALT+JmYWAFzQAEqQKALQAG4o4ACuMIwAjKIOzq4+-gB8weYY4QCqdNrAMAnJqRkeKjka+YwAHEKFYdAACkl6tGjq0ERJ+PiQdowATKJoEkQw-i31Xo1+gR3F0ADCvHKkIPhY8YkpTADMM3MwaSJZLm4BQaZF4fBE8q69-eqQsVOiIlmKHm0H8ADYRHJeJVoLw4pB8EoGm5QgARPCQIiVWJeRLjSBoNCQ6EwOEI+yOe4+UIsXhJOQ4ghUShQfAiIA">
    <img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-sffs.png" />
    </a>
    </div>
</div>

<p>Simple implementations plug this hole with a store-load barrier
between the second and third steps, or implement the store with an
atomic read-modify-write instruction that doubles as a barrier.  Both
modifications are safe and recover SC semantics, but incur a
non-negligible overhead (the barrier forces the out of order execution
engine to flush before accepting more work) which is only necessary a
minority of the time.</p>

<p>The pattern here is similar to the event count, where the slow path
signals the fast path that the latter should do something different.
However, where the slow path for event counts wants to wait forever if
the fast path never makes progress, hazard pointer and epoch
reclamation must detect that case and ignore sleeping threads (that
are not in the middle of a read-side SMR critical section).</p>

<p>In this kind of asymmetric synchronisation pattern, we wish to move as
much of the overhead to the slow (cold) path.  Linux 4.3 gained the
<a href="http://man7.org/linux/man-pages/man2/membarrier.2.html">membarrier</a>
syscall for exactly this use case.  The slow path can execute its
write(s) before making a <code>membarrier</code> syscall.  Once the syscall
returns, any fast path write that has yet to be visible (hasn’t
retired yet), along with every subsequent instruction in program
order, started in a state where the slow path’s writes were visible.
As the next diagram shows, this global barrier lets us rule out the
one anomalous execution possible under TSO, without adding any special
barrier to the fast path.</p>

<center>
<a href="http://sequencediagram.org/index.html#initialData=C4S2BsFMAIBUGUDy0CGBnAngWy5YAnEAY2gDNwUBzM8EAB2gAoAxeeZgGmgCMV9DI+SABMAlAChxdPqCL0UAO2DRm6ZdOAALccJTAUvNDACykLAHt8GKTOLyl0eOHMB3aBu3jVadXs0AeAFpA0wsrAC5oACVIFGFoADcUcABXGEYARglvXy1AgD5MgDZRUMsMSIAFFO5aNE1oIhT+SAdGACYJNGkiGECMoq81dz8gkLNyyIBhcwVSEHwsROS0pgBmCSdXEbz8sojoAFU6XWAYJNT0rPEAXhuAcWdecGgAIT4BfCZSS24QYTQ0GEkAoGBE7hqdU0oju4i2bg8Y32FUcREUENqIHq4I6XR6fXaQA">
<img src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/tso-flip-barrier-fssf.png" />
</a>
</center>

<p>The problem with <code>membarrier</code> is that it comes in two flavours: slow,
or not scalable.  The initial, unexpedited, version waits for kernel
RCU to run its callback, which, on my machine, takes anywhere between
25 and 50 milliseconds.  The reason it’s so slow is that the
condition for an RCU grace period to elapse are more demanding than
a global barrier, and may even require multiple such
barriers.  For example, if we used the same scheme to nest epoch
reclamation ten deep, the outermost reclaimer would be 1024 times
slower than the innermost one.  In reaction to this slowness,
potential users of <code>membarrier</code> went back to triggering
<a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">IPIs</a>, e.g.,
by <a href="https://linux.die.net/man/3/mprotect"><code>mprotect</code>ing</a> a dummy page.
<code>mprotect</code> isn’t guaranteed to act as a barrier, and does not do so on
AArch64, so Linux 4.16 added an “expedited” mode to <code>membarrier</code>.  In
that expedited mode, each membarrier syscall sends an IPI to every
other core…  when I look at machines with hundreds of cores, \(n -
1\) IPI per core, a couple times per second on every \(n\) core,
start to sound like a bad idea.</p>

<p>Let’s go back to the observation we made for event count: any
interrupt acts as a barrier for us, in that any instruction that
retires after the interrupt must observe writes made before the
interrupt.  Once the hazard pointer slow path has overwritten a
pointer, or the epoch slow path advanced the current epoch, we can
simply look at the current time, and wait until an interrupt has been
handled at a later time on all cores.  The slow path can then scan all
the fast path state for evidence that they are still using the
overwritten pointer or the previous epoch: any fast path that has not
published that fact before the interrupt will eventually execute the
second and third steps after the interrupt, and that last step
will notice the slow path’s update.</p>

<p>There’s a lot of information in <code>/proc</code> that lets us conservatively
determine when a new interrupt has been handled on every core.
However, it’s either too granular (<code>/proc/stat</code>) or
extremely slow to generate (<code>/proc/schedstat</code>).  More importantly,
even with <code>ftrace</code>, we can’t easily ask to be woken up when something
interesting happens, and are forced to poll files for updates
(never mind the weirdly hard to productionalise kernel interface).</p>

<p>What we need is a way to read, for each core, the last time it was
definitely processing an interrupt. Ideally, we could also block and
let the OS wake up our waiter on changes to the oldest “last
interrupt” timestamp, across all cores.  On x86, that’s enough to get
us the asymmetric barriers we need for hazard pointers and epoch
reclamation, even if only <code>IRET</code> is serialising, and not
interrupt handler entry.  Once a core’s update to its “last interrupt”
timestamp is visible, any write prior to the update, and thus any
write prior to the interrupt is also globally visible: we can only
observe the timestamp update from a different core than the updater,
in which case TSO saves us, or after the handler has returned with a
serialising IRET.</p>

<p>We can bundle all that logic in a <a href="https://github.com/pkhuong/barrierd/blob/master/signal.ebpf.inc">short eBPF program</a>.<sup id="fnref:ebpf" role="doc-noteref"><a href="#fn:ebpf" class="footnote" rel="footnote">2</a></sup>
The program has a map of thread-local arrays (of 1 CLOCK_MONOTONIC
timestamp each), a map of perf event queues (one per CPU), and an
array of 1 “watermark” timestamp.  Whenever the program runs, it gets
the current time.  That time will go in the thread-local array of
interrupt timestamps.  Before storing a new value in that array, the program
first reads the previous interrupt time: if that time is less than or
equal to the watermark, we should wake up userspace by enqueueing in
event in perf.  The enqueueing is conditional because perf has more
overhead than a thread-local array, and because we want to minimise
spurious wake-ups.  A high signal-to-noise ratio lets userspace set up
the read end of the perf queue to wake up on every event and thus
minimise update latency.</p>

<p>We now need a <a href="https://github.com/pkhuong/barrierd">single global daemon</a>
to attach the eBPF program to an arbitrary set of software tracepoints
triggered by interrupts (or PMU events that trigger interrupts), to
hook the perf fds to epoll, and to re-read the map of interrupt
timestamps whenever epoll detects a new perf event.  That’s what the
rest of the code handles: setting up tracepoints, attaching the eBPF
program, convincing perf to wake us up, and hooking it all up to
<code>epoll</code>.  On my fully loaded 24-core E5-46xx running Linux 4.18 with
security patches, the daemon uses ~1-2% (much less on 4.16) of
a core to read the map of timestamps every time it’s woken up every ~4
milliseconds.  <code>perf</code> shows the non-JITted eBPF program itself uses
~0.1-0.2% of every core.</p>

<p>Amusingly enough, while eBPF offers maps that are safe for concurrent
access in eBPF programs, the same maps come with no guarantee when
accessed from userspace, via the syscall interface.  However, the
implementation uses a hand-rolled long-by-long copy loop, and, <em>on
x86-64</em>, our data all fit in longs.  I’ll hope that the kernel’s
compilation flags (e.g., <code>-ffree-standing</code>) suffice to prevent GCC
from recognising <code>memcpy</code> or <code>memmove</code>, and that we thus get atomic
store and loads on x86-64.  Given the quality of eBPF documentation,
I’ll bet that this implementation accident is actually part of
the API.  Every BPF map is single writer (either per-CPU in the
kernel, or single-threaded in userspace), so this should work.</p>

<p>Once the <code>barrierd</code> daemon is running, any program can <code>mmap</code> its data
file to find out the last time we definitely know each core had
interrupted userspace, without making any further syscall or incurring
any IPI.  We can also use regular synchronisation to let the daemon
wake up threads waiting for interrupts as soon as the oldest interrupt
timestamp is updated.  Applications don’t even need to call
<code>clock_gettime</code> to get the current time: the daemon also works in
terms of a virtual time that it updates in the <code>mmap</code>ed data file.</p>

<p>The <code>barrierd</code> data file also includes an array of per-CPU structs
with each core’s timestamps (both from CLOCK_MONOTONIC and in virtual
time).  A client that knows it will only execute on a subset of CPUs,
e.g., cores 2-6, can compute its own “last interrupt” timestamp by
only looking at entries 2 to 6 in the array.  The daemon even wakes up
any futex waiter on the per-CPU values whenever they change.  The
convenience interface is pessimistic, and assumes that client code
might run on every configured core.  However, anyone can <code>mmap</code> the
same file and implement tighter logic.</p>

<p>Again, there’s a snag with tickless kernels.  In the default
configuration already, a fully idle core might not process timer
interrupts.  The <code>barrierd</code> daemon detects when a core is falling
behind, and starts looking for changes to <code>/proc/stat</code>.  This backup
path is slower and coarser grained, but always works with idle cores.
More generally, the daemon might be running on a system with dedicated
cores.  I thought about causing interrupts by re-affining RT threads,
but that seems counterproductive.  Instead, I think the right approach
is for users of <code>barrierd</code> to treat dedicated cores specially.
Dedicated threads can’t (shouldn’t) be interrupted, so they can
regularly increment a watchdog counter with a serialising instruction.
Waiters will quickly observe a change in the counters for dedicated
threads, and may use <code>barrierd</code> to wait for barriers on preemptively
shared cores.  Maybe dedicated threads should be able to
hook into <code>barrierd</code> and check-in from time to time.  That would break
the isolation between users of <code>barrierd</code>, but threads on dedicated
cores are already in a privileged position.</p>

<p>I quickly compared the barrier latency on an unloaded 4-way E5-46xx
running Linux 4.16, with a sample size of 20000 observations per
method (I had to remove one outlier at 300ms).  The synchronous
methods <code>mprotect</code> (which abuses <code>mprotect</code> to send IPIs by removing
and restoring permissions on a dummy page), or explicit <code>IPI</code> via
expedited membarrier, are much faster than the other (unexpedited
<code>membarrier</code> with kernel RCU, or <code>barrierd</code> that counts interrupts).
We can zoom in on the IPI-based methods, and see that an expedited
membarrier (<code>IPI</code>) is usually slightly faster than <code>mprotect</code>;
<code>IPI</code> via expedited membarrier hits a worst-case of 0.041 ms, versus
0.046 for <code>mprotect</code>.</p>

<!-- # > ggplot(data[data$method == 'IPI' | data$method == 'mprotect', ], aes(x=latency, color=method)) + geom_density() + labs(title="Distribution of IPI-based global barrier latency on an unloaded system", colour="Method") + xlab("Latency (ms)") + ylab("Density") + coord_cartesian(xlim=c(0, 0.02)) -->

<p><a href="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unloaded-global-ipi-large.png">
<img class="center" src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unloaded-global-ipi.png" />
</a></p>

<p>The performance of IPI-based barriers should be roughly independent of
system load. However, we did observe a slowdown for expedited
membarrier (between \(68.4-73.0\%\) of the time, 
\(p &lt; 10\sp{-12}\) according to a binomial test<sup id="fnref:CSM" role="doc-noteref"><a href="#fn:CSM" class="footnote" rel="footnote">3</a></sup>) on the same 
4-way system, when all CPUs were running CPU-intensive code at low
priority.  In this second experiment, we have a sample size of one
million observations for each method, and the worst case for <code>IPI</code> via
expedited membarrier was 0.076 ms (0.041 ms on an unloaded system),
compared to a more stable 0.047 ms for <code>mprotect</code>.</p>

<!-- # > ggplot(loaded[loaded$method == 'IPI' | loaded$method == 'mprotect', ], aes(x=latency, color=method)) + geom_density() + labs(title="Distribution of IPI-based global barrier latency on a loaded system", colour="Method") + xlab("Latency (ms)") + ylab("Density") + coord_cartesian(xlim=c(0, 0.04)) -->

<p><a href="/images/2019-01-09-preemption-is-gc-for-memory-reordering/loaded-global-ipi-large.png">
<img class="center" src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/loaded-global-ipi.png" />
</a></p>

<p>Now for non-IPI methods: they should be slower than methods that
trigger synchronous IPIs, but hopefully have lower overhead and scale
better, while offering usable latencies.</p>

<p>On an unloaded system, the interrupts that drive <code>barrierd</code> are less
frequent, sometimes outright absent, so unexpedited <code>membarrier</code>
achieves faster response times.  We can even observe <code>barrierd</code>’s
fallback logic, which scans <code>/proc/stat</code> for evidence of idle CPUs
after 10 ms of inaction: that’s the spike at 20ms.  The values for
<code>vtime</code> show the additional slowdown we can expect if we wait on
<code>barrierd</code>’s virtual time, rather than directly reading
<code>CLOCK_MONOTONIC</code>.  Overall, the worst case latencies for <code>barrierd</code>
(53.7 ms) and <code>membarrier</code> (39.9 ms) aren’t that different, but I
should add another fallback mechanism based on <code>membarrier</code> to improve
<code>barrierd</code>’s performance on lightly loaded machines.</p>

<!-- # > ggplot(data[data$method != 'IPI' & data$method != 'mprotect' & data$latency < 86, ], aes(x=latency, color=method)) + geom_density() + labs(title="Distribution of preemption-based global barrier latency on an unloaded system", colour="Method") + xlab("Latency (ms)") + ylab("Density") -->

<p><a href="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unloaded-global-preemption-large.png">
<img class="center" src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/unloaded-global-preemption.png" />
</a></p>

<p>When the same 4-way, 24-core, system is under load, interrupts are
fired much more frequently <em>and reliably</em>, so <code>barrierd</code> shines, but
everything has a longer tail, simply because of preemption of the
benchmark process.  Out of the one million observations we have for
each of unexpedited <code>membarrier</code>, <code>barrierd</code>, and <code>barrierd</code> with
virtual time on this loaded system, I eliminated 54 values over
100 ms (18 for <code>membarrier</code>, 29 for <code>barrierd</code>, and 7 for virtual
time).  The rest is shown below.  <code>barrierd</code> is consistently much
faster than <code>membarrier</code>, with a geometric mean speedup of 23.8x.  In
fact, not only can we expect <code>barrierd</code> to finish before an
unexpedited <code>membarrier</code> \(99.99\%\) of the time
(\(p&lt;10\sp{-12}\) according to a binomial test), but we can even expect <code>barrierd</code> to be 10 times as
fast \(98.3-98.5\%\) of the time (\(p&lt;10\sp{-12}\)).  The gap is
so wide that even the opportunistic virtual-time approach is faster
than <code>membarrier</code> (geometric mean of 5.6x), but this time with a mere
three 9s (as fast as membarrier \(99.91-99.96\%\) of the time,
\(p&lt;10\sp{-12}\)).</p>

<!-- # ggplot(loaded[loaded$method != 'IPI' & loaded$method != 'mprotect' & loaded$latency < 100, ], aes(x=latency, color=method)) + geom_density() + labs(title="Distribution of preemption-based global barrier latency on a loaded system", colour="Method") + xlab("Latency (ms)") + ylab("Density") + coord_cartesian(xlim=c(0, 100)) -->

<p><a id="loaded-preemption-latency" href="/images/2019-01-09-preemption-is-gc-for-memory-reordering/loaded-global-preemption-large.png">
<img class="center" src="/images/2019-01-09-preemption-is-gc-for-memory-reordering/loaded-global-preemption.png" />
</a></p>

<p>With <a href="https://github.com/pkhuong/barrierd">barrierd</a>, we get implicit
barriers with worse overhead than unexpedited <code>membarrier</code> (which is
essentially free since it piggybacks on kernel RCU, another sunk
cost), but 1/10th the latency (0-4 ms instead of 25-50 ms).  In
addition, interrupt tracking is per-CPU, not per-thread, so it only
has to happen in a global single-threaded daemon; the rest of
userspace can obtain the information it needs without causing
additional system overhead.  More importantly, threads don’t have to
block if they use <code>barrierd</code> to wait for a system-wide barrier. That’s
useful when, e.g., a thread pool worker is waiting for a reverse
barrier before sleeping on a futex.  When that worker blocks in
<code>membarrier</code> for 25ms or 50ms, there’s a potential hiccup where a work
unit could sit in the worker’s queue for that amount of time before it
gets processed.  With <code>barrierd</code> (or the event count described earlier), the worker can spin and wait for work units to
show up until enough time has passed to sleep on the futex.</p>

<p>While I believe that information about interrupt times should be made
available without tracepoint hacks, I don’t know if a syscall like
<code>membarrier</code> is really preferable to a shared daemon like <code>barrierd</code>.
The one salient downside is that <code>barrierd</code> slows down when some CPUs
are idle; that’s something we can fix by including a <code>membarrier</code>
fallback, or by sacrificing power consumption and forcing kernel
ticks, even for idle cores.</p>

<h1 id="preemption-can-be-an-asset">Preemption can be an asset</h1>

<p>When we write lock-free code in userspace, we always have preemption
in mind.  In fact, the primary reason for lock-free code in userspace
is to ensure consistent latency despite potentially adversarial
scheduling.  We spend so much effort to make our algorithms work
despite interrupts and scheduling that we can fail to see how interrupts
can help us.  Obviously, there’s a cost to making our code
preemption-safe, but preemption isn’t an option.  Much like
garbage collection in managed language, preemption is a feature we
can’t turn off.  Unlike GC, it’s not obvious how to make use of
preemption in lock-free code, but this post shows it’s not impossible.</p>

<p>We can use preemption to get asymmetric barriers, nearly for free,
with <a href="https://github.com/pkhuong/barrierd">a daemon like <code>barrierd</code></a>.  I see a duality between
preemption-driven barriers and techniques like <a href="https://www.cs.tau.ac.il/~mad/publications/asplos2014-ffwsq.pdf">Bounded TSO</a>:
the former are relatively slow, but offer hard bounds, while the
latter guarantee liveness, usually with negligible latency, but
without any time bound.</p>

<p>I used preemption to make single-writer event counts faster
(comparable to a regular non-atomic counter), and to provide a
lower-latency alternative to <code>membarrier</code>’s asymmetric barrier.
In a similar vein,
<a href="https://www2.seas.gwu.edu/~parmer/publications/rtas15speck.pdf">SPeCK</a>
uses time bounds to ensure scalability, at the expense of a bit of
latency, by enforcing periodic TLB reloads instead of relying on
synchronous shootdowns.  What else can we do with interrupts, timer or
otherwise?</p>

<p><small>Thank you Samy, Gabe, and Hanes for discussions on an earlier
draft. Thank you Ruchir for improving this final version.</small></p>

<h1 id="ps-event-count-without-non-atomic-rmw">P.S. event count without non-atomic RMW?</h1>

<p>The single-producer event count specialisation relies on non-atomic
read-modify-write instructions, which are hard to find outside x86.  I
think the flag flip pattern in epoch and hazard pointer reclamation
shows that’s not the only option.</p>

<p>We need two control words, one for the version counter, and another
for the sleepers flag.  The version counter is only written by the
incrementer, with regular non-atomic instructions, while the flag word
is written to by multiple producers, always with atomic instructions.</p>

<p>The challenge is that OS blocking primitives like futex only let us
conditionalise the sleep on a single word.  We could try to pack a
pair of 16-bit <code>short</code>s in a 32-bit <code>int</code>, but that doesn’t give us a
lot of room to avoid wrap-around.  Otherwise, we can guarantee that
the sleepers flag is only cleared immediately before incrementing the
version counter.  That suffices to let sleepers only conditionalise on
the version counter… but we still need to trigger a wake-up if the
sleepers flag was flipped between the last clearing and the increment.</p>

<p>On the increment side, the logic looks like</p>

<pre><code>must_wake = false
if sleepers flag is set:
    must_wake = true
    clear sleepers flag
increment version
if must_wake or sleepers flag is set:
    wake up waiters
</code></pre>

<p>and, on the waiter side, we find</p>

<pre><code>if version has changed
    return
set sleepers flag
sleep if version has not changed
</code></pre>

<p>The separate “sleepers flag” word doubles the space usage, compared
to the single flag bit in the x86 single-producer version.  Composite
OS uses that two-word solution in
<a href="https://github.com/gwsystems/composite/issues/377">blockpoints</a>,
and the advantages seem to be simplicity and additional flexibility in
data layout.  I don’t know that we can implement this scheme more
efficiently in the single producer case, under other memory models
than TSO.  If this two-word solution is only useful for non-x86 TSO,
that’s essentially SPARC, and I’m not sure that platform still
warrants the maintenance burden.</p>

<p>But, we’ll see, maybe we can make the above work on AArch64 or POWER.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:model-ooe" role="doc-endnote">
      <p>I actually prefer another, more intuitive, explanation that isn’t backed by official documentation.The store buffer in x86-TSO doesn’t actually exist in silicon: it represents the instructions waiting to be retired in the out-of-order execution engine. Precise interrupts seem to imply that even entering the interrupt handler flushes the OOE engine’s state, and thus acts as a full barrier that flushes the conceptual store buffer. <a href="#fnref:model-ooe" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ebpf" role="doc-endnote">
      <p>I used raw eBPF instead of the C frontend because that frontend relies on a ridiculous amount of runtime code that parses an ELF file <em>when loading the eBPF snippet</em> to know what eBPF maps to setup and where to backpatch their <code>fd</code> number. I also find there’s little advantage to the C frontend for the scale of eBPF programs (at most 4096 instructions, usually much fewer). I did use <code>clang</code> to generate a starting point, but it’s not that hard to tighten 30 instructions in ways that a compiler can’t without knowing what part of the program’s semantics is essential. The <code>bpf</code> syscall can also <a href="https://github.com/pkhuong/barrierd/blob/00dfded07b1ee86ab171757f2270e71849474d73/setup.c#L294">populate a string buffer with additional information</a> when loading a program. That’s helpful to know that something was assembled wrong, or to understand why the verifier is rejecting your program. <a href="#fnref:ebpf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:CSM" role="doc-endnote">
      <p>I computed these extreme confidence intervals with my <a href="/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method/">old code to test statistical SLOs</a>. <a href="#fnref:CSM" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Restartable sequences with the polysemic null segment selector]]></title>
    <link href="https://www.pvk.ca/Blog/2018/08/25/restartable-sequences-with-the-polysemic-null-segment-selector/"/>
    <updated>2018-08-25T21:57:06-04:00</updated>
    <id>https://www.pvk.ca/Blog/2018/08/25/restartable-sequences-with-the-polysemic-null-segment-selector</id>
    <content type="html"><![CDATA[<p>Implementing <a href="https://en.wikipedia.org/wiki/Non-blocking_algorithm">non-blocking algorithms</a> is one of the few things that are
easier to code in a kernel than in userspace (the only other one I can
think of is physically wrecking a machine). In the kernel, we only
have to worry about designing a protocol that achieves forward
progress even if some threads stop participating. We must guarantee
the absence of conceptual locks (we can’t have 
operations that must be paired, or barriers that everyone must
reach), but are free to implement the protocol by naïvely
spinlocking around individual steps: kernel code can temporarily
<a href="https://elixir.bootlin.com/linux/v4.18/source/kernel/locking/spinlock.c#L61">disable preemption and interrupts</a> to bound a critical section’s execution time.</p>

<p>Getting these non-blocking protocols right is still challenging, but
the challenge is one fundamental for reliable systems. The same
problems, solutions, and space/functionality trade-offs appear in
all distributed systems. Some would even argue that the kind of
interfaces that guarantee lock- or wait- freedom are closer to the
object oriented ideals.</p>

<p>Of course, there is still a place for clever instruction sequences
that avoid internal locks, for code that may be paused anywhere without
freezing the whole system: interrupts can’t always be disabled, read
operations should avoid writing to shared memory if they can,
and a single atomic read-modify-write operation may be faster than
locking. The key point for me is that this complexity is opt-in: we
can choose to tackle it incrementally, as a performance problem rather
than as a prerequisite for correctness.</p>

<p>We don’t have the same luxury in userspace. We can’t start by focusing
on the fundamentals of a non-blocking algorithm, and only implement
interruptable sequences where it makes sense. Userspace can’t disable
preemption, so we must think about the minutiae of interruptable code
sequences from the start; non-blocking algorithms in userspace are
always in hard mode, where every step of the protocol might be paused
at any instruction.</p>

<p>Specifically, the problem with non-blocking code in user space isn’t
that threads or processes can be preempted at any point, but rather
that the preemption can be observed. It’s a
<a href="http://fare.tunes.org/tmp/emergent/pclsr.htm">PCLSRing</a> issue! Even
Unices guarantee programmers won’t observe a thread in the middle of a
syscall: when a thread (process) must be interrupted, any pending
syscall either runs to completion, or returns with an error<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
What we need is a similar guarantee for steps of our own
non-blocking protocols<sup id="fnref:sun" role="doc-noteref"><a href="#fn:sun" class="footnote" rel="footnote">2</a></sup>.</p>

<p>Hardware transactional memory kind of solves the problem
(preemption aborts any pending transaction) but is a bit slow<sup id="fnref:a" role="doc-noteref"><a href="#fn:a" class="footnote" rel="footnote">3</a></sup>, and
needs a fallback mechanism. Other emulation schemes for PCLSRing
userspace code divide the problem in two:</p>

<ol>
  <li>Determining that another thread was preempted in the middle of a
critical section.</li>
  <li>Guaranteeing that that other thread will not blindly resume
executing its critical section (i.e., knowing that the thread knows
that we know it’s been interrupted<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">4</a></sup>).</li>
</ol>

<p>The first part is relatively easy. For per-CPU data, it suffices to
observe that we are running on a given CPU (e.g., core #4), and that
another thread claims to own the same CPU’s (core #4’s) data. For
global locks,
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.6486&amp;rep=rep1&amp;type=pdf">we can instead spin for a while before entering a slow path</a> 
that determines whether the holder has been preempted, by reading
scheduling information in <a href="http://man7.org/linux/man-pages/man5/proc.5.html"><code>/proc</code></a>.</p>

<p>The second part is harder. I have played with schemes that relied on
<a href="http://man7.org/linux/man-pages/man7/signal.7.html">signals</a>, but was never satisfied: I found Linux perf
will rarely, but not never, drop interrupts when I used it to
“profile” context switches, and signaling when we determine that
the holder has been pre-empted has memory visibility issues for
per-CPU data<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">5</a></sup>.</p>

<p>Until earlier this month, the best known solution on mainline Linux
involved <em>cross-modifying code</em>! When a CPU executes a memory write
instruction, that write is affected by the registers, virtual memory
mappings, and the instruction’s bytes. Contemporary operating systems
rarely let us halt and tweak another thread’s general purpose
registers (Linux won’t let us self-<a href="http://man7.org/linux/man-pages/man2/ptrace.2.html">ptrace</a>, nor pause an individual
thread). Virtual memory mappings are per-process, and can’t be
modified from the outside. The only remaining angle is modifying the
premptee’s machine code.</p>

<p>That’s what <a href="https://github.com/facebookexperimental/Rseq">Facebook’s experimental library Rseq (restartable sequences)</a> actually does.</p>

<p>I’m not happy with that solution either: while it “works,” it requires
per-thread clones of each critical section, and makes us deal with
cross-modifying code. I’m not comfortable with leaving code pages
writable, and we also have to guarantee the pre-emptee’s writes are
visible. For me, the only defensible implementation is to modify
the code by mmap-ing pages in place, which incurs an IPI per
modification. The total system overhead thus scales superlinearly with
the number of CPUs.</p>

<p>With Mathieu Desnoyers’s, Paul Turner’s, and Andrew Hunter’s patch to
<a href="https://github.com/torvalds/linux/blob/v4.18/kernel/rseq.c">add an rseq syscall to Linux 4.18</a>, we
finally have a decent answer. Rather than triggering special code when
a thread detects that another thread has been pre-empted in the middle
of a critical section, userspace can associate recovery code with the
address range for each restartable critical section’s
instructions. Whenever the kernel preempts a thread, it detects
whether the interruptee is in such a restartable sequence, and, if so,
redirects the instruction pointer to the associated recovery code.
This essentially means that critical sections must be read-only
except for the last instruction in the section, but that’s not too
hard to satisfy. It also means that we incur recovery even when no one
would have noticed, but the overhead should be marginal (there’s at
most one recovery per timeslice), and we get a simpler programming
model in return.</p>

<p>Earlier this year, I found another way to prevent critical sections
from resuming normal execution after being preempted. It’s a total
hack that exercises a state saving defect in Linux/x86-64, but I’m
comfortable sharing it now that Rseq is in mainline: if anyone needs
the functionality, they can update to 4.18, or backport the feature.</p>

<h2 id="heres-a-riddle">Here’s a riddle!</h2>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>riddle.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="k">const</span> <span class="kt">char</span> <span class="n">values</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="sc">&#39;X&#39;</span><span class="p">,</span> <span class="sc">&#39;Y&#39;</span> <span class="p">};</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kt">char</span>
</span><span class="line"><span class="nf">read_value</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="cm">/*</span>
</span><span class="line"><span class="cm">         * New feature in GCC 6; inline asm would also works.</span>
</span><span class="line"><span class="cm">         * https://gcc.gnu.org/onlinedocs/gcc-6.1.0/gcc/Named-Address-Spaces.html#Named-Address-Spaces</span>
</span><span class="line"><span class="cm">         */</span>
</span><span class="line">        <span class="k">return</span> <span class="o">*</span><span class="p">(</span><span class="k">const</span> <span class="n">__seg_gs</span> <span class="kt">char</span> <span class="o">*</span><span class="p">)(</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">values</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="cm">/* ... */</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">before_switch</span> <span class="o">=</span> <span class="n">read_value</span><span class="p">();</span>  <span class="cm">/* Returns &#39;X&#39;. */</span>
</span><span class="line">        <span class="n">usleep</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">);</span>  <span class="cm">/* Or any other wait for preemption. */</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">after_switch</span> <span class="o">=</span> <span class="n">read_value</span><span class="p">();</span>  <span class="cm">/* Returns &#39;Y&#39;. */</span>
</span><span class="line">        <span class="cm">/* ... */</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With an appropriate setup, the <code>read_value</code> function above will return
a different value once the executing thread is switched out. No, the
kernel isn’t overwriting read-only data while we’re switched out. When
I listed the set of inputs that affect a memory store or load
instruction (general purpose registers, virtual memory mappings, and
the instruction bytes), I left out one last x86 thing: segment registers.</p>

<p>Effective addresses on x86oids are about as feature rich as it gets:
they sum a base address, a shifted index, a constant offset, <em>and,
optionally, a segment base</em>. Today, we simply use segment bases to implement
thread-local storage (each thread’s <code>FS</code> or <code>GS</code> offset points to its
thread-local block), but that usage
repurposes memory segmentation, an old 8086 feature… and x86-64 still
maintains some backward compatibility with its 16-bit ancestor.
There’s a lot of unused complexity there, so it’s plausible that we’ll
find information leaks or otherwise flawed architectural state
switching by poking around segment registers.</p>

<h2 id="how-to-set-that-up">How to set that up</h2>

<p>After learning about this <a href="http://lackingrhoticity.blogspot.com/2018/01/observing-interrupts-from-userland-on-x86.html">trick to observe interrupts from userland</a>,
I decided to do a close reading of Linux’s task switching code on
x86-64 and eventually found <a href="https://elixir.bootlin.com/linux/v4.18/source/arch/x86/kernel/process_64.c#L174">this interesting comment</a><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">6</a></sup>.</p>

<p>Observing a value of <code>0</code> in the <code>FS</code> or <code>GS</code> registers can mean
two things:</p>

<ol>
  <li>Userspace explicitly wrote the null segment selector in there,
and reset the segment base to <code>0</code>.</li>
  <li>The kernel wrote a <code>0</code> in there before setting up the segment base
directly, with <code>WR{FS,GS}BASE</code> or by writing to a model-specific
register (MSR).</li>
</ol>

<p>Hardware has to efficiently keep track of which is actually in
effect. If userspace wrote a <code>0</code> in <code>FS</code> or <code>GS</code>, prefixing an
instruction with that segment has no impact; if the MSR write is
still active (and is non-zero), using that segment must
impact effective address computation.</p>

<p>There’s no easy way to do the same in software. Even in ring 0, the
only sure-fire way to distinguish between the two cases is to actually
read the current segment base value, and that’s slow. Linux instead
fast-paths the common case, where the segment register is 0 because
the kernel is handling segment bases.  It prioritises that use case so
much that the code knowingly sacrifices correctness when userspace
writes <code>0</code> in a segment register after asking the kernel to setup its
segment base directly.</p>

<p>This incorrectness is acceptable because it only affects the thread
that overwrites its segment register, and no one should go through
that sequence of operations. Legacy code can still manipulate segment
descriptor tables and address them in segment registers.  However,
being legacy code, it won’t use the modern syscall that directly
manipulates the segment base. Modern code can let the kernel
set the segment base without playing with descriptor tables, and
has no reason to look at segment registers.</p>

<p>The only way to observe the buggy state saving is to go looking for
it, with something like the code below (which uses <code>GS</code> because <code>FS</code>
is already taken by <code>glibc</code> to implement thread-local storage).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>h4x.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cp">#define RUN_ME </span><span class="cm">/*</span>
</span><span class="line"><span class="cm">gcc-6 -std=gnu99 $0 -o h4x &amp;&amp; ./h4x; exit $?;</span>
</span><span class="line">
</span><span class="line"><span class="cm">Should output</span>
</span><span class="line"><span class="cm">Reads: XXYX</span>
</span><span class="line"><span class="cm">Re-reads: XYX</span>
</span><span class="line"><span class="cm">*/</span><span class="cp"></span>
</span><span class="line"><span class="cp">#define _GNU_SOURCE</span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;asm/prctl.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;assert.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;stdint.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;sys/prctl.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;sys/syscall.h&gt;</span><span class="cp"></span>
</span><span class="line"><span class="cp">#include</span> <span class="cpf">&lt;unistd.h&gt;</span><span class="cp"></span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="k">const</span> <span class="kt">char</span> <span class="n">values</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span> <span class="sc">&#39;X&#39;</span><span class="p">,</span> <span class="sc">&#39;Y&#39;</span> <span class="p">};</span>
</span><span class="line">
</span><span class="line"><span class="cm">/* Directly set GS&#39;s base with a syscall. */</span>
</span><span class="line"><span class="k">static</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">set_gs_base</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">base</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">syscall</span><span class="p">(</span><span class="n">__NR_arch_prctl</span><span class="p">,</span> <span class="n">ARCH_SET_GS</span><span class="p">,</span> <span class="n">base</span><span class="p">);</span>
</span><span class="line">        <span class="n">assert</span><span class="p">(</span><span class="n">ret</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/* Write a 0 in GS. */</span>
</span><span class="line"><span class="k">static</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">set_gs</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">short</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">asm</span> <span class="k">volatile</span><span class="p">(</span><span class="s">&quot;movw %0, %%gs&quot;</span> <span class="o">::</span> <span class="s">&quot;r&quot;</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">:</span> <span class="s">&quot;memory&quot;</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/* Read gs:values[0]. */</span>
</span><span class="line"><span class="k">static</span> <span class="kt">char</span>
</span><span class="line"><span class="nf">read_value</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="cm">/*</span>
</span><span class="line"><span class="cm">         * New feature in GCC 6; inline asm would also works.</span>
</span><span class="line"><span class="cm">         * https://gcc.gnu.org/onlinedocs/gcc-6.1.0/gcc/Named-Address-Spaces.html#Named-Address-Spaces</span>
</span><span class="line"><span class="cm">         */</span>
</span><span class="line">        <span class="k">return</span> <span class="o">*</span><span class="p">(</span><span class="k">const</span> <span class="n">__seg_gs</span> <span class="kt">char</span> <span class="o">*</span><span class="p">)(</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">values</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">**</span><span class="n">argv</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">reads</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">re_reads</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
</span><span class="line">
</span><span class="line">        <span class="n">reads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_value</span><span class="p">();</span>
</span><span class="line">        <span class="n">reads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">set_gs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">read_value</span><span class="p">());</span>
</span><span class="line">        <span class="n">reads</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">set_gs_base</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">read_value</span><span class="p">());</span>
</span><span class="line">        <span class="n">reads</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">set_gs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">read_value</span><span class="p">());</span>
</span><span class="line">
</span><span class="line">        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Reads: %.4s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">reads</span><span class="p">);</span>
</span><span class="line">        <span class="n">fflush</span><span class="p">(</span><span class="nb">NULL</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="n">re_reads</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">read_value</span><span class="p">();</span>
</span><span class="line">        <span class="n">re_reads</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">usleep</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">),</span> <span class="n">read_value</span><span class="p">());</span>
</span><span class="line">        <span class="n">re_reads</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">set_gs</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">read_value</span><span class="p">());</span>
</span><span class="line">
</span><span class="line">        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Re-reads: %.3s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">re_reads</span><span class="p">);</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Running the above on my Linux 4.14/x86-64 machine yields</p>

<pre><code>$ gcc-6 -std=gnu99 h4x.c &amp;&amp; ./a.out
Reads: XXYX
Re-reads: XYX
</code></pre>

<p>The first set of reads shows that:</p>

<ol>
  <li>our program starts with no offset in <code>GS</code> (<code>reads[0] == values[0]</code>)</li>
  <li>explicitly setting <code>GS</code> to 0 does not change that (<code>reads[1] == values[0]</code>)</li>
  <li>changing the <code>GS</code> base to 1 with <code>arch_prctl</code> does work (<code>reads[2] == values[1]</code>)</li>
  <li>resetting the <code>GS</code> selector to 0 resets the base (<code>reads[3] == values[0]</code>).</li>
</ol>

<p>The second set of reads shows that:</p>

<ol>
  <li>the reset base survives short syscalls (<code>re_reads[0] == values[0]</code>)</li>
  <li>an actual context switch reverts the <code>GS</code> base to the <code>arch_prctl</code>
value (<code>re_reads[1] == values[1]</code>)</li>
  <li>writing a 0 in <code>GS</code> resets the base again (<code>re_reads[2] == values[0]</code>).</li>
</ol>

<h2 id="cute-hack-why-is-it-useful">Cute hack, why is it useful?</h2>

<p>The property demonstrated in the hack above is that, after our call to
<code>arch_prctl</code>, we can write a <code>0</code> in <code>GS</code> with a regular instruction to
temporarily reset the <code>GS</code> base to 0, and know it will revert to the
<code>arch_prctl</code> offset again when the thread resumes execution, after
being suspended.</p>

<p>We now have to ensure our restartable sequences are no-ops when the
<code>GS</code> base is reset to the <code>arch_prctl</code> offset, and that the no-op is
detected as such. For example, we could set the <code>arch_prctl</code> offset to
something small, like 4 or 8 bytes, and make sure that any address we
wish to mutate in a critical section is followed by 4 or 8 bytes of
padding that can be detected as such.  If a thread is switched out in
the middle of a critical section, its <code>GS</code> base will be reset to 4 or
8 when the thread resumes execution; we must guarantee that this 
offset will make the critical section’s writes fail.</p>

<p>If a write is a compare-and-swap, we only have to make sure the
padding’s value is unambiguously different from real data: reading the
padding instead of the data will make compare-and-swap fail, and the
old value will tell us that it failed because we read padding, which
should only happen after the section is pre-empted. We can play
similar tricks with fetch-and-add (e.g., real data is always even,
while the padding is odd), or atomic bitwise operations (steal the
sign bit).</p>

<p>If we’re willing to eat a signal after a context switch, we can set
the <code>arch_prctl</code> offset to something very large, and take a
segmentation fault after being re-scheduled.  Another option is to set
the <code>arch_prctl</code> offset to 1, and use a double-wide compare-and-swap
(<code>CMPXCHG16B</code>), or turn on the AC (alignment check) bit in
EFLAGS. After a context switch, our destination address will be
misaligned, which will trigger a <code>SIGBUS</code> that we can handle.</p>

<p>The last two options aren’t great, but, if we make sure to regularly
write a 0 in <code>GS</code>, signals should be triggered rarely, only when
pre-emption happens between the last write to <code>GS</code> and a critical
section. They also have the advantages of avoiding the need for
padding, and making it trivial to detect when a restartable section was
interrupted. Detection is crucial because it often isn’t safe to
assume an operation failed when it succeeded (e.g., unwittingly
succeeding at popping from a memory allocator’s freelist would leak
memory). When a <code>GS</code>-prefixed instruction fails, we must be able to
tell from the instruction’s result, and nothing else. We can’t just
check if the segment base is still what we expect, after the fact: our
thread could have been preempted right after the special <code>GS</code>-prefixed
instruction, before our check.</p>

<p>Once we have restartable sections, we can use them to implement
per-CPU data structures (instead of per-thread), or to let thread
acquire locks and hold them until they are preempted: with
restartable sections that only write if there was no preemption
between the lock acquisition and the final store instruction, we can
<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.6486&amp;rep=rep1&amp;type=pdf">create a revocable lock abstraction</a> and implement <a href="https://www.cl.cam.ac.uk/research/srg/netos/papers/2002-casn.pdf">wait-free coöperation</a> or <a href="http://mcg.cs.tau.ac.il/papers/spaa2010-fc.pdf">flat-combining</a>.</p>

<p>Unfortunately, our restartable sections will always be hard to debug:
observing a thread’s state in a regular debugger like GDB will reset
the <code>GS</code> base and abort the section. That’s not unique to the segment
hack approach.  Hardware transactional memory will abort critical
sections when debugged, and there’s similar behaviour with the
official <code>rseq</code> syscall. It’s hard enough to PCLSR userspace code; it
would be even harder to
PCLSR-except-when-the-interruption-is-for-debugging.</p>

<h2 id="whos-to-blame">Who’s to blame?</h2>

<p>The null <code>GS</code> hack sounds like it only works because of a pile of
questionable design decisions. However, if we look at the historical
context, I’d say everything made sense.</p>

<p>Intel came up with segmentation back when 16 bit pointers were big,
but 64KB of RAM not quite capacious enough.  They didn’t have 32 bit
(never mind 64 bit) addresses in mind, nor threads; they only wanted
to address 1 MB of RAM with their puny registers.  When thread
libraries abused segments to implement thread-local storage, the only
other options were to over-align the stack and hide information there,
or to steal a register. Neither sounds great, especially with x86’s
six-and-a-half general purpose registers.  Finally, when AMD decided
to rip out segmentation, but keep <code>FS</code> and <code>GS</code>, they needed to make
porting x86 code as easy as possible, since that was the whole value
proposition for AMD64 over Itanium.</p>

<p>I guess that’s what systems programming is about. We take our
tools, get comfortable with their imperfections, and use that
knowledge to build new tools by breaking the ones we already have (<a href="https://www.usenix.org/system/files/1311_05-08_mickens.pdf">#Mickens</a>).</p>

<p><small>Thank you Andrew for a fun conversation that showed the segment
hack might be of interest to someone else, and to Gabe for snarkily
reminding us <code>Rseq</code> is another Linux/Silicon Valley
re-invention.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>That’s not as nice as rewinding the PC to just before the syscall, with a fixed up state that will resume the operation, but is simpler to implement, and usually good enough. Classic worst is better (Unix semantics are also safer with concurrency, but that could have been opt-in…). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:sun" role="doc-endnote">
      <p>That’s not a new observation, and SUN heads like to point to prior art like <a href="https://pdfs.semanticscholar.org/5c9e/780fb6e6890d853fb5e44d1b7ce51a68a900.pdf">Dice’s and Garthwaite’s Mostly Lock-Free Malloc</a>, <a href="https://www.usenix.org/legacy/events/vee05/full_papers/p24-garthwaite.pdf">Garthwaite’s, Dice’s, and White’s work on Preemption notification for per-CPU buffers</a>, or <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.219.6486&amp;rep=rep1&amp;type=pdf">Harris’s and Fraser’s Revocable locks</a>. Linux sometimes has to reinvent everything with its special flavour. <a href="#fnref:sun" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:a" role="doc-endnote">
      <p>For instance, <a href="http://supertech.csail.mit.edu/papers/Kuszmaul15.pdf">SuperMalloc optimistically uses TSX to access per-CPU caches</a>, but TSX is slow enough that SuperMalloc first tries to use a per-thread cache. <a href="https://timharris.uk/papers/2016-lhp.pdf">Dice and Harris</a> explored the use of hardware transactional lock elision solely to abort on context switches; they maintained high system throughput under contention by trying the transaction once before falling back to a regular lock. <a href="#fnref:a" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I did not expect systems programming to get near multi-agent epistemic logic ;) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Which is fixable with LOCKed instructions, but that defeats the purpose of per-CPU data. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>I actually found the logic bug before the Spectre/Meltdown fire drill and was worried the hole would be plugged. This one survived the purge. <em>fingers crossed</em> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Relaxed revocable locks: mutual exclusion modulo preemption]]></title>
    <link href="https://www.pvk.ca/Blog/2017/06/05/relaxed-revocable-locks-mutual-exclusion-modulo-preemption/"/>
    <updated>2017-06-05T00:25:59-04:00</updated>
    <id>https://www.pvk.ca/Blog/2017/06/05/relaxed-revocable-locks-mutual-exclusion-modulo-preemption</id>
    <content type="html"><![CDATA[<p><em>Update: there’s a way to detect “running” status even across cores.
 It’s not pretty.  Search for <code>/proc/sched_debug</code>.</em></p>

<p>The hard part about locking tends not to be the locking itself, but
preemption.  For example, if you structure a memory allocator
like jemalloc, you want as few arenas as possible; one per CPU would
be ideal, while one per thread would affect fragmentation and make
some operations scale linearly with the number of threads.  However,
you don’t want to get stuck when a thread is preempted while it owns
an arena.  The usual fix is two-pronged:</p>

<ol>
  <li>have a few arenas per CPU (e.g., jemalloc defaults to 4x the
number of CPUs);</li>
  <li>hold exclusive ownership for short critical sections.</li>
</ol>

<p>The first tweak isn’t that bad; scaling the number of arenas, stats
regions, etc. with the number of CPUs is better than scaling with the
number of threads.  The second one really hurts performance: <em>each</em>
allocation must acquire a lock with an interlocked write.  Even if the
arena is (mostly) CPU-local, the atomic wrecks your pipeline.</p>

<p>It would be nice to have locks that a thread can acquire once per
scheduling quantum, and benefit from ownership until the thread is
scheduled out.  We could then have a few arenas per CPU (if only to
handle migration), but amortise lock acquisition over the timeslice.</p>

<p>That’s not a new idea.  <a href="http://dl.acm.org/citation.cfm?id=512451">Dice and Garthwaite</a> described this
exact application in 2002
<a href="https://pdfs.semanticscholar.org/5c9e/780fb6e6890d853fb5e44d1b7ce51a68a900.pdf">(PDF)</a>
and refer to older work for uniprocessors.  However, I think the best
exposition of the idea is Harris and Fraser’s
<a href="http://dl.acm.org/citation.cfm?id=1065954">Revocable locks for non-blocking programming</a>,
published in 2005 <a href="https://pdfs.semanticscholar.org/21eb/486b4ef3b059d782ea1976b4ea5985e417df.pdf">(PDF)</a>.  Harris and Fraser want revocable locks for
non-blocking multiwriter code; our problem is easier, but only marginally so.
Although the history of revocable locks is pretty Solaris-centric, Linux
is catching up.   Google, Facebook, and EfficiOS (LTTng) have been pushing for
<a href="https://lwn.net/Articles/697979/">restartable sequences</a>, which is essentially
OS support for sections that are revoked on context switches.  Facebook even
has a pure userspace implementation with
<a href="https://github.com/facebookexperimental/Rseq">Rseq</a>; they report good
results for jemalloc.</p>

<p>Facebook’s Rseq implements almost exactly what I described above, for
the exact same reason (speeding up a memory allocator or replacing
miscellaneous per-thread structs with ~per-CPU data).  However,
they’re trying to port a kernel idiom directly to userspace:
restartable sequences implement strict per-CPU data.  With kernel
supports, that makes sense.  Without such support though, strict
per-CPU data incurs a lot of extra complexity when a thread migrates
to a new CPU: Rseq needs an asymmetric fence to ensure that the
evicted thread observes its eviction and publishes any write it
performed before being evicted.</p>

<p>I’m not sure that’s the best fit for userspace.  We can avoid a lot of
complexity by instead dynamically allocating a few arenas (exclusive
data) per CPU and assuming only a few threads at a time will be
migrated <em>while owning arenas</em>.</p>

<p>Here’s the relaxed revocable locks interface I propose:</p>

<ol>
  <li>Each thread has a thread state struct.  That state struct has:
    <ul>
      <li>a generation counter;</li>
      <li>a canceled counter (generation - 1 or equal to generation);</li>
      <li>a signaled counter (generation - 1 or equal to generation);</li>
      <li>an acknowledged cancel counter (generation - 1 or equal to generation);</li>
      <li>an “in critical section” flag (pointer to a revocable lock).</li>
    </ul>
  </li>
  <li>
    <p>Locks are owned by a pair of thread state struct and generation
counter (ideally packed in one word, but two words are doable).
Threads acquire locks with normal compare-and-swap, but may bulk
revoke every lock they own by advancing their generation counter.</p>
  </li>
  <li>
    <p>Threads may execute any number of conditional stores per lock
acquisition.  Lock acquisition returns an ownership descriptor
(pair of thread state struct and generation counter), and
<code>rlock_store_64(descriptor, lock, dst, value)</code> stores <code>value</code> in
<code>dst</code> if the descriptor still owns the lock and the ownership has
not been cancelled.</p>
  </li>
  <li>Threads do not have to release lock ownership to let others make
progress: any thread may attempt to cancel another thread’s
ownership of a lock.  After <code>rlock_owner_cancel(descriptor, lock)</code>
returns successfully, the victim will not execute a conditional
store under the notion that it still owns <code>lock</code> with <code>descriptor</code>.</li>
</ol>

<p>The only difference from Rseq is that <code>rlock_owner_cancel</code> may fail.
In practice, it will only fail if a thread on CPU A attempts to cancel
ownership for a thread that’s currently running on another CPU B.
That could happen after migration, but also when an administrative
task iterates through every (pseudo-)per-CPU struct without changing
its CPU mask.  Being able to iterate through all available
pseudo-per-CPU data without migrating to the CPU is big win for slow
paths; another advantage of not assuming strict per-CPU affinity.</p>

<p>Rather than failing on migration, Rseq issues an asymmetric fence to
ensure both its writes and the victim’s writes are visible.  At best,
that’s implemented with inter-processor interrupts (IPIs) that scale
linearly with the number of CPUs… for a point-to-point signal.  I
oversubscribed a server with 2-4x more threads than CPUs, and thread
migrations happened at a constant frequency per CPU.  Incurring
<code>O(#CPU)</code> IPIs for every migration makes the <em>per-CPU</em> overhead of
Rseq linear with the number of CPUs (cores) in the system.  I’m also
wary of the high rate of code self/cross -modification in Rseq:
<code>mprotect</code> incurs IPIs when downgrading permissions, so Rseq must
leave some code page with writes enabled.  These downsides (potential
for IPI storm and lack of W\^X) aren’t unique to Rseq.  I think
they’re inherent to emulating unpreempted per-CPU data in userspace
without explicit OS support.</p>

<p>When <code>rlock_owner_cancel</code> fails, I expect callers to iterate down the
<em>list</em> of pseudo-per-CPU structs associated with the CPU and eventually
append a new struct to that list.  In theory, we could end up with as
many structs in that list as the peak number of thread on that CPU; in
practice, it should be a small constant since <code>rlock_owner_cancel</code>
only fails after thread migration.</p>

<h2 id="code-for-rlock-linuxx86-64-only">Code for Rlock (Linux/x86-64 only)</h2>

<p>I <a href="https://gist.github.com/pkhuong/a622e031e92f7fdfb1df1b49a7627d54">dumped my code as a gist</a>, but it is definitely hard to follow, so I’ll try to
explain it here.</p>

<p>Bitpacked ownership records must include the address of the owner
struct and a sequence counter.  Ideally, we’d preallocate some address
space and only need 20-30 bits to encode the address.  For now, I’m
sticking to 64 byte aligned allocations and rely on x86-64’s 48 bits
of address space.  With 64 bit owner/sequence records, an <code>rlock</code>
is a 64 bit spinlock.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">typedef</span> <span class="k">union</span> <span class="n">rlock_owner_seq</span> <span class="p">{</span>
</span><span class="line">        <span class="kt">uint64_t</span> <span class="n">bits</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">uint64_t</span> <span class="nl">sequence</span><span class="p">:</span><span class="mi">22</span><span class="p">;</span>
</span><span class="line">                <span class="kt">uint64_t</span> <span class="nl">address</span><span class="p">:</span><span class="mi">42</span><span class="p">;</span>
</span><span class="line">        <span class="p">};</span>
</span><span class="line"><span class="p">}</span> <span class="n">rlock_owner_seq_t</span><span class="p">;</span>
</span><span class="line">
</span><span class="line"><span class="k">struct</span> <span class="n">rlock</span> <span class="p">{</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">owner</span><span class="p">;</span>
</span><span class="line"><span class="p">};</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>In the easy case, acquiring an <code>rlock</code> means:</p>

<ol>
  <li>reading the <code>owner</code> field (with a 64 bit load);</li>
  <li>confirming that the owner has advanced its sequence;</li>
  <li>CASing in our own <code>rlock_owner_seq_t</code>.</li>
</ol>

<p>But first, we must make canonicalise our own <code>owner</code> struct.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">struct</span> <span class="n">rlock_owner</span> <span class="p">{</span>
</span><span class="line">        <span class="cm">/* SPMC. */</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">seq</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* MPMC: Asked to cancel up to here (inclusively). */</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">cancel_sequence</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* MPMC: Signaled to cancel up to here (inclusively). */</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">signal_sequence</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* SPMC: Acked cancel ask up to here (inclusively). */</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">acked_sequence</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* Private: forcibly release lock after too many ops. */</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">op_count</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* SPMC */</span>
</span><span class="line">        <span class="kt">pid_t</span> <span class="n">tid</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* SPMC; &quot;in critical section&quot; flag. */</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock</span> <span class="o">*</span><span class="n">critical_section</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span> <span class="n">__attribute__</span><span class="p">((</span><span class="n">__aligned__</span><span class="p">(</span><span class="mi">64</span><span class="p">)));</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Rlock lazily allocates an <code>rlock_owner</code> per thread and stores it in
TLS; we can’t free that memory without some safe memory reclamation
scheme (and I’d like to use Rlock to implement SMR), but it is
possible to use a type-stable freelist.</p>

<p>Regardless of the allocation/reuse strategy, canonicalising an rlock
means making sure we observe any cancellation request.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">static</span> <span class="kr">inline</span> <span class="kt">bool</span>
</span><span class="line"><span class="nf">update_self</span><span class="p">(</span><span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">self</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">snapshot</span> <span class="o">=</span> <span class="p">{</span> <span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span> <span class="p">};</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">cancel_sequence</span> <span class="o">=</span> <span class="n">ck_pr_load_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">cancel_sequence</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* We&#39;ve been asked to cancel if cancel_sequence == seq.sequence. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">LIKELY</span><span class="p">(</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">sequence</span> <span class="o">!=</span> <span class="n">cancel_sequence</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">ck_pr_fas_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">cancel_sequence</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">.</span><span class="n">sequence</span><span class="p">);</span>
</span><span class="line">        <span class="n">ck_pr_fas_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">signal_sequence</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">.</span><span class="n">sequence</span><span class="p">);</span>
</span><span class="line">        <span class="n">ck_pr_fas_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">acked_sequence</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">.</span><span class="n">sequence</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="n">snapshot</span><span class="p">.</span><span class="n">sequence</span><span class="o">++</span><span class="p">;</span>
</span><span class="line">        <span class="n">ck_pr_fas_64</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span><span class="p">);</span>
</span><span class="line">        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kr">inline</span> <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span>
</span><span class="line"><span class="nf">get_self</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">self</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">self</span> <span class="o">=</span> <span class="n">rlock_self</span><span class="p">;</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">UNLIKELY</span><span class="p">(</span><span class="n">self</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="n">self</span> <span class="o">=</span> <span class="n">allocate_self</span><span class="p">();</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">update_self</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
</span><span class="line">        <span class="k">return</span> <span class="n">self</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To acquire a lock we observe the current owner, attempt to cancel its
ownership, and (if we did cancel ownership) CAS in our own
owner/sequence descriptor.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="n">rlock_owner_seq_t</span>
</span><span class="line"><span class="nf">rlock_lock</span><span class="p">(</span><span class="k">struct</span> <span class="n">rlock</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">self</span> <span class="o">=</span> <span class="n">get_self</span><span class="p">();</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">seq</span><span class="p">,</span> <span class="n">snapshot</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Load the current owner. */</span>
</span><span class="line">        <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">ck_pr_load_64</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">owner</span><span class="p">.</span><span class="n">bits</span><span class="p">);</span>
</span><span class="line">        <span class="cm">/* Easy case: we already own the lock. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">==</span> <span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(;;)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">seq</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Make sure the current owner isn&#39;t anymore. */</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">rlock_owner_cancel</span><span class="p">(</span><span class="n">snapshot</span><span class="p">,</span> <span class="n">lock</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                        <span class="cm">/* Couldn&#39;t; return 0. */</span>
</span><span class="line">                        <span class="n">seq</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">                        <span class="k">return</span> <span class="n">seq</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Replace the old owner with ourself. */</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">ck_pr_cas_64_value</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">owner</span><span class="p">.</span><span class="n">bits</span><span class="p">,</span>
</span><span class="line">                    <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span><span class="p">,</span> <span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                        <span class="cm">/* Success! */</span>
</span><span class="line">                        <span class="k">break</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* CAS failed.  snapshot.bits has the new owner. */</span>
</span><span class="line">                <span class="cm">/* Eagerly observe any cancellation. */</span>
</span><span class="line">                <span class="n">update_self</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
</span><span class="line">                <span class="cm">/* CAS failed. Spin a bit. */</span>
</span><span class="line">                <span class="n">ck_pr_stall</span><span class="p">();</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="n">seq</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Most of the trickiness hides in <code>rlock_owner_cancel</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="kt">bool</span>
</span><span class="line"><span class="nf">rlock_owner_cancel</span><span class="p">(</span><span class="k">union</span> <span class="n">rlock_owner_seq</span> <span class="n">owner</span><span class="p">,</span>
</span><span class="line">    <span class="k">struct</span> <span class="n">rlock</span> <span class="o">*</span><span class="n">evict</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">victim</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)((</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">owner</span><span class="p">.</span><span class="n">address</span> <span class="o">*</span> <span class="mi">64</span><span class="p">);</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">snapshot</span><span class="p">;</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">acked</span><span class="p">;</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">sequence</span> <span class="o">=</span> <span class="n">owner</span><span class="p">.</span><span class="n">sequence</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">assert</span><span class="p">(</span><span class="n">evict</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>
</span><span class="line">        <span class="cm">/* Easy case: no owner. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">victim</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">ck_pr_load_64</span><span class="p">(</span><span class="o">&amp;</span><span class="n">victim</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">);</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">!=</span> <span class="n">owner</span><span class="p">.</span><span class="n">bits</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="cm">/* The victim has already moved on to a new sequence value. */</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">acked</span> <span class="o">=</span> <span class="n">ck_pr_load_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">victim</span><span class="o">-&gt;</span><span class="n">acked_sequence</span><span class="p">);</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">mod_lte</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">acked</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="cm">/* We already have acked cancellation &gt;= sequence. */</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Advance the victim&#39;s cancel counter to sequence. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">ensure_cancel_sequence</span><span class="p">(</span><span class="n">victim</span><span class="p">,</span> <span class="n">sequence</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="cm">/* Already advanced; nothing to do! */</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">victim_running</span><span class="p">(</span><span class="n">victim</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="cm">/* The victim isn&#39;t obviously scheduled out;</span>
</span><span class="line">
</span><span class="line"><span class="cm">                /* See if we must ensure visibility of our cancel. */</span>
</span><span class="line">                <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">ck_pr_load_64</span><span class="p">(</span><span class="o">&amp;</span><span class="n">victim</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">);</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span> <span class="o">==</span> <span class="n">owner</span><span class="p">.</span><span class="n">bits</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="n">ensure_signal_sequence</span><span class="p">(</span><span class="n">victim</span><span class="p">,</span> <span class="n">sequence</span><span class="p">);</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">ck_pr_load_ptr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">victim</span><span class="o">-&gt;</span><span class="n">critical_section</span><span class="p">)</span> <span class="o">!=</span> <span class="n">evict</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="cm">/*</span>
</span><span class="line"><span class="cm">                 * Easy case: victim isn&#39;t in a critical section with</span>
</span><span class="line"><span class="cm">                 * our lock.  The victim has either been scheduled out</span>
</span><span class="line"><span class="cm">                 * since we called `ensure_cancel_sequence`, our went</span>
</span><span class="line"><span class="cm">                 * through a context switch at least once.  In either</span>
</span><span class="line"><span class="cm">                 * case, it has already observed the cancellation or</span>
</span><span class="line"><span class="cm">                 * will before the next critical section.</span>
</span><span class="line"><span class="cm">                 */</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/*</span>
</span><span class="line"><span class="cm">         * The victim might be in the middle of a critical section.</span>
</span><span class="line"><span class="cm">         * Send a signal that&#39;ll skip the critical section if</span>
</span><span class="line"><span class="cm">         * necessary.</span>
</span><span class="line"><span class="cm">         */</span>
</span><span class="line">        <span class="n">ensure_signal_sequence</span><span class="p">(</span><span class="n">victim</span><span class="p">,</span> <span class="n">sequence</span><span class="p">);</span>
</span><span class="line">        <span class="cm">/*</span>
</span><span class="line"><span class="cm">         * If the victim is definitely not running, it either has</span>
</span><span class="line"><span class="cm">         * already executed the signal handler or will before resuming</span>
</span><span class="line"><span class="cm">         * normal execution.  If the victim might be running,</span>
</span><span class="line"><span class="cm">         * we can only hope we got lucky.</span>
</span><span class="line"><span class="cm">         */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">victim_running</span><span class="p">(</span><span class="n">victim</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/*</span>
</span><span class="line"><span class="cm">         * We know the vitim was scheduled out before we signaled for</span>
</span><span class="line"><span class="cm">         * cancellation.  We can see if the victim has released our</span>
</span><span class="line"><span class="cm">         * critical section at least once since then.</span>
</span><span class="line"><span class="cm">         */</span>
</span><span class="line">        <span class="k">return</span> <span class="p">(</span><span class="n">ck_pr_load_ptr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">victim</span><span class="o">-&gt;</span><span class="n">critical_section</span><span class="p">)</span> <span class="o">!=</span> <span class="n">evict</span><span class="p">);</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The fancy stuff begins around <code>ensure_cancel_sequence(victim, sequence);</code>.
Our code maintains the invariant that the MPMC sequences
(<code>cancel_sequence</code>, <code>signal_sequence</code>) are either the SPMC <code>sequence - 1</code>
 (normal state), or exactly the SPMC sequence (cancellation
request).</p>

<p><code>ensure_cancel_sequence</code> CASes the <code>cancel_sequence</code> field from its
expected value of <code>owner.sequence - 1</code> to <code>owner.sequence</code>.  If
the actual value is neither of them, the owner has already
advanced to a new sequence value, and we’re done.</p>

<p>Otherwise, we have to hope the victim isn’t running.</p>

<p>Now comes the really tricky stuff.  Our CAS is immediately visible
globally.  The issue is that the victim might already be in the middle
of a critical section.  When writers executes a critical sections, they:</p>

<ol>
  <li>Set the critical section flag (with a normal write);</li>
  <li>Check that the lock hasn’t been revoked;</li>
  <li>Perform the write;</li>
  <li>Clear the critical section flag.</li>
</ol>

<p>It’s really hard to guarantee that the write in step 1 is visible
(without killing performance in the common case), and if it is, that
the victim isn’t about to execute step 3.</p>

<p>We get that guarantee by determining that the victim hasn’t been
continuously executing since the time we attempted to CAS the
<code>cancel_sequence</code> forward.  That’s (hopefully) enough of a barrier to
order the CAS, step 1, and our read of the critical section flag.</p>

<p>That’s not information that Linux exposes directly.  However, we can
borrow a trick from <code>Rseq</code> and read <code>/proc/self/task/[tid]/stat</code>.  The
contents of that file include whether the task is (R)unnable (or
(S)leeping, waiting for (D)isk, etc.), and the CPU on which the task
last executed.</p>

<p>If the task isn’t runnable, it definitely hasn’t been running
continuously since the CAS.  If the task is runnable but last ran on
the CPU the current thread is itself running on (and the current
thread wasn’t migrated in the middle of reading the stat file), it’s
not running now.</p>

<p>If the task is runnable on another CPU, we can try to look at
<code>/proc/sched_debug</code>: each CPU has a <code>.curr-&gt;pid</code> line that tells us
the PID of the task that’s currently running (0 for none).  That file
has a lot of extra information so reading it is <em>really</em> slow, but we
only need to do that after migrations.</p>

<p>Finally, the victim might really be running.  Other proposals would
fire an IPI; we instead ask the caller to allocate a few more
pseudo-per-CPU structs.</p>

<p>Assuming we did get a barrier out of the scheduler, we hopefully
observe that the victim’s critical section flag is clear.  If that
happens, we had:</p>

<ol>
  <li>CAS the cancellation sequence;</li>
  <li>Barrier <em>in the victim</em> from being scheduled out;</li>
  <li>Critical section flag was empty after the CAS.</li>
</ol>

<p>This guarantees that the victim hasn’t been in the same critical
section since the CAS in step 1.  Either it’s not in a critical
section, or if it is, it’s a fresh one that will observe the CAS.
It’s safe to assume the victim has been successfully evicted.</p>

<p>The less happy path happens when we observe that the victim’s critical
section flag is set.  We must assume that it was scheduled out in
the middle of a critical section.  We’ll send a (POSIX) signal to the
victim: the handler will skip over the critical section if the victim
is still in one.  Once that signal is sent, we know that the first
thing Linux will do is execute the handler when the victim resumes
execution.  If the victim is still not running after <code>tgkill</code>
returned, we’re good to go: if the victim is still in the critical
section, the handler will fire when it resumes execution.</p>

<p>Otherwise, the victim might have been scheduled in between the CAS and
the signal; we still have the implicit barrier given by the context
switch between CAS and signal, but we can’t rely on signal execution.
We can only hope to observe that the victim has noticed the
cancellation request and advanced its sequence, or that it cleared its
critical section flag.</p>

<p>The rest is straightforward.  The <code>rlock_store_64</code> must observe any
cancellation, ensure that it still holds the lock, and enter the
critical section:</p>

<ol>
  <li>set the critical section flag (overwrite with the lock’s address);</li>
  <li>check again that we still hold the lock and have not been asked to cancel;</li>
  <li>flip the result flag to “success”;</li>
  <li>store.</li>
</ol>

<p>Once it leaves the critical section, <code>rlock_store_64</code> clears the
critical section flags, looks for any cancellation request, and
returns success/failure.  The critical section is in inline assembly
for the signal handler: executing the store in step 4 implicitly
marks the end of the critical section.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="kt">bool</span>
</span><span class="line"><span class="nf">rlock_store_64</span><span class="p">(</span><span class="n">rlock_owner_seq_t</span> <span class="n">snapshot</span><span class="p">,</span>
</span><span class="line">    <span class="k">struct</span> <span class="n">rlock</span> <span class="o">*</span><span class="n">lock</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="o">*</span><span class="n">dst</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">value</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">self</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)((</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">snapshot</span><span class="p">.</span><span class="n">address</span> <span class="o">*</span> <span class="mi">64</span><span class="p">);</span>
</span><span class="line">        <span class="n">rlock_owner_seq_t</span> <span class="n">seq</span><span class="p">;</span>
</span><span class="line">        <span class="kt">uint32_t</span> <span class="n">op_count</span><span class="p">;</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">status</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">seq</span><span class="p">.</span><span class="n">bits</span> <span class="o">=</span> <span class="n">self</span><span class="o">-&gt;</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span><span class="p">;</span>
</span><span class="line">        <span class="n">op_count</span> <span class="o">=</span> <span class="o">++</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">op_count</span><span class="p">;</span>
</span><span class="line">        <span class="cm">/* We cancelled this lock. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">UNLIKELY</span><span class="p">(</span><span class="n">seq</span><span class="p">.</span><span class="n">bits</span> <span class="o">!=</span> <span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* The handler will reset RAX to 1 on skip. */</span>
</span><span class="line">        <span class="n">status</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="k">asm</span> <span class="k">volatile</span><span class="p">(</span>
</span><span class="line">            <span class="cm">/* Move the lock&#39;s address in the critical section flag. */</span>
</span><span class="line">            <span class="s">&quot;0: movq %[lock], %[critical_section]</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="cm">/* Do we still own the lock? */</span>
</span><span class="line">            <span class="s">&quot;cmpq %[owner], %[snapshot]</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="s">&quot;jne 1f</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="cm">/* Were we asked to cancel? */</span>
</span><span class="line">            <span class="s">&quot;cmpl %[cancelled], %[seq]</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="s">&quot;je 1f</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="cm">/* Success path! Set status to 0. */</span>
</span><span class="line">            <span class="s">&quot;xorl %[status], %[status]</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="cm">/* Store the value in *dst. */</span>
</span><span class="line">            <span class="s">&quot;movq %[value], %[dst]</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="cm">/* End of critical section. */</span>
</span><span class="line">            <span class="s">&quot;1:</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">
</span><span class="line">            <span class="cm">/*</span>
</span><span class="line"><span class="cm">             * Make sure the signal handler knows where the</span>
</span><span class="line"><span class="cm">             * critical section code begins &amp; ends.</span>
</span><span class="line"><span class="cm">             */</span>
</span><span class="line">            <span class="s">&quot;.pushsection rlock_store_list, </span><span class="se">\&quot;</span><span class="s">a</span><span class="se">\&quot;</span><span class="s">, @progbits</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="s">&quot;.quad 0b, 1b</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">            <span class="s">&quot;.popsection</span><span class="se">\n\t</span><span class="s">&quot;</span>
</span><span class="line">                <span class="o">:</span> <span class="p">[</span><span class="n">status</span><span class="p">]</span> <span class="s">&quot;+a&quot;</span><span class="p">(</span><span class="n">status</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">critical_section</span><span class="p">]</span> <span class="s">&quot;+m&quot;</span><span class="p">(</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">critical_section</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">dst</span><span class="p">]</span> <span class="s">&quot;=m&quot;</span><span class="p">(</span><span class="o">*</span><span class="n">dst</span><span class="p">)</span>
</span><span class="line">                <span class="o">:</span> <span class="p">[</span><span class="n">lock</span><span class="p">]</span> <span class="s">&quot;r&quot;</span><span class="p">(</span><span class="n">lock</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">snapshot</span><span class="p">]</span> <span class="s">&quot;r&quot;</span><span class="p">(</span><span class="n">snapshot</span><span class="p">.</span><span class="n">bits</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">owner</span><span class="p">]</span> <span class="s">&quot;m&quot;</span><span class="p">(</span><span class="n">lock</span><span class="o">-&gt;</span><span class="n">owner</span><span class="p">.</span><span class="n">bits</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">seq</span><span class="p">]</span> <span class="s">&quot;r&quot;</span><span class="p">((</span><span class="kt">uint32_t</span><span class="p">)</span><span class="n">seq</span><span class="p">.</span><span class="n">sequence</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">cancelled</span><span class="p">]</span> <span class="s">&quot;m&quot;</span><span class="p">(</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">cancel_sequence</span><span class="p">),</span>
</span><span class="line">                  <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="s">&quot;r&quot;</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</span><span class="line">                <span class="o">:</span> <span class="s">&quot;memory&quot;</span><span class="p">,</span> <span class="s">&quot;cc&quot;</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Clear the flag. */</span>
</span><span class="line">        <span class="n">ck_pr_store_ptr</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">critical_section</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Acknowledge any cancellation request. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">UNLIKELY</span><span class="p">(</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="n">update_self</span><span class="p">(</span><span class="n">self</span><span class="p">);</span>
</span><span class="line">                <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Force lock reacquisition after a couple thousand writes. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">UNLIKELY</span><span class="p">(</span><span class="n">op_count</span> <span class="o">&gt;=</span> <span class="n">OP_LIMIT</span><span class="p">))</span> <span class="p">{</span>
</span><span class="line">                <span class="n">self</span><span class="o">-&gt;</span><span class="n">op_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">                <span class="n">rlock_owner_release</span><span class="p">();</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Finally, the signal handler for rlock cancellation requests iterates
through the <code>rlock_store_list</code> section until it finds a record that
strictly includes the instruction pointer.  If there is such a record,
the thread is in a critical section, and we can skip it by overwriting
<code>RIP</code> (to the end of the critical section) and setting <code>RAX</code> to 1.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"rlock.c" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="kt">void</span>
</span><span class="line"><span class="nf">rlock_signal_handler</span><span class="p">(</span><span class="kt">int</span> <span class="n">signal</span><span class="p">,</span> <span class="kt">siginfo_t</span> <span class="o">*</span><span class="n">info</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="n">ucontext_t</span> <span class="o">*</span><span class="n">ctx</span> <span class="o">=</span> <span class="n">arg</span><span class="p">;</span>
</span><span class="line">        <span class="n">mcontext_t</span> <span class="o">*</span><span class="n">mctx</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">ctx</span><span class="o">-&gt;</span><span class="n">uc_mcontext</span><span class="p">;</span>
</span><span class="line">        <span class="k">struct</span> <span class="n">rlock_owner</span> <span class="o">*</span><span class="n">self</span> <span class="o">=</span> <span class="n">rlock_self</span><span class="p">;</span>
</span><span class="line">        <span class="kt">uintptr_t</span> <span class="n">rip</span><span class="p">;</span>
</span><span class="line">        <span class="kt">size_t</span> <span class="n">nloc</span> <span class="o">=</span> <span class="n">__stop_rlock_store_list</span> <span class="o">-</span> <span class="n">__start_rlock_store_list</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">signal</span><span class="p">;</span>
</span><span class="line">        <span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">info</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">rip</span> <span class="o">=</span> <span class="p">(</span><span class="kt">uintptr_t</span><span class="p">)</span><span class="n">mctx</span><span class="o">-&gt;</span><span class="n">gregs</span><span class="p">[</span><span class="n">REG_RIP</span><span class="p">];</span>
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nloc</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">struct</span> <span class="n">rlock_store</span> <span class="n">record</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="n">record</span> <span class="o">=</span> <span class="n">__start_rlock_store_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">rip</span> <span class="o">&lt;</span> <span class="n">record</span><span class="p">.</span><span class="n">begin</span> <span class="o">||</span> <span class="n">rip</span> <span class="o">&gt;=</span> <span class="n">record</span><span class="p">.</span><span class="n">end</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="k">continue</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="n">assert</span><span class="p">(</span><span class="n">self</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">);</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* skip the critical instruction. */</span>
</span><span class="line">                <span class="n">mctx</span><span class="o">-&gt;</span><span class="n">gregs</span><span class="p">[</span><span class="n">REG_RIP</span><span class="p">]</span> <span class="o">=</span> <span class="n">record</span><span class="p">.</span><span class="n">end</span><span class="p">;</span>
</span><span class="line">                <span class="cm">/* set the interrupted flag. */</span>
</span><span class="line">                <span class="n">mctx</span><span class="o">-&gt;</span><span class="n">gregs</span><span class="p">[</span><span class="n">REG_RAX</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span><span class="line">                <span class="k">return</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* Might as well publish that we observed any cancellation request. */</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">self</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">ck_pr_fas_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">acked_sequence</span><span class="p">,</span>
</span><span class="line">                    <span class="n">ck_pr_load_32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">self</span><span class="o">-&gt;</span><span class="n">cancel_sequence</span><span class="p">));</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="silly-benchmarks">Silly benchmarks</h2>

<p>On my 2.9 GHz Sandy Bridge, a baseline loop to increment a counter a
billion times takes 6.9 cycles per increment, which makes sense given
that I use inline assembly loads and stores to prevent any compiler
cleverness.</p>

<p>The same loop with an interlocked store (<code>xchg</code>) takes 36 cycles per
increment.</p>

<p>Interestingly, an <code>xchg</code>-based spinlock around normal increments only
takes 31.7 cycles per increment (0.44 IPC).  If we wish to back our
spinlocks with futexes, we must unlock with an interlocked write; releasing
the lock with a compare-and-swap brings us to 53.6 cycles per
increment (0.30 IPC)!  Atomics really mess with pipelining: unless
they’re separated by dozens or even hundreds of instructions, their
barrier semantics (that we usually need) practically forces an
in-order, barely pipelined, execution.</p>

<p>FWIW, 50ish cycles per transaction is close to what I see in
microbenchmarks for Intel’s RTM/HLE.  So, while the overhead of TSX is
non-negligible for very short critical sections, it seems more than
reasonable for adaptive locks (and TSX definitely helps when
preemption happens, as shown by Dice and Harris in
<a href="https://timharris.uk/papers/2016-lhp.pdf">Lock Holder Preemption Avoidance via Transactional Lock Elision</a>).</p>

<p>Finally, the figure that really matters: when incrementing with
<code>rlock_store_64</code>, we need 13 cycles per increment.  That loop hits
2.99 IPC, so I think the bottleneck is just the number of instructions
in <code>rlock_store_64</code>.  The performance even seems independent of the
number of worker threads, as long as they’re all on the same CPU.</p>

<p>In tabular form:</p>

<pre><code>| Method               | Cycle / increment | IPC  |
|----------------------|-------------------|------|
| Vanilla              |             6.961 | 1.15 |
| xchg                 |            36.054 | 0.22 |
| FAS spinlock         |            31.710 | 0.44 |
| FAS-CAS lock         |            53.656 | 0.30 |
| Rlock, 1 thd         |            13.044 | 2.99 |
| Rlock, 4 thd / 1 CPU |            13.099 | 2.98 |
| Rlock, 256 / 1       |            13.952 | 2.96 |
| Rlock, 2 / 2         |            13.047 | 2.99 |
</code></pre>

<p>Six more cycles per write versus thread-private storage really isn’t
that bad (accessing TLS in a shared library might add as much
overhead)… especially compared to 25-50 cycles (in addition to
indirect slowdowns from the barrier semantics) with locked
instructions.</p>

<p>I also have a statistics-gathering mode that lets me vary the fraction
of cycles spent in critical sections.  On my server, the frequency of
context switches between CPU-intensive threads scheduled on the same
CPU increases in steps until seven or eight threads; at that point,
the frequency tops out at one switch per jiffy (250 Hz).  Apart from
this scheduling detail, evictions act as expected (same
logic as for sampled profiles).  The number of evictions is almost
equal to the number of context switches, which is proportional to the
runtime.  However, the number of hard evictions (with the victim in a
critical section) is always proportional to the number of critical
section executed: roughly one in five million critical section is
preempted.  That’s even less than the one in two million we’d expect
from the ~six cycle per critical section: that kind of makes sense
with out of order execution, given that the critical section should
easily flow through the pipeline and slip past timer interrupts.</p>

<h2 id="trade-offs">Trade-offs</h2>

<p>The main trade-off is that rlocks do not attempt to handle thread
migrations: when a thread migrates to another CPU, we let it assume
(temporary) exclusive ownership of its pseudo-per-CPU struct instead
of issuing IPIs.  That’s good for simplicity, and also – arguably –
for scaling.  The scaling argument is weak, given how efficient IPIs
seem to be.  However, IPIs feel like one of these operations for which
most of the cost is indirect and hard to measure.  The overhead isn’t
only (or even mostly) incurred by the thread that triggers the IPIs:
each CPU must stop what it’s currently doing, flush the pipeline,
switch to the kernel to handle the interrupt, and resume execution.  A
scheme that relies on IPIs to handle events like thread migrations
(rare, but happens at a non-negligible base rate) will scale badly to
really large CPU counts, and, more importantly, may make it hard to
identify when the IPIs hurt overall system performance.</p>

<p>The other important design decision is that rlocks uses signals
instead of cross-modifying code.  I’m not opposed to cross-modifying
code, but I cringe at the idea of leaving writable and executable
pages lying around just for performance.  Again, we could <code>mprotect</code>
around cross-modification, but <code>mprotect</code> triggers IPIs, and that’s
exactly what we’re trying to avoid.  Also, if we’re going to
<code>mprotect</code> in the common case, we might as well just <code>mmap</code> in
different machine code; that’s likely a bit faster than two <code>mprotect</code>
and definitely safer (I would use this <code>mmap</code> approach for revocable
multi-CPU locks à la Harris and Fraser).</p>

<p>The downside of using signals is that they’re more invasive than
cross-modifying code.  If user code expects any (async) signal, its
handlers must either mask the rlock signal away and not use rlocks, or
call the rlock signal handler… not transparent, but not exacting
either.</p>

<p>Rlocks really aren’t that much code (560 LOC), and that code is fairly
reasonable (no mprotect or self-modification trick, just signals).
After more testing and validation, I would consider merging them in
<a href="http://concurrencykit.org/">Concurrency Kit</a> for production use.</p>

<p>Next step: either <code>mmap</code>-based strict revocable locks for non-blocking
concurrent code, or a full implementation of pseudo-per-CPU data based
on relaxed rlocks.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock-free sequence locks]]></title>
    <link href="https://www.pvk.ca/Blog/2015/01/13/lock-free-mutual-exclusion/"/>
    <updated>2015-01-13T02:30:00-05:00</updated>
    <id>https://www.pvk.ca/Blog/2015/01/13/lock-free-mutual-exclusion</id>
    <content type="html"><![CDATA[<p>Specialised locking schemes and lock-free data structures are a big
part of my work these days.  I think the main reason the situation is
tenable is that, very early on, smart people decided to focus on an
SPMC architecture: single writer (producer), multiple readers
(consumers).</p>

<p>As programmers, we have a tendency to try and maximise generality: if
we can support multiple writers, why would one bother with measly SPMC
systems?  The thing is SPMC is harder than SPSC, and MPMC is
even more complex.  Usually, more concurrency means programs are harder to
get right, harder to scale and harder to maintain.  Worse: it also
makes it more difficult to provide theoretical progress guarantees.</p>

<p>Apart from architecting around simple cases, there’s a few ways to
deal with this reality.  We can define new, weaker, classes of program,
like obstruction-freedom: a system is obstruction-free when one thread
is guaranteed to make progress if <em>every other thread is suspended</em>.  We
can also <a href="http://www.cosy.sbg.ac.at/research/tr/2010-07_Kirsch_Payer_Roeck.pdf">weaken the guarantees of our data structure</a>.  For example,
rather than exposing a single FIFO, we could distribute load and
contention across multiple queues; we lose strict FIFO order, but we
also eliminate a system bottleneck.  Another option is to try and
identify how real computers are more powerful than our abstract
models: some argue that, <a href="http://arxiv.org/abs/1311.3200">realistically, many lock-free schemes are wait-free</a>, and others <a href="http://www.cs.technion.ac.il/~mad/publications/asplos2014-ffwsq.pdf">exploit the fact that x86-TSO machines have finite store buffers</a>.</p>

<p>Last week, I got lost doodling with x86-specific cross-modifying code,
but still stumbled on a cute example of a simple lock-free protocol:
lock-free sequence locks.  This sounds like an oxymoron, but I promise
it makes sense.</p>

<h1 id="lock-free-sequence-locks">Lock-free sequence locks</h1>

<p>It helps to define the terms
better. <a href="http://en.wikipedia.org/wiki/Non-blocking_algorithm#Lock-freedom">Lock-freedom</a>
means that the overall system will always make progress, even if some
(but not all) threads are suspended.
<a href="http://en.wikipedia.org/wiki/Seqlock">Classical sequence</a> locks are
an optimistic form of write-biased reader/writer locks: concurrent
writes are forbidden (e.g., with a spinlock), read transactions abort
whenever they observe that writes are in progress, and a generation
counter avoids
<a href="http://en.wikipedia.org/wiki/ABA_problem">ABA problems</a> (when a read
transaction would observe that no write is in progress before and after a
quick write).</p>

<p>In
<a href="http://www.cs.rochester.edu/u/scott/papers/2010_EuroPar_TML.pdf">Transactional Mutex Locks (PDF)</a>,
sequence locks proved to have enviable performance on small systems and
scaled decently well for read-heavy workloads.  They even allowed lazy
upgrades from reader to writer by atomically checking that the
generation has the expected value when acquiring the sequence lock for
writes.  However, we lose nearly all progress guarantees: one
suspended writer can freeze the whole system.</p>

<p>The central trick of lock-freedom is cooperation: it doesn’t matter if
a thread is suspended in the middle of a critical section, as long as
any other thread that would block can instead complete the work that
remains.  In general, this is pretty hard, but we can come up with
restricted use cases that are idempotent.  For lock-free sequence
locks, the critical section is a precomputed set of writes: a series
of assignments that must appear to execute atomically.  It’s fine if
writes happen multiple times, as long as they stop before we move on
to another set of writes.</p>

<p>There’s a primitive based on compare-and-swap that can easily achieve
such conditional writes: restricted double compare and single swap
(RDCSS, introduced in
<a href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2002-casn.pdf">A Practical Multi-Word Compare-and-Swap (PDF)</a>).
RDCSS atomically checks if both a control word (e.g., a generation
counter) and a data word (a mutable cell) have the expected values and,
if so, writes a new value in the data word.  The pseudocode for
regular writes looks like</p>

<pre><code>if (CAS(self.data, self.old, self) == fail) {
    return fail;
}
   
if (*self.control != self.expected) {
    CAS(self.data, self, self.old);
    return fail;
}

CAS(self.data, self, self.new);
return success;
</code></pre>

<p>The trick is that, if the first CAS succeeds, we always know how to
undo it (<code>data</code>’s old value must be <code>self.old</code>), and that
information is stored in <code>self</code> so any thread that observes the first
CAS has enough information to complete or rollback the RDCSS.  The
only annoying part is that we need a two-phase commit: reserve <code>data</code>,
confirm that <code>control</code> is as <code>expected</code>, and only then write to <code>data</code>.</p>

<p>For the cost of two compare-and-swap per write – plus one to acquire the
sequence lock – writers don’t lock out other writers (writers help
each other make progress instead).  Threads (especially readers) can
still suffer from starvation, but at least the set of writes can be
published ahead of time, so readers can even lookup in that set rather
than waiting for/helping writes to complete.  The generation counter
remains a bottleneck, but, as long as writes are short and happen
rarely, that seems like an acceptable trade to avoid the 3n CAS in
multi-word compare and swap.</p>

<h1 id="real-code">Real code</h1>

<p>Here’s what the scheme looks like in SBCL.</p>

<p>First, a mutable box because we don’t have raw pointers (I could also
have tried to revive my sb-locative hack) in CL.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defstruct (box
</span><span class="line">            (:constructor make-box (%value)))
</span><span class="line">  %value)</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Next, the type for write records: we have the the value for the next
generation (once the write is complete) and a hash table of box to
pairs of old and new values.  There’s a key difference with the way
RDCSS is used to implement multiple compare and swap: we don’t check
for mismatches in the old value and simply assume that it is correct.</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defstruct (record
</span><span class="line">             (:constructor %make-record (generation ops)))
</span><span class="line">  (generation (error "Missing arg") :type fixnum :read-only t)
</span><span class="line">  ;; map of box -&gt; (cons old new).  I use a hash table for
</span><span class="line">  ;; convenience but I doubt it's the right choice.
</span><span class="line">  (ops (error "Missing arg") :type hash-table :read-only t))
</span><span class="line">(declaim (freeze-type record))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The central bottleneck is the sequence lock, which each (read)
transaction must snapshot before attempting to read consistent
values.</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(declaim (type (or (and unsigned-byte fixnum) record) **current-record**))
</span><span class="line">(defglobal **current-record** 0)
</span><span class="line">
</span><span class="line">(defvar *initial-record*)
</span><span class="line">
</span><span class="line">(defun snapshot-generation ()
</span><span class="line">  (let ((initial *initial-record*))
</span><span class="line">    (if (record-p initial)
</span><span class="line">        (record-generation initial)
</span><span class="line">        initial)))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The generation associated with a snapshot is the snapshot if it is a
positive fixnum, otherwise it is the write record’s generation.</p>

<p>Before using any read, we make sure that the generation counter hasn’t
changed.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun check ()
</span><span class="line">  #-(or x86 x86-64) (sb-thread:barrier (:read)) ; x86 don't reorder reads
</span><span class="line">  (let ((initial *initial-record*)
</span><span class="line">        (current **current-record**))
</span><span class="line">    (unless (or (eql initial current)
</span><span class="line">                (and (record-p initial)
</span><span class="line">                     (eql (record-generation initial) current)))
</span><span class="line">      (throw 'fail t))))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I see two ways to deal with starting a read transaction while a write
is in progress: we can help the write complete, or we can overlay the
write on top of the current heap in software.  I chose the latter:
reads can already be started by writers.  If a write is in progress
when we start a transaction, we stash the write set in <code>*current-map*</code>
and lookup there first:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defvar *current-map* nil)
</span><span class="line">
</span><span class="line">(defun box-value (box)
</span><span class="line">  (prog1 (let* ((map *current-map*)
</span><span class="line">                (value (if map
</span><span class="line">                           (cdr (gethash box map (box-%value box)))
</span><span class="line">                           (box-%value box))))
</span><span class="line">           (if (record-p value)
</span><span class="line">               ;; if we observe a record, either a new write is in
</span><span class="line">               ;; progress and (check) is about to fail, or this is
</span><span class="line">               ;; for an old (already completed) write that succeeded
</span><span class="line">               ;; partially by accident.  In the second case, we want
</span><span class="line">               ;; the *old* value.
</span><span class="line">               (car (gethash box (record-ops value)))
</span><span class="line">               value))
</span><span class="line">    (check)))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We’re now ready to start read transactions.  We take a snapshot of the
generation counter, update <code>*current-map*</code>, and try to execute a
function that uses <code>box-value</code>.  Again, we don’t need a read-read
barrier on x86oids (nor on SPARC, but SBCL doesn’t have threads on
that platform).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun call-with-transaction (function &amp;rest arguments)
</span><span class="line">  (catch 'fail
</span><span class="line">    (let* ((*initial-record* **current-record**)
</span><span class="line">           (*current-map* (and (record-p *initial-record*)
</span><span class="line">                               (record-ops *initial-record*))))
</span><span class="line">      #-(or x86 x86-64) (sb-thread:barrier (:read))
</span><span class="line">      (return-from call-with-transaction
</span><span class="line">        (values (apply function arguments) t))))
</span><span class="line">  (values nil nil))
</span><span class="line">
</span><span class="line">(defmacro with-transaction ((&amp;rest bindings) &amp;body body)
</span><span class="line">  `(call-with-transaction (lambda ,(mapcar #'first bindings)
</span><span class="line">                            ,@body)
</span><span class="line">                          ,@(mapcar #'second bindings)))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The next function is the keystone: helping a write record go through
exactly once.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun help (record)
</span><span class="line">  (flet ((write-one (box old new)
</span><span class="line">           ;; if record isn't the current generation anymore,
</span><span class="line">           ;; it has already been completed
</span><span class="line">           (unless (eq **current-record** record)
</span><span class="line">               (return-from help nil))
</span><span class="line">             (let ((actual (sb-ext:cas (box-%value box) old record)))
</span><span class="line">               (when (eql actual new) ;; already done? next!
</span><span class="line">                 (return-from write-one))
</span><span class="line">               
</span><span class="line">               ;; definite failure -&gt; no write went though; leave.
</span><span class="line">               (unless (or (eql actual old)
</span><span class="line">                           (eql actual record))
</span><span class="line">                 (return-from help nil))
</span><span class="line">
</span><span class="line">               ;; check for activity before the final write
</span><span class="line">               (unless (eq **current-record** record)
</span><span class="line">                 (sb-ext:cas (box-%value box) record old)
</span><span class="line">                 (return-from help nil))
</span><span class="line">
</span><span class="line">               ;; Really perform write (this can only fail if
</span><span class="line">               ;; another thread already succeeded).
</span><span class="line">               (sb-ext:cas (box-%value box) record new))))
</span><span class="line">    (maphash (lambda (box op)
</span><span class="line">               (write-one box (car op) (cdr op)))
</span><span class="line">             (record-ops record)))
</span><span class="line">  ;; Success! move the generation counter forward.
</span><span class="line">  (eql record (sb-ext:cas (symbol-value '**current-record**)
</span><span class="line">                          record
</span><span class="line">                          (record-generation record))))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Now we can commit with a small wrapper around <code>help</code>. Transactional
mutex lock has the idea of transaction that are directly created as
write transactions.  We assume that we always know how to undo writes,
so transactions can only be upgraded from reader to writer.
Committing a write will thus check that the generation counter is
still consistent with the (read) transaction before publishing the new
write set and helping it forward.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun commit (record)
</span><span class="line">  (check-type record record)
</span><span class="line">  (let ((initial
</span><span class="line">          (loop
</span><span class="line">           (let ((value **current-record**))
</span><span class="line">             (check)
</span><span class="line">             (if (record-p value)
</span><span class="line">                 (help value)
</span><span class="line">                 (return value))))))
</span><span class="line">    (unless (and (eql (sb-ext:cas (symbol-value '**current-record**)
</span><span class="line">                                  initial record)
</span><span class="line">                      initial)
</span><span class="line">                 (help record))
</span><span class="line">      (throw 'fail t))
</span><span class="line">    t))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And now some syntactic sugar to schedule writes</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defvar *write-record*)
</span><span class="line">
</span><span class="line">(defun call-with-write-record (function)
</span><span class="line">  (let ((*write-record* (%make-record (mod (1+ (snapshot-generation))
</span><span class="line">                                           (1+ most-positive-fixnum))
</span><span class="line">                                      (make-hash-table))))
</span><span class="line">    (multiple-value-prog1 (funcall function)
</span><span class="line">      (commit *write-record*))))
</span><span class="line">
</span><span class="line">(defun (setf box-value) (value box)
</span><span class="line">  (setf (gethash box (record-ops *write-record*))
</span><span class="line">        (cons (box-value box) value))
</span><span class="line">  value)
</span><span class="line">
</span><span class="line">(defmacro with-write (() &amp;body body)
</span><span class="line">  `(call-with-write-record (lambda ()
</span><span class="line">                             ,@body)))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>That’s enough for a smoke test on my dual core laptop.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defvar *a* (make-box 0))
</span><span class="line">(defvar *b* (make-box 0))
</span><span class="line">(defvar *semaphore* (sb-thread:make-semaphore))
</span><span class="line">
</span><span class="line">(defun test-reads (n)
</span><span class="line">  (let ((a *a*)
</span><span class="line">        (b *b*))
</span><span class="line">    (sb-thread:wait-on-semaphore *semaphore*)
</span><span class="line">    (loop repeat n
</span><span class="line">          count (with-transaction ()
</span><span class="line">                  (assert (eql (box-value a) (box-value b)))
</span><span class="line">                  t))))
</span><span class="line">
</span><span class="line">(defun test-writes (n)
</span><span class="line">  (let ((a *a*)
</span><span class="line">        (b *b*))
</span><span class="line">    (sb-thread:wait-on-semaphore *semaphore*)
</span><span class="line">    (loop repeat n
</span><span class="line">          count (with-transaction ()
</span><span class="line">                  (with-write ()
</span><span class="line">                    (incf (box-value a))
</span><span class="line">                    (incf (box-value b)))
</span><span class="line">                  t))))</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The function <code>test-reads</code> counts the number of successful read
transactions and checks that <code>(box-value a)</code> and <code>(box-value b)</code> are
always equal. That consistency is preserved by <code>test-writes</code>, which
counts the number of times it succeeds in incrementing both
<code>(box-value a)</code> and <code>(box-value b)</code>.</p>

<p>The baseline case should probably be serial execution, while the ideal
case for transactional mutex lock is when there is at most one
writer.  Hopefully, lock-free sequence locks also does well when there
are multiple writers.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun test-serial (n)
</span><span class="line">  (setf *a* (make-box 0)
</span><span class="line">        *b* (make-box 0)
</span><span class="line">        *semaphore* (sb-thread:make-semaphore :count 4))
</span><span class="line">  (list (test-reads (* 10 n))
</span><span class="line">        (test-reads (* 10 n))
</span><span class="line">        (test-writes n)
</span><span class="line">        (test-writes n)))
</span><span class="line">
</span><span class="line">(defun test-single-writer (n)
</span><span class="line">  (setf *a* (make-box 0)
</span><span class="line">        *b* (make-box 0)
</span><span class="line">        *semaphore* (sb-thread:make-semaphore))
</span><span class="line">  (let ((threads
</span><span class="line">          (list (sb-thread:make-thread #'test-reads :arguments (* 10 n))
</span><span class="line">                (sb-thread:make-thread #'test-reads :arguments (* 10 n))
</span><span class="line">                (sb-thread:make-thread #'test-writes
</span><span class="line">                                       :arguments (ceiling (* 1.45 n))))))
</span><span class="line">    (sb-thread:signal-semaphore *semaphore* 3)
</span><span class="line">    (mapcar (lambda (x)
</span><span class="line">              (ignore-errors (sb-thread:join-thread x)))
</span><span class="line">            threads)))
</span><span class="line">
</span><span class="line">(defun test-multiple-writers (n)
</span><span class="line">  (setf *a* (make-box 0)
</span><span class="line">        *b* (make-box 0)
</span><span class="line">        *semaphore* (sb-thread:make-semaphore))
</span><span class="line">  (let ((threads
</span><span class="line">          (list (sb-thread:make-thread #'test-reads :arguments (* 10 n))
</span><span class="line">                (sb-thread:make-thread #'test-reads :arguments (* 10 n))
</span><span class="line">                (sb-thread:make-thread #'test-writes :arguments n)
</span><span class="line">                (sb-thread:make-thread #'test-writes :arguments n))))
</span><span class="line">    (sb-thread:signal-semaphore *semaphore* 4)
</span><span class="line">    (mapcar (lambda (x)
</span><span class="line">              (ignore-errors (sb-thread:join-thread x)))
</span><span class="line">            threads)))</span></code></pre></td></tr></table></div></figure></notextile></div>

<h1 id="lets-try-this">Let’s try this!</h1>

<p>First, the serial case. As expected, all the transactions succeed, in
6.929 seconds total (6.628 without GC time).  With one writer and two
readers, all the writes succeed (as expected), and 98.5% of reads do as
well; all that in 4.186 non-GC seconds, a 65% speed up.  Finally, with
two writers and two readers, 76% of writes and 98.5% of reads complete in
4.481 non-GC seconds.  That 7% slowdown compared to the single-writer
case is pretty good: my laptop only has two cores, so I would expect
more aborts on reads and a lot more contention with, e.g., a spinlock.</p>

<pre><code>CL-USER&gt; (gc :full t) (time (test-serial 1000000))
Evaluation took:
  6.929 seconds of real time
  6.944531 seconds of total run time (6.750770 user, 0.193761 system)
  [ Run times consist of 0.301 seconds GC time, and 6.644 seconds non-GC time. ]
  100.23% CPU
  11,063,956,432 processor cycles
  3,104,014,784 bytes consed
  
(10000000 10000000 1000000 1000000)
CL-USER&gt; (gc :full t) (time (test-single-writer 1000000))
Evaluation took:
  4.429 seconds of real time
  6.465016 seconds of total run time (5.873936 user, 0.591080 system)
  [ Run times consist of 0.243 seconds GC time, and 6.223 seconds non-GC time. ]
  145.97% CPU
  6,938,703,856 processor cycles
  2,426,404,384 bytes consed
  
(9863611 9867095 1450000)
CL-USER&gt; (gc :full t) (time (test-multiple-writers 1000000))
Evaluation took:
  4.782 seconds of real time
  8.573603 seconds of total run time (7.644405 user, 0.929198 system)
  [ Run times consist of 0.301 seconds GC time, and 8.273 seconds non-GC time. ]
  179.30% CPU
  7,349,757,592 processor cycles
  3,094,950,400 bytes consed
  
(9850173 9853102 737722 730614)
</code></pre>

<p>How does a straight mutex do with four threads?</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(defun test-mutex (n)
</span><span class="line">  (let ((mutex (sb-thread:make-mutex))
</span><span class="line">        (semaphore (sb-thread:make-semaphore))
</span><span class="line">        (a 0)
</span><span class="line">        (b 0))
</span><span class="line">    (flet ((reader (n)
</span><span class="line">             (sb-thread:wait-on-semaphore semaphore)
</span><span class="line">             (loop repeat n do
</span><span class="line">               (sb-thread:with-mutex (mutex)
</span><span class="line">                 (assert (eql a b)))))
</span><span class="line">           (writer (n)
</span><span class="line">             (sb-thread:wait-on-semaphore semaphore)
</span><span class="line">             (loop repeat n do
</span><span class="line">               (sb-thread:with-mutex (mutex)
</span><span class="line">                 (incf a)
</span><span class="line">                 (incf b)))))
</span><span class="line">      (let ((threads
</span><span class="line">              (list (sb-thread:make-thread #'reader
</span><span class="line">                                           :arguments (* 10 n))
</span><span class="line">                    (sb-thread:make-thread #'reader
</span><span class="line">                                           :arguments (* 10 n))
</span><span class="line">                    (sb-thread:make-thread #'writer
</span><span class="line">                                           :arguments (ceiling (* .75 n)))
</span><span class="line">                    (sb-thread:make-thread #'writer
</span><span class="line">                                           :arguments (ceiling (* .75 n))))))
</span><span class="line">        (sb-thread:signal-semaphore semaphore 4)
</span><span class="line">        (mapc #'sb-thread:join-thread threads)))))</span></code></pre></td></tr></table></div></figure></notextile></div>

<pre><code>CL-USER&gt; (gc :full t) (time (test-mutex 1000000))
Evaluation took:
  5.814 seconds of real time
  11.226734 seconds of total run time (11.169670 user, 0.057064 system)
  193.10% CPU
  9,248,370,000 processor cycles
  1,216 bytes consed
  
(#&lt;SB-THREAD:THREAD FINISHED values: NIL {1003A6E1F3}&gt;
 #&lt;SB-THREAD:THREAD FINISHED values: NIL {1003A6E383}&gt;
 #&lt;SB-THREAD:THREAD FINISHED values: NIL {1003A6E513}&gt;
 #&lt;SB-THREAD:THREAD FINISHED values: NIL {1003A6E6A3}&gt;)
</code></pre>

<p>There’s almost no allocation (there’s no write record), but the lack
of read parallelism makes locks about 20% slower than the lock-free
sequence lock.  A reader-writer lock would probably close that gap.
The difference is that the lock-free sequence lock has stronger
guarantees in the worst case: no unlucky preemption (or crash, with
shared memory IPC) can cause the whole system to stutter or even halt.</p>

<p>The results above correspond to my general experience.  Lock-free
algorithms aren’t always (or even regularly) more efficient than well
thought out locking schemes; however, they are more robust and easier
to reason about.  When throughput is more than adequate, it makes
sense to eliminate locks, not to improve the best or even the average
case, but rather to eliminate a class of worst cases – including
deadlocks.</p>

<p>P.S., here’s a sketch of the horrible cross-modifying code hack.  It
turns out that the instruction cache is fully coherent on (post-586)
x86oids; the prefetch queue will even reset itself based on the linear
(virtual) address of writes.  With a single atomic byte write, we can
turn a <code>xchg (%rax), %rcx</code> into <code>xchg (%rbx), %rcx</code>, where <code>%rbx</code>
points to a location that’s safe to mutate arbitrarily.  That’s an
atomic store predicated on the value of a control word elsewhere
(hidden in the instruction stream itself, in this case).  We can then
dedicate one sequence of machine to each transaction and reuse them
via some
<a href="http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf">Safe Memory Reclamation mechanism (PDF)</a>.</p>

<p>There’s one issue: even without preemption (if a writer is pre-empted,
it should see the modified instruction upon rescheduling), stores
can take pretty long to execute: in the worst case, the CPU has to
translate to a physical address and wait for the bus lock.  I’m pretty
sure there’s a bound on how long a <code>xchg m, r64</code> can take, but I
couldn’t find any documentation on hard figure.  If we knew that <code>xchg
m, r64</code> never lasts more than, e.g., 10k cycles, a program could wait
that many cycles before enqueueing a new write.  That wait is bounded
and, as long as writes are disabled very rarely, should improve
the worst-case behaviour without affecting the average throughput.</p>
]]></content>
  </entry>
  
</feed>
