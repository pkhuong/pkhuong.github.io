<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PerfAnalysis | Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/Blog/categories/perfanalysis/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2022-12-29T15:47:06-05:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Too much locality... for stores to forward]]></title>
    <link href="https://www.pvk.ca/Blog/2020/02/01/too-much-locality-for-store-forwarding/"/>
    <updated>2020-02-01T17:29:21-05:00</updated>
    <id>https://www.pvk.ca/Blog/2020/02/01/too-much-locality-for-store-forwarding</id>
    <content type="html"><![CDATA[<p><small>Apologies for the failed <a href="https://www.youtube.com/watch?v=K3DRkVjuqmc">Cake reference</a>.<br />
2020-02-02: Refer to Travis Downs’s investigation into this pathological case for forwarding.</small></p>

<p>I’ve been responsible for <a href="https://backtrace.io/">Backtrace.io</a>’s crash analytic database<sup id="fnref:started-work" role="doc-noteref"><a href="#fn:started-work" class="footnote" rel="footnote">1</a></sup> for a couple months now.
I have focused my recent efforts on improving query times for in-memory grouped aggregations, i.e.,
the archetypal MapReduce use-case where we generate key-value pairs, and <a href="https://en.wikipedia.org/wiki/Fold_(higher-order_function)">fold</a> over the values
for each key in some <a href="https://en.wikipedia.org/wiki/Semigroup">(semi)</a><a href="https://en.wikipedia.org/wiki/Group_(mathematics)">group</a>.
We have a cute cache-efficient data structure for this type of workload;
the inner loop simply inserts in a small hash table with <a href="/Blog/more_numerical_experiments_in_hashing.html">Robin Hood linear probing</a>,
in order to guarantee entries in the table are ordered by hash value.  This
ordering lets us easily dump the entries in sorted order, and <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/1993/6309.html">block</a> the merge loop for an arbitrary number of sorted arrays
into a unified, larger, ordered hash table (which we can, again, dump to a sorted array).<sup id="fnref:more-later" role="doc-noteref"><a href="#fn:more-later" class="footnote" rel="footnote">2</a></sup></p>

<h1 id="observation">Observation</h1>

<p>As I updated more operators to use this data structure, I noticed that we were spending a lot of time in its inner loop.
In fact, <a href="http://www.brendangregg.com/linuxperf.html">perf</a> showed that the query server as a whole was spending 4% of its CPU time on one instruction in that loop:</p>

<pre><code> 2.17 |       movdqu     (%rbx),%xmm0
39.63 |       lea        0x1(%r8),%r14  # that's 40% of the annotated function
      |       mov        0x20(%rbx),%rax
 0.15 |       movaps     %xmm0,0xa0(%rsp)
</code></pre>

<p>The first thing to note is that instruction-level profiling tends to put the blame on the instruction <em>following</em> the one that triggered a sampling interrupt.
It’s not the <code>lea</code> (which computes <code>r14 &lt;- r8 + 1</code>) that’s slow, but the <code>movdqu</code> just before.
So, what is that <code>movdqu</code> loading into <code>xmm0</code>?  Maybe it’s just a normal cache miss, something inherent to the workload.</p>

<p>I turned on source locations <a href="http://man7.org/linux/man-pages/man1/perf-report.1.html">(hit <code>s</code> in <code>perf report</code>)</a>, and observed that this instruction was simply copying to the stack an argument that was passed by address.
The source clearly showed that the argument should be hot in cache: the inner loop was essentially</p>

<pre><code>A1. Generate a new key-value pair
B1. Mangle that kv pair just a bit to turn it into a hash element
C1. Insert the new hash element
A2.
B2.
C2.
</code></pre>

<p>and the <code>movdqu</code> happens in step C, to copy the element that step B just constructed.<sup id="fnref:dont-copy" role="doc-noteref"><a href="#fn:dont-copy" class="footnote" rel="footnote">3</a></sup></p>

<p>At this point, an important question suggests itself: does it matter?
We could simply increase the size of the base case and speed up the rest of the bottom-up recursion… eventually, the latency for the random accesses in the initial hash table will dominate the inner loop.</p>

<p>When I look into the performance of these deep inner loop, my goal isn’t only to do the same thing better.
The big wins, in my experience, come from the additional design freedom that we get from being able to find new uses for the same code.
Improved latency, throughput, or memory footprint really shine when the increased optionality from multiple such improvements compounds and lets us consider a much larger design space for the project as a whole.
That’s why I wanted to make sure this hash table insertion loop worked on as wide a set of parameter as possible: because that will give future me the ability to combine versatile tools.</p>

<h1 id="hypothesis">Hypothesis</h1>

<p>Back to the original question. Why do we spend so many cycles loading data we just wrote to cache?</p>

<p>The answer is in the question and in the title of this post: too little time elapses between the instructions that write data to the cache and the ones that read the same data.<sup id="fnref:but-forwarding" role="doc-noteref"><a href="#fn:but-forwarding" class="footnote" rel="footnote">4</a></sup>
A modern out-of-order machine (e.g., most amd64 chips) can execute multiple instructions at the same time, and will start executing instructions as soon as their operands are ready, even when earlier instructions in program order are still waiting for theirs.
Machine code is essentially a messy way to encode a <a href="https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/">dataflow graph</a>, 
which means our job as micro-optimisers is, at a high level, to avoid long dependency chains and make the dataflow graph as wide as possible.
When that’s too hard, we should distribute as much scheduling slack as possible between nodes in a chain, in order to absorb the knock-on effects of cache misses and other latency spikes.
If we fail, the chip will often find itself with no instruction ready to execute; stalling the pipeline like that is like slowing down by a factor of 10.</p>

<p>The initial inner loop simply executes steps A, B, and C in order, where step C depends on the result of step B, and step B on that of step A.
In theory, a chip with a wide enough instruction reordering window could pipeline multiple loop iterations.
In practice, real hardware can only <a href="http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/">plan on the order of 100-200 instructions ahead</a>, and that mechanism depends on branches being predicted correctly.
We have to explicitly insert slack in our dataflow schedule, and we must distribute it well enough for instruction reordering to see the gaps.</p>

<p>This specific instance is a particularly bad case for contemporary machines:
step B populates the entry with regular (64-bit) register writes,
while step C copies the same bytes with vector reads and writes.
<a href="https://gist.github.com/travisdowns/bc9af3a0aaca5399824cf182f85b9e9c">Travis Downs looked into this forwarding scenario</a> and found that no other read-after-write setup behaves this badly, on Intel or AMD.
That’s probably why the <code>movdqu</code> vector load instruction was such an issue.
If the compiler had emitted the copy with GPR reads and writes,
that <em>might</em> have been enough for the hardware to hide the latency.
However, as <a href="https://twitter.com/trav_downs/status/1223766684932222976">Travis points out on Twitter</a>, it’s hard for a compiler to get that right across compilation units.
In any case, our most reliable (more so than passing this large struct by value and hoping the compiler will avoid mismatched instructions) and powerful tool to fix this at the source level is to schedule operations manually.</p>

<p>The dataflow graph for each loop iteration is currently a pure chain:</p>

<pre><code>         A1
         |
         v
         B1
         |
         v
         C1
                A2
                |
                v
                B2
                |
                v
                C2
</code></pre>

<p>How does one add slack to these chains? With bounded queues!</p>

<h1 id="experiment">Experiment</h1>

<p>My first fix was to add a one-element buffer between steps B and C.  The inner loop became</p>

<pre><code>A1. Generate a new key-value pair
C0. Insert the hash element from the previous iteration
B1. Mangle the kv pair and stash that in the buffer
A2.
C1.
B2
etc.
</code></pre>

<p>which yields a dataflow graph like</p>

<pre><code>        |     A1
        v     |
        C0    |
              |
              v
              B1
              |
              |     A2
              v     |
              C1    |
                    |
                    v
                    B2
                    |
</code></pre>

<p>We’ve introduced slack between steps A and B (there’s now step C from the previous iteration between them), and between steps B and C (we shifted step A from the next iteration between them).
There isn’t such a long delay between the definition of a value and its use that the data is likely to be evicted from L1.
However, there is more than enough work available between them to keep the pipeline busy with useful work while C waits for B’s result, or B for A’s.
That was a nice single-digit improvement in query latency for my internal benchmark, just by permuting a loop.</p>

<p>If a one-element buffer helps, we should clearly experiment with the buffer size, and that’s where I found a more impactful speed-up.
Once we have an array of elements to insert in a hash table, we can focus on a bulk insert of maybe 8 or 10 elements: instead of trying to improve the latency for
individual writes, we can focus on the throughput for multiple inserts at once.
That’s good because <a href="http://www.stuartcheshire.org/rants/Latency.html">throughput is an easier problem than latency</a>.
In the current case, passing the whole buffer to the hash table code made it easier to <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/1993/6309.html">pipeline the insert loop in software</a>:
we can compute hashes ahead of time, and accelerate random accesses to the hash table with <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=_mm_prefetch&amp;expand=4391">software prefetching</a>.
The profile for the new inner loop is flatter, and the hottest part is as follows</p>

<pre><code>      |       mov        0x8(%rsp),%rdx
 9.91 |       lea        (%r12,%r12,4),%rax
 0.64 |       prefetcht0 (%rdx,%rax,8)
17.04 |       cmp        %rcx,0x28(%rsp)
</code></pre>

<p>Again, the blame for a “slow” instruction hits the following instruction, so it’s not <code>lea</code> (multiplying by 5) or <code>cmp</code> that are slow; it’s the load from the stack and the prefetch.
The good news is that these instructions do not have any dependent.  It’s all prefetching, and that’s only used for its side effects.
Moreover, they come from a block of code that was pipelined in software and executes one full iteration ahead of where its side effects might be useful.
It doesn’t really matter if these instructions are slow: they’re still far from being on the critical path!  This last restructuring yielded a 20% speed-up on a few slow queries.</p>

<p>I described two tools that I use regularly when optimising code for contemporary hardware.
Finding ways to scatter around scheduling slack is always useful, both in software and in real life planning.<sup id="fnref:unless-people" role="doc-noteref"><a href="#fn:unless-people" class="footnote" rel="footnote">5</a></sup>
One simple way to do so is to add bounded buffers, and to flush buffers as soon as they fill up (or refill when they become empty), instead of waiting until the next write to the buffer.
However, I think the more powerful transformation is using buffering to expose bulk operations, which tends to open up more opportunities than just doing the same thing in a loop.
In the case above, we found a 20% speed-up; for someone who visit their <a href="https://help.backtrace.io/en/articles/2765535-triage">Backtrace dashboard</a> a couple times a day, that can add up to an hour or two at the end of the year.</p>

<p>TL;DR: When a function is hot enough to look into, it’s worth asking why it’s called so often, in order to focus on higher level bulk operations.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:started-work" role="doc-endnote">
      <p>And by that, I mean I started working there a couple months ago (: <a href="#fnref:started-work" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:more-later" role="doc-endnote">
      <p>I think that’s a meaty idea, and am planning a longer post on that data structure and where it fits in the hash/sort join continuum. <a href="#fnref:more-later" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:dont-copy" role="doc-endnote">
      <p>Would I have avoided this issue if I had directly passed by value? The resulting code might have been friendlier to store-to-load forwarding than loading a whole 128 bit SSE register, but see the next footnote. <a href="#fnref:dont-copy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-forwarding" role="doc-endnote">
      <p>Store-to-load forwarding can help improve the performance of this pattern, when we use forwarding patterns that the hardware supports. However, this mechanism can only decrease the penalty of serial dependencies, e.g., by shaving away some or all of the time it takes to store a result to cache and load it back; even when results can feed directly into dependencies, we still have to wait for inputs to be computed. This is fundamentally a scheduling issue. <a href="#fnref:but-forwarding" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:unless-people" role="doc-endnote">
      <p>Unless you’re writing schedule optimising software and people will look at the result. A final hill climbing pass to make things look artificially tight often makes for an easier sale in that situation. <a href="#fnref:unless-people" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Confidence Sequence Method: a computer-age test for statistical SLOs]]></title>
    <link href="https://www.pvk.ca/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method/"/>
    <updated>2018-07-06T18:02:40-04:00</updated>
    <id>https://www.pvk.ca/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method</id>
    <content type="html"><![CDATA[<p><em>This post goes over <a href="https://github.com/pkhuong/csm">some code that I pushed to github today</a>. All the snippets below should be <a href="https://github.com/pkhuong/csm/blob/master/csm.lisp">in the repo</a>,
which also <a href="https://github.com/pkhuong/csm/blob/master/csm.h">includes</a>
<a href="https://github.com/pkhuong/csm/blob/master/csm.c">C</a> and
<a href="https://github.com/pkhuong/csm/blob/master/csm.py">Python code</a> with the same structure.</em></p>

<p>I recently resumed thinking about balls and bins for hash tables. This
time, I’m looking at large bins (on the order of one 2MB huge page).
There are 
<a href="https://en.wikipedia.org/wiki/Cuckoo_hashing">many</a>
<a href="https://xlinux.nist.gov/dads/HTML/twoLeftHashing.html">hashing</a>
<a href="https://en.wikipedia.org/wiki/Dynamic_perfect_hashing">methods</a>
with solid worst-case guarantees that unfortunately query multiple
uncorrelated locations; I feel like we could automatically adapt them
to modern hierarchical storage (or address translation) to make them
more efficient, for a small loss in density.</p>

<p>In theory, 
<a href="https://en.wikipedia.org/wiki/Balls_into_bins#Random_allocation">large enough bins can be allocated statically with a minimal waste of space</a>.
I wanted some actual non-asymptotic numbers, so I 
<a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/balls-and-bins.ispc">ran numerical experiments</a> and got the following distribution of
global utilisation (fill rate) when the first bin fills up.</p>

<p><a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid.png">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid-small.png" />
</a></p>

<p>It looks like, even with one thousand bins of thirty thousand values,
we can expect almost 98% space utilisation until the first bin
saturates. I want something more formal.</p>

<p>Could I establish something like a service level objective, “When
distributing balls randomly between one thousand bins with individual
capacity of thirty thousand balls, we can utilise at least 98% of the
total space before a bin fills up, x% of the time?”</p>

<p>The natural way to compute the “x%” that makes the proposition
true is to first fit a distribution on the observed data, then find
out the probability mass for that distribution that lies above 98%
fill rate. Fitting distributions takes a lot of judgment, and I’m not
sure I trust myself that much.</p>

<p>Alternatively, we can observe independent
identically distributed fill rates, check if they achieve 98% space
utilisation, and bound the success rate for this
<a href="https://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli process</a>.</p>

<p>There are some non-trivial questions associated with this approach.</p>

<ol>
  <li>How do we know when to stop generating more observations… without
fooling ourselves with \(p\)-hacking?</li>
  <li>How can we generate something like a confidence interval for the
success rate?</li>
</ol>

<p>Thankfully, I have been sitting on a
<a href="https://github.com/pkhuong/csm">software package</a>
to compute satisfaction rate for exactly this kind of <a href="https://landing.google.com/sre/book/chapters/service-level-objectives.html">SLO</a>-type properties,
properties of the form “this indicator satisfies $PREDICATE x% of the
time,” with arbitrarily bounded false positive rates.</p>

<p>The code takes care of adaptive stopping, generates a credible
interval, and spits out a report like this <a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/1k-30k-report.svg"><img src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/1k-30k-report.svg" width="100px" style="vertical-align: middle" /></a>:
we see the threshold (0.98), the empirical success rate estimate
(0.993 ≫ 0.98), a credible interval for the success rate, and
the shape of the probability mass for success rates.</p>

<p>This post shows how to compute credible intervals for the Bernoulli’s
success rate, how to implement a dynamic stopping criterion, and how
to combine the two while compensating for multiple hypothesis
testing. It also gives two examples of converting more general
questions to SLO form, and answers them with the same code.</p>

<h1 id="credible-intervals-for-the-binomial">Credible intervals for the Binomial</h1>

<p>If we run the same experiment \(n\) times, and observe \(a\)
successes (\(b = n - a\) failures), it’s natural to ask for
an estimate of the success rate \(p\) for the underlying 
<a href="https://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli process</a>,
assuming the observations are independent and identically distributed.</p>

<p>Intuitively, that estimate should be close to \(a / n\), the
empirical success rate, but that’s not enough. I also want something
that reflects the uncertainty associated with small \(n\), much like
in the following 
<a href="http://serialmentor.com/blog/2017/9/15/goodbye-joyplots">ridge line plot</a>,
where different phrases are assigned not only a different average
probability, but also a different spread.</p>

<p><a href="https://github.com/zonination/perceptions">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/perception-probability.png" />
</a></p>

<p>I’m looking for an interval of plausible success rates \(p\) that
responds to both the empirical success rate \(a / n\) and the sample
size \(n\); that interval should be centered around \(a / n\), be
wide when \(n\) is small, and become gradually tighter as \(n\)
increases.</p>

<p>The Bayesian approach is straightforward, if we’re willing to shut up
and calculate. Once we fix the underlying success rate \(p = \hat{p}\),
the conditional probability of observing \(a\) successes and \(b\)
failures is</p>

<p><span>
\[P((a, b) | p = \hat{p}) \sim \hat{p}^{a} \cdot (1 - \hat{p})^{b},\]
</span>
where the right-hand side is a proportion<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, rather than a
probability.</p>

<p>We can now apply 
<a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’s theorem</a> to 
invert the condition and the event. The inversion will give us the
conditional probability that \(p = \hat{p}\), given that we observed
\(a\) successes and \(b\) successes. We only need to impose a
prior distribution on the underlying rate \(p\). For simplicity,
I’ll go with the uniform \(U[0, 1]\), i.e., every success rate is
equally plausible, at first. We find</p>

<p><span>
\[P(p = \hat{p} | (a, b)) = \frac{P((a, b) | p = \hat{p}) P(p = \hat{p})}{P(a, b)}.\]
</span></p>

<p>We already picked the uniform prior, 
\(P(p = \hat{p}) = 1,\quad\forall\, \hat{p}\in [0,1],\)
and the denominator is a constant with respect to \(\hat{p}\).
The expression simplifies to</p>

<p><span>
\[P(p = \hat{p} | (a, b)) \sim \hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b},\]
</span>
or, if we normalise to obtain a probability,</p>

<p><span>
\[P(p = \hat{p} | (a, b)) = \frac{\hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b}}{\int\sb{0}\sp{1} \hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b}\, d\hat{p}} = \textrm{Beta}(a+1, b+1).\]
</span></p>

<p>A bit of calculation, and we find that our credibility estimate for
the underlying success rate follows a
<a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>. If
one is really into statistics, they can observe that the uniform prior
distribution is just the \(\textrm{Beta}(1, 1)\) distribution, and
rederive that the Beta is the 
<a href="https://en.wikipedia.org/wiki/Conjugate_prior_distribution">conjugate distribution</a>
 for the 
<a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a>
distribution.</p>

<p>For me, it suffices to observe that the distribution
\(\textrm{Beta}(a+1, b+1)\) is unimodal, does peak around
\(a / (a + b)\), and becomes tighter as the number of observations
grows. In the following image, I plotted three Beta distributions, all
with empirical success rate 0.9; red corresponds to \(n = 10\)
(\(a = 9\), \(b = 1\), \(\textrm{Beta}(10, 2)\)), black to 
\(n = 100\) (\(\textrm{Beta}(91, 11)\)), and blue to \(n =
1000\) (\(\textrm{Beta}(901, 101)\)).</p>

<p><a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/credible-beta-large.png">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/credible-beta.png" />
</a></p>

<p>We calculated, and we got something that matches my
intuition. Before trying to understand <em>what it means</em>, let’s take a
detour to simply plot points from that un-normalised proportion function
\(\hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b}\), on an arbitrary \(y\)
axis.</p>

<p>Let \(\hat{p} = 0.4\), \(a = 901\), \(b = 101\). Naïvely
entering the expression at the REPL yields nothing useful.</p>

<pre><code>CL-USER&gt; (* (expt 0.4d0 901) (expt (- 1 0.4d0) 101))
0.0d0
</code></pre>

<p>The issue here is that the un-normalised proportion is so small that
it underflows double floats and becomes a round zero. We can
guess that the normalisation factor \(\frac{1}{\mathrm{Beta}(\cdot,\cdot)}\)
quickly grows very large, which will bring its own set of issues
when we do care about the normalised probability.</p>

<p>How can we renormalise a set of points without underflow? The usual
trick to handle extremely small or large magnitudes is to work in the
log domain. Rather than computing
\(\hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b}\), we shall compute</p>

<p><span>
\[\log\left[\hat{p}\sp{a} \cdot (1 - \hat{p})\sp{b}\right] = a \log\hat{p} + b \log (1 - \hat{p}).\]
</span></p>

<pre><code>CL-USER&gt; (+ (* 901 (log 0.4d0)) (* 101 (log (- 1 0.4d0))))
-877.1713374189787d0
CL-USER&gt; (exp *)
0.0d0
</code></pre>

<p>That’s somewhat better: the log-domain value is not \(-\infty\), but
converting it back to a regular value still gives us 0.</p>

<p>The \(\log\) function is monotonic, so we can find the maximum
proportion value for a set of points, and divide everything by that
maximum value to get plottable points. There’s one last thing that
should change: when \(x\) is small, \(1 - x\) will round most of
\(x\) away.
<a href="https://www.johndcook.com/blog/2010/06/07/math-library-functions-that-seem-unnecessary/">Instead of <code>(log (- 1 x))</code>, we should use <code>(log1p (- x))</code></a>
to compute \(\log (1 + -x) = \log (1 - x)\). Common 
Lisp did not standardise <a href="https://linux.die.net/man/3/log1p"><code>log1p</code></a>,
but SBCL does have it in internals, as a wrapper around <code>libm</code>. We’ll
just abuse that for now.</p>

<pre><code>CL-USER&gt; (defun proportion (x) (+ (* 901 (log x)) (* 101 (sb-kernel:%log1p (- x)))))
PROPORTION
CL-USER&gt; (defparameter *points* (loop for i from 1 upto 19 collect (/ i 20d0)))
*POINTS*
CL-USER&gt; (reduce #'max *points* :key #'proportion)
-327.4909190001001d0
</code></pre>

<p>We have to normalise in the log domain, which is simply a subtraction:
\(\log(x / y) = \log x - \log y\). In the case above, we will subtract
\(-327.49\ldots\), or add <em>a massive</em> \(327.49\ldots\) to each log
proportion (i.e., multiply by \(10\sp{142}\)). The resulting values
should have a reasonably non-zero range.</p>

<pre><code>CL-USER&gt; (mapcar (lambda (x) (cons x (exp (- (proportion x) *)))) *points*)
((0.05d0 . 0.0d0)
 (0.1d0 . 0.0d0)
 [...]
 (0.35d0 . 3.443943164733533d-288)
 [...]
 (0.8d0 . 2.0682681158181894d-16) 
 (0.85d0 . 2.6252352579425913d-5)
 (0.9d0 . 1.0d0)
 (0.95d0 . 5.65506756824607d-10))
</code></pre>

<p>There’s finally some signal in there. This is still just an
un-normalised proportion function, not a probability density function,
but that’s already useful to show the general shape of the density
function, something like the following, for \(\mathrm{Beta}(901, 101)\).</p>

<p><a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/proportion.svg">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/proportion.svg" />
</a></p>

<p>Finally, we have a probability density function for the Bayesian update of
our belief about the success rate after \(n\) observations of a
Bernoulli process, and we know how to compute its proportion
function. Until now, I’ve carefully avoided the question of what all
these computations even mean. No more (:</p>

<p>The Bayesian view assumes that the underlying success rate
(the value we’re trying to estimate) is unknown, but sampled from some
distribution. In our case, we assumed a uniform
distribution, i.e., that every success rate is <em>a priori</em> equally
likely. We then observe \(n\) outcomes (successes or failures), and
assign an updated probability to each success rate. It’s like
a many-world interpretation in which we assume we live in one of a set of
worlds, each with a success rate sampled from the uniform
distribution; after observing 900 successes and 100 failures,
we’re more likely to be in a world where the success rate is 0.9 than
in one where it’s 0.2. With Bayes’s theorem to formalise the update,
we assign posterior probabilities to each potential success rate
value.</p>

<p>We can compute an
<a href="https://en.wikipedia.org/wiki/Credible_interval">equal-tailed credible interval</a>
from that 
\(\mathrm{Beta}(a+1,b+1)\)
posterior distribution by excluding the left-most values, \([0,
l)\), such that the 
<a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta</a> CDF 
(<a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">cumulative distribution function</a>) 
at \(l\) is \(\varepsilon / 2\), and doing
the same with the right most values to cut away \(\varepsilon / 2\) of the
probability density. The CDF for \(\mathrm{Beta}(a+1,b+1)\) at
\(x\) is the <a href="https://dlmf.nist.gov/8.17">incomplete beta function</a>,
\(I\sb{x}(a+1,b+1)\). That
function is really hard to compute (<a href="http://www.dtic.mil/dtic/tr/fulltext/u2/a210118.pdf">this technical report</a> 
detailing <a href="https://dl.acm.org/citation.cfm?id=131776">Algorithm 708</a>
deploys <em>five</em> different evaluation strategies), so I’ll address that
later.</p>

<p>The more orthodox “frequentist” approach to confidence intervals
treats the whole experiment, from data colleaction to analysis (to
publication, independent of the observations 😉) as an 
<a href="https://en.wikipedia.org/wiki/Atlantic_City_algorithm">Atlantic City algorithm</a>:
if we allow a false positive rate of \(\varepsilon\) (e.g.,
\(\varepsilon=5\%\)), the experiment must return a confidence
interval that includes the actual success rate (population statistic
or parameter, in general) with probability \(1 - \varepsilon\), for
any actual success rate (or underlying population statistic /
parameter). When the procedure fails, with probability at most
\(\varepsilon\), it is allowed to fail in an arbitrary manner.</p>

<p>The same Atlantic City logic applies to \(p\)-values. An experiment
(data collection and analysis) that accepts when the \(p\)-value is
at most \(0.05\) is an Atlantic City algorithm that returns a
correct result (including “don’t know”) with probability at least
\(0.95\), and is otherwise allowed to yield any result with
probability at most \(0.05\). The \(p\)-value associated with a
conclusion, e.g., “success rate is more than 0.8” (the confidence level
associated with an interval) means something like “I’m pretty sure that the
success rate is more than 0.8, because the odds of observing our data
if that were false are small (less than 0.05).” If we set that
threshold (of 0.05, in the example) <em>ahead of time</em>, we get an
Atlantic City algorithm to determine if “the success rate is more than
0.8” with failure probability 0.05. (In practice,
reporting is censored in all sorts of ways, so…)</p>

<p>There are ways to recover a classical confidence interval, given \(n\) observations from a Bernoulli. However,
they’re pretty convoluted, and, as 
<a href="https://bayes.wustl.edu/etj/articles/confidence.pdf">Jaynes argues in his note on confidence intervals</a>, the classical
approach gives values that are roughly the same<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> as the Bayesian
approach… so I’ll just use the Bayesian credibility interval instead.</p>

<p><em>See <a href="https://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval/2287#2287">this stackexchange post</a> 
for a lot more details.</em></p>

<h1 id="dynamic-stopping-for-binomial-testing">Dynamic stopping for Binomial testing</h1>

<p>The way statistics are usually deployed is that someone collects a
data set, as rich as is practical, and squeezes that static data set
dry for significant results. That’s exactly the setting for the
credible interval computation I sketched in the previous section.</p>

<p>When studying the properties of computer programs or systems, we can
usually generate additional data on demand, given more time. The problem
is knowing when it’s ok to stop wasting computer time, because we have
enough data… and how to determine that without running into multiple
hypothesis testing issues (<a href="https://www.google.com/search?q=a%2Fb+test+early+stopping">ask anyone who’s run A/B tests</a>).</p>

<p>Here’s an example of an intuitive but completely broken dynamic
stopping criterion. Let’s say we’re trying to find out if the success
rate is less than or greater than 90%, and are willing to be wrong 5%
of the time. We could get \(k\) data points, run a statistical test
on those data points, and stop if the data let us conclude with 95%
confidence that the underlying success rate differs from
90%. Otherwise, collect \(2k\) fresh points, run the same test;
collect \(4k, \ldots, 2\sp{i}k\) points. Eventually, we’ll have enough data.</p>

<p>The issue is that each time we execute the statistical test that
determines if we should stop, we run a 5% risk of being totally
wrong. For an extreme example, if the success rate is exactly 90%, we
will eventually stop, with probability 1. When we do stop, we’ll
inevitably conclude that the success rate differs from 90%, and we
will be wrong. The worst-case (over all underlying success rates)
false positive rate is 100%, not 5%!</p>

<p>In my experience, programmers tend to sidestep the question by wasting
CPU time with a large, fixed, number of iterations…  people are then
less likely to run our statistical tests, since they’re so slow, and
everyone loses (the other popular option is to impose a reasonable CPU
budget, with error thresholds so lax we end up with a smoke test).</p>

<p>Robbins, in 
<a href="https://projecteuclid.org/euclid.aoms/1177696786">Statistical Methods Related to the Law of the Iterated Logarithm</a>,
introduces a criterion that, given a threshold success rate \(p\) and a
sequence of (infinitely many!) observations from the same Bernoulli with
unknown success rate parameter, will be satisfied infinitely often when
\(p\) differs from the Bernoulli’s success rate. Crucially,
Robbins also bounds the false positive rate, the probability that the
criterion be satisfied <em>even once in the infinite sequence of
observations</em> if the Bernoulli’s unknown success rate is exactly equal
to \(p\). That criterion is</p>

<p><span>
\[{n \choose a} p\sp{a} (1-p)\sp{n-a} \leq \frac{\varepsilon}{n+1},\]
</span></p>

<p>where \(n\) is the number of observations, \(a\) the number of
successes, \(p\) the threshold success rate, and \(\varepsilon\)
the error (false positive) rate. As the number of
observation grows, the criterion becomes more and more stringent to
maintain a bounded false positive rate over the whole infinite
sequence of observations.</p>

<p>There are similar “Confidence Sequence” results for other
distributions (see, for example,
<a href="https://projecteuclid.org/euclid.aos/1176343406">this paper of Lai</a>),
but we only care about the Binomial here.</p>

<p>More recently, <a href="https://arxiv.org/abs/1611.01675">Ding, Gandy, and Hahn</a>
showed that Robbins’s criterion also guarantees that, when it is
satisfied, the empirical success rate (\(a/n\)) lies on the correct
side of the threshold \(p\) (same side as the actual unknown success
rate) with probability \(1-\varepsilon\). This result leads
them to propose the use of Robbins’s criterion to stop Monte Carlo
statistical tests, which they refer to as the Confidence Sequence Method (CSM).</p>

<pre><code>(defun csm-stop-p (successes failures threshold eps)
  "Pseudocode, this will not work on a real machine."
  (let ((n (+ successes failures)))
    (&lt;= (* (choose n successes) 
           (expt threshold successes)
           (expt (- 1 threshold) failures))
        (/ eps (1+ n)))))
</code></pre>

<p>We may call this predicate at any time with more independent and
identically distributed results, and stop as soon as it returns true.</p>

<p>The CSM is simple (it’s all in Robbins’s criterion), but still
provides good guarantees. The downside is that it is conservative when
we have a limit on the number of observations: the method “hedges”
against the possibility of having a false positive in the infinite
number of observations after the limit, observations we will never
make. For computer-generated data sets, I think having a principled
limit is pretty good; it’s not ideal to ask for more data than
strictly necessary, but not a blocker either.</p>

<p>In practice, there are still real obstacles to implementing the CSM on
computers with finite precision (floating point) arithmetic,
especially since I want to preserve the method’s theoretical guarantees
(i.e., make sure rounding is one-sided to overestimate the left-hand
side of the inequality).</p>

<p>If we implement the expression well, the effect of rounding on
correctness should be less than marginal. However, I don’t want to be
stuck wondering if my bad results are due to known approximation
errors in the method, rather than errors in the code. Moreover, if we
do have a tight expression with little rounding errors, adjusting it
to make the errors one-sided should have almost no impact. That seems
like a good trade-off to me, especially if I’m going to use the CSM
semi-automatically, in continuous integration scripts, for example.</p>

<p>One look at <code>csm-stop-p</code> shows we’ll have the same problem we had with
the proportion function for the Beta distribution: we’re multiplying
very small and very large values. We’ll apply the same fix: work in
the log domain and exploit \(\log\)’s monotonicity.</p>

<p><span>
\[{n \choose a} p\sp{a} (1-p)\sp{n-a} \leq \frac{\varepsilon}{n+1}\]
</span></p>

<p>becomes</p>

<p><span>
\[\log {n \choose a} + a \log p + (n-a)\log (1-p) \leq \log\varepsilon -\log(n+1),\]
</span>
or, after some more expansions, and with \(b = n - a\),</p>

<p><span>
\[\log n! - \log a! - \log b! + a \log p + b \log(1 - p) + \log(n+1) \leq \log\varepsilon.\]
</span></p>

<p>The new obstacle is computing the factorial \(x!\), or the
log-factorial \(\log x!\). We shouldn’t compute the
factorial iteratively: otherwise, we could spend more time in the stopping criterion than in the data generation subroutine.
<a href="https://www-fourier.ujf-grenoble.fr/~marin/une_autre_crypto/articles_et_extraits_livres/Robbin_H.-A_remark_on_Stirling%5C's_Formula.pdf">Robbins has another useful result</a>
for us:</p>

<p><span>
\[\sqrt{2\pi} n\sp{n + 1/2} \exp(-n) \exp\left(\frac{1}{12n+1}\right) &lt; n! &lt; \sqrt{2\pi} n\sp{n + 1/2} \exp(-n) \exp\left(\frac{1}{12n}\right),\]
</span></p>

<p>or, in the log domain,</p>

<p><span>
\[\log\sqrt{2\pi} + \left(n + \frac{1}{2}\right)\log n -n + \frac{1}{12n+1} &lt; \log n! &lt; \log\sqrt{2\pi} + \left(n + \frac{1}{2}\right)\log n -n +\frac{1}{12n}.\]
</span></p>

<p>This double inequality gives us a way to over-approximate 
\(\log {n \choose a} = \log \frac{n!}{a! b!} = \log n! - \log a! - \log b!,\)
where \(b = n - a\):</p>

<p><span>
\[\log {n \choose a} &lt; -\log\sqrt{2\pi} + \left(n + \frac{1}{2}\right)\log n -n +\frac{1}{12n} - \left(a + \frac{1}{2}\right)\log a +a - \frac{1}{12a+1}  - \left(b + \frac{1}{2}\right)\log b +b - \frac{1}{12b+1},\]
</span></p>

<p>where the right-most expression in Robbins’s double inequality
replaces \(\log n!\), which must be over-approximated, and the
left-most \(\log a!\) and \(\log b!\), which must be
under-approximated.</p>

<p>Robbins’s approximation works well for us because, it is
one-sided, and guarantees that the (relative) error in \(n!\),
\(\frac{\exp\left(\frac{1}{12n}\right) - \exp\left(\frac{1}{12n+1}\right)}{n!},\)
is small, even for small values like \(n = 5\) 
(error \(&lt; 0.0023\%\)), and decreases with \(n\): as we perform
more trials, the approximation is increasingly accurate, thus less
likely to spuriously prevent us from stopping.</p>

<p>Now that we have a conservative approximation of Robbins’s criterion
that only needs the four arithmetic operations and logarithms (and
<code>log1p</code>), we can implement it on a real computer. The only challenge
left is regular floating point arithmetic stuff: if rounding must
occur, we must make sure it is in a safe (conservative) direction for
our predicate.</p>

<p>Hardware usually lets us manipulate the rounding mode to force
floating point arithmetic operations to round up or down, instead of
the usual round to even. However, that tends to be slow, so most
language (implementations) don’t support changing the rounding mode, or
do so badly…  which leaves us in a multi-decade
hardware/software co-evolution Catch-22.</p>

<p>I could think hard and derive tight bounds on the round-off error, but I’d
rather apply a bit of brute force. IEEE-754 compliant implementations
must round the four basic operations correctly. This means that
\(z = x \oplus y\) is at most half a ULP away from \(x + y,\) 
and thus either \(z = x \oplus y \geq x + y,\) or the next floating
point value after \(z,\) \(z^\prime \geq x + y\). We can find this
“next value” portably in Common Lisp, with
<code>decode-float</code>/<code>scale-float</code>, and some hand-waving for denormals.</p>

<pre><code>(defun next (x &amp;optional (delta 1))
  "Increment x by delta ULPs. Very conservative for
   small (0/denormalised) values."
  (declare (type double-float x)
           (type unsigned-byte delta))
  (let* ((exponent (nth-value 1 (decode-float x)))
         (ulp (max (scale-float double-float-epsilon exponent)
                   least-positive-normalized-double-float)))
    (+ x (* delta ulp))))
</code></pre>

<p>I prefer to manipulate IEEE-754 bits directly. That’s theoretically
not portable, but the platforms I care about make sure we can treat
floats as sign-magnitude integers.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>next </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
</pre></td><td class="code"><pre><code class="lisp"><span class="line"><span></span><span class="o">#+</span><span class="nv">sbcl</span>
</span><span class="line"><span class="p">(</span><span class="k">progn</span>
</span><span class="line">  <span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">inline</span> <span class="nv">%float-bits</span> <span class="nv">%bits-float</span> <span class="nv">next</span> <span class="nv">prev</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">%float-bits</span> <span class="p">(</span><span class="nv">x</span><span class="p">)</span>
</span><span class="line">    <span class="s">&quot;Convert a double float x to sign-extended sign/magnitude, and</span>
</span><span class="line"><span class="s">     then to 2&#39;s complement.&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="k">let*</span> <span class="p">((</span><span class="nv">hi</span> <span class="p">(</span><span class="nv">sb-kernel:double-float-high-bits</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">           <span class="p">(</span><span class="nv">lo</span> <span class="p">(</span><span class="nv">sb-kernel:double-float-low-bits</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">           <span class="p">(</span><span class="nv">word</span> <span class="p">(</span><span class="nb">+</span> <span class="p">(</span><span class="nb">ash</span> <span class="p">(</span><span class="nb">ldb</span> <span class="p">(</span><span class="nb">byte</span> <span class="mi">31</span> <span class="mi">0</span><span class="p">)</span> <span class="nv">hi</span><span class="p">)</span> <span class="mi">32</span><span class="p">)</span> <span class="nv">lo</span><span class="p">)))</span>
</span><span class="line">      <span class="c1">;; hi is the high half of the 64 bit sign-magnitude</span>
</span><span class="line">      <span class="c1">;; representation… in two&#39;s complement. Extract the significand,</span>
</span><span class="line">      <span class="c1">;; and then apply the sign bit. We want to preserve signed zeros,</span>
</span><span class="line">      <span class="c1">;; so return -1 - word instead of -word.</span>
</span><span class="line">      <span class="c1">;;</span>
</span><span class="line">      <span class="c1">;; (- -1 word) = (lognot word) = (logxor word -1).</span>
</span><span class="line">      <span class="p">(</span><span class="nb">logxor</span> <span class="nv">word</span> <span class="p">(</span><span class="nb">ash</span> <span class="nv">hi</span> <span class="mi">-32</span><span class="p">))))</span>
</span><span class="line">
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">%bits-float</span> <span class="p">(</span><span class="nv">bits</span><span class="p">)</span>
</span><span class="line">    <span class="s">&quot;Convert 2&#39;s complement to sign-extended sign/magnitude, then</span>
</span><span class="line"><span class="s">     double float.&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="p">(</span><span class="kt">signed-byte</span> <span class="mi">64</span><span class="p">)</span> <span class="nv">bits</span><span class="p">))</span>
</span><span class="line">    <span class="c1">;; convert back to sign-magnitude: if bits is negative, all but the</span>
</span><span class="line">    <span class="c1">;; sign bit must be flipped again.</span>
</span><span class="line">    <span class="p">(</span><span class="k">let</span> <span class="p">((</span><span class="nv">bits</span> <span class="p">(</span><span class="nb">logxor</span> <span class="nv">bits</span>
</span><span class="line">                        <span class="p">(</span><span class="nb">ldb</span> <span class="p">(</span><span class="nb">byte</span> <span class="mi">63</span> <span class="mi">0</span><span class="p">)</span> <span class="p">(</span><span class="nb">ash</span> <span class="nv">bits</span> <span class="mi">-64</span><span class="p">)))))</span>
</span><span class="line">      <span class="p">(</span><span class="nv">sb-kernel:make-double-float</span> <span class="p">(</span><span class="nb">ash</span> <span class="nv">bits</span> <span class="mi">-32</span><span class="p">)</span>
</span><span class="line">                                   <span class="p">(</span><span class="nb">ldb</span> <span class="p">(</span><span class="nb">byte</span> <span class="mi">32</span> <span class="mi">0</span><span class="p">)</span> <span class="nv">bits</span><span class="p">))))</span>
</span><span class="line">
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">next</span> <span class="p">(</span><span class="nv">x</span> <span class="k">&amp;optional</span> <span class="p">(</span><span class="nv">delta</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="s">&quot;Increment x by delta ULPs.&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">)</span>
</span><span class="line">             <span class="p">(</span><span class="k">type</span> <span class="kt">unsigned-byte</span> <span class="nv">delta</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nv">%bits-float</span> <span class="p">(</span><span class="nb">+</span> <span class="p">(</span><span class="nv">%float-bits</span> <span class="nv">x</span><span class="p">)</span> <span class="nv">delta</span><span class="p">)))</span>
</span><span class="line">
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">prev</span> <span class="p">(</span><span class="nv">x</span> <span class="k">&amp;optional</span> <span class="p">(</span><span class="nv">delta</span> <span class="mi">1</span><span class="p">))</span>
</span><span class="line">    <span class="s">&quot;Decrement x by delta ULPs.&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">)</span>
</span><span class="line">             <span class="p">(</span><span class="k">type</span> <span class="kt">unsigned-byte</span> <span class="nv">delta</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nv">%bits-float</span> <span class="p">(</span><span class="nb">-</span> <span class="p">(</span><span class="nv">%float-bits</span> <span class="nv">x</span><span class="p">)</span> <span class="nv">delta</span><span class="p">))))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<pre><code>CL-USER&gt; (double-float-bits pi)
4614256656552045848
CL-USER&gt; (double-float-bits (- pi))
-4614256656552045849
</code></pre>

<p>The two’s complement value for <code>pi</code> is one less than 
<code>(- (double-float-bits pi))</code> because two’s complement does not support
signed zeros.</p>

<pre><code>CL-USER&gt; (eql 0 (- 0))
T
CL-USER&gt; (eql 0d0 (- 0d0))
NIL
CL-USER&gt; (double-float-bits 0d0)
0
CL-USER&gt; (double-float-bits -0d0)
-1
</code></pre>

<p>We can quickly check that the round trip from float to integer and back
is an identity.</p>

<pre><code>CL-USER&gt; (eql pi (bits-double-float (double-float-bits pi)))
T
CL-USER&gt; (eql (- pi) (bits-double-float (double-float-bits (- pi))))
T
CL-USER&gt; (eql 0d0 (bits-double-float (double-float-bits 0d0)))
T
CL-USER&gt; (eql -0d0 (bits-double-float (double-float-bits -0d0)))
T
</code></pre>

<p>We can also check that incrementing or decrementing the integer
representation does increase or decrease the floating point value.</p>

<pre><code>CL-USER&gt; (&lt; (bits-double-float (1- (double-float-bits pi))) pi)
T
CL-USER&gt; (&lt; (bits-double-float (1- (double-float-bits (- pi)))) (- pi))
T
CL-USER&gt; (bits-double-float (1- (double-float-bits 0d0)))
-0.0d0
CL-USER&gt; (bits-double-float (1+ (double-float-bits -0d0)))
0.0d0
CL-USER&gt; (bits-double-float (1+ (double-float-bits 0d0)))
4.9406564584124654d-324
CL-USER&gt; (bits-double-float (1- (double-float-bits -0d0)))
-4.9406564584124654d-324
</code></pre>

<p>The code doesn’t handle special values like infinities or NaNs, but
that’s out of scope for the CSM criterion anyway. That’s all we need
to nudge the result of the four operations to guarantee an over- or
under- approximation of the real value. We can also look at the
documentation for our <code>libm</code> (e.g., <a href="https://www.gnu.org/software/libc/manual/html_node/Errors-in-Math-Functions.html">for GNU libm</a>) 
to find error bounds on functions like <code>log</code>; GNU claims their
<code>log</code> is never off by more than 3 ULP. We can round up to the
fourth next floating point value to obtain a conservative upper bound
on \(\log x\).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>log </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
</pre></td><td class="code"><pre><code class="lisp"><span class="line"><span></span><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">type</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">31</span><span class="p">)</span> <span class="vg">*libm-error-limit*</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defvar</span> <span class="vg">*libm-error-limit*</span> <span class="mi">4</span>
</span><span class="line">  <span class="s">&quot;Assume libm is off by less than 4 ULPs.&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">inline</span> <span class="nv">log-up</span> <span class="nv">log-down</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">log-up</span> <span class="p">(</span><span class="nv">x</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Conservative upper bound on log(x).&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">log</span> <span class="nv">x</span><span class="p">)</span> <span class="vg">*libm-error-limit*</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">log-down</span> <span class="p">(</span><span class="nv">x</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Conservative lower bound on log(x).&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nv">prev</span> <span class="p">(</span><span class="nb">log</span> <span class="nv">x</span><span class="p">)</span> <span class="vg">*libm-error-limit*</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="o">#+</span><span class="nv">sbcl</span>
</span><span class="line"><span class="p">(</span><span class="k">progn</span>
</span><span class="line">  <span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">inline</span> <span class="nv">log1p-up</span> <span class="nv">log1p-down</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">log1p-up</span> <span class="p">(</span><span class="nv">x</span><span class="p">)</span>
</span><span class="line">    <span class="s">&quot;Convervative upper bound on log(1 + x).&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nv">sb-kernel:%log1p</span> <span class="nv">x</span><span class="p">)</span> <span class="vg">*libm-error-limit*</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">  <span class="p">(</span><span class="nb">defun</span> <span class="nv">log1p-down</span> <span class="p">(</span><span class="nv">x</span><span class="p">)</span>
</span><span class="line">    <span class="s">&quot;Conservative lower bound on log(1 + x)&quot;</span>
</span><span class="line">    <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">x</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nv">prev</span> <span class="p">(</span><span class="nv">sb-kernel:%log1p</span> <span class="nv">x</span><span class="p">)</span> <span class="vg">*libm-error-limit*</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I could go ahead and use the building blocks above (ULP nudging for
directed rounding) to directly implement Robbins’s criterion,</p>

<p><span>
\[\log {n \choose a} + a \log p + b\log (1-p) + \log(n+1) \leq \log\varepsilon,\]
</span></p>

<p>with Robbins’s factorial approximation,</p>

<p><span>
\[\log {n \choose a} &lt; -\log\sqrt{2\pi} + \left(n + \frac{1}{2}\right)\log n -n +\frac{1}{12n} - \left(a + \frac{1}{2}\right)\log a +a - \frac{1}{12a+1}  - \left(b + \frac{1}{2}\right)\log b +b - \frac{1}{12b+1}.\]
</span></p>

<p>However, even in the log domain, there’s a lot of cancellation: we’re
taking the difference of relatively large numbers to find a small
result. It’s possible to avoid that by re-associating some of the
terms above, e.g., for \(a\):</p>

<p><span>
\[-\left(a + \frac{1}{2}\right) \log a + a - a \log p = 
   -\frac{\log a}{2} + a (-\log a + 1 - \log p).\]
</span></p>

<p>Instead, I’ll just brute force things (again) with 
<a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm">Kahan summation</a>.
Shewchuk’s presentation in <a href="http://www.cs.cmu.edu/~quake/robust.html">Adaptive Precision Floating-Point Arithmetic and Fast Robust Geometric Predicates</a>
highlights how the only step where we may lose precision to
rounding is when we add the current compensation term to the new
summand. We can implement Kahan summation with directed rounding in
only that one place: all the other operations are exact!</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>"kahan summation" </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
</pre></td><td class="code"><pre><code class="lisp"><span class="line"><span></span><span class="c1">;;; Kahan-style summation.</span>
</span><span class="line"><span class="c1">;;;</span>
</span><span class="line"><span class="c1">;;; Represent the accumulator as an evaluated sum of two doubles. As</span>
</span><span class="line"><span class="c1">;;; long as the compensation term is initially 0, the result is a safe</span>
</span><span class="line"><span class="c1">;;; upper bound on the real value, and the two terms are</span>
</span><span class="line"><span class="c1">;;; &quot;non-overlapping.&quot;  For more details, see &quot;Adaptive Precision</span>
</span><span class="line"><span class="c1">;;; Floating-Point Arithmetic and Fast Robust Geometric Predicates&quot;,</span>
</span><span class="line"><span class="c1">;;; Shewchuk, 1997; Technical report CMU-CS-96-140R / Discrete &amp; Comp</span>
</span><span class="line"><span class="c1">;;; Geom 18(3), October 1997. Theorem 6 in particular.</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">inline</span> <span class="nv">sum-update-up</span> <span class="nv">sum-update-finish</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">sum-update-up</span> <span class="p">(</span><span class="nv">accumulator</span> <span class="nv">compensation</span> <span class="nv">term</span> <span class="k">&amp;optional</span> <span class="nv">ordered</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Given an evaluated sum</span>
</span><span class="line"><span class="s">     (accumulator + compensation),</span>
</span><span class="line"><span class="s">   return a new unevaluated sum for an upper bound on</span>
</span><span class="line"><span class="s">     (accumulator + compensation + term).</span>
</span><span class="line">
</span><span class="line"><span class="s">   If ordered, assume</span>
</span><span class="line"><span class="s">     term &lt; accumulator,</span>
</span><span class="line"><span class="s">   or</span>
</span><span class="line"><span class="s">     accumulator = compensation = 0.&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">accumulator</span> <span class="nv">compensation</span>
</span><span class="line">                 <span class="nv">term</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">when</span> <span class="p">(</span><span class="nb">and</span> <span class="p">(</span><span class="nb">not</span> <span class="nv">ordered</span><span class="p">)</span>
</span><span class="line">             <span class="p">(</span><span class="nb">&lt;</span> <span class="p">(</span><span class="nb">abs</span> <span class="nv">accumulator</span><span class="p">)</span> <span class="p">(</span><span class="nb">abs</span> <span class="nv">term</span><span class="p">)))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">rotatef</span> <span class="nv">accumulator</span> <span class="nv">term</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="k">let*</span> <span class="p">((</span><span class="nv">rest-1</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">compensation</span> <span class="nv">term</span><span class="p">)))</span> <span class="c1">; safe upper bound on c + t</span>
</span><span class="line">         <span class="p">(</span><span class="nb">rest</span> <span class="p">(</span><span class="k">if</span> <span class="p">(</span><span class="nb">&lt;=</span> <span class="nv">compensation</span> <span class="mf">0d0</span><span class="p">)</span>       <span class="c1">; tighter, still safe.</span>
</span><span class="line">                   <span class="p">(</span><span class="nb">min</span> <span class="nv">term</span> <span class="nv">rest-1</span><span class="p">)</span>
</span><span class="line">                   <span class="nv">rest-1</span><span class="p">))</span>
</span><span class="line">         <span class="c1">;; Perform a Dekker sum of accumulator + rest. The result is</span>
</span><span class="line">         <span class="c1">;; exact, so no need for next/prev here.</span>
</span><span class="line">         <span class="c1">;;</span>
</span><span class="line">         <span class="c1">;; Precondition: |accumulator| &gt;= |rest| (or accumulator = 0).</span>
</span><span class="line">         <span class="p">(</span><span class="nv">a</span> <span class="nv">accumulator</span><span class="p">)</span>
</span><span class="line">         <span class="p">(</span><span class="nv">b</span> <span class="nb">rest</span><span class="p">)</span>
</span><span class="line">         <span class="p">(</span><span class="nv">x</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">a</span> <span class="nv">b</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">b-virtual</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">x</span> <span class="nv">a</span><span class="p">))</span>     <span class="c1">; b-virtual = value really added to a</span>
</span><span class="line">         <span class="p">(</span><span class="nv">y</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">b</span> <span class="nv">b-virtual</span><span class="p">)))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">values</span> <span class="nv">x</span> <span class="nv">y</span><span class="p">)))</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">sum-update-finish</span> <span class="p">(</span><span class="nv">accumulator</span> <span class="nv">compensation</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Return a conservative upper bound for accumulator + compensation.</span>
</span><span class="line">
</span><span class="line"><span class="s">   In theory, (+ accumulator compensation) is equal to accumulator.</span>
</span><span class="line"><span class="s">   In practice, it doesn&#39;t hurt to do this right. The second return</span>
</span><span class="line"><span class="s">   value is the new compensation term (should never be positive).&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="k">declare</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="nv">accumulator</span> <span class="nv">compensation</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="k">let*</span> <span class="p">((</span><span class="nv">raw-sum</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">accumulator</span> <span class="nv">compensation</span><span class="p">)))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">sum</span> <span class="p">(</span><span class="k">if</span> <span class="p">(</span><span class="nb">&gt;</span> <span class="nv">compensation</span> <span class="mf">0d0</span><span class="p">)</span>
</span><span class="line">                  <span class="nv">raw-sum</span>
</span><span class="line">                  <span class="c1">;; if compensation &lt;= 0, acc is already an upper</span>
</span><span class="line">                  <span class="c1">;; bound.</span>
</span><span class="line">                  <span class="p">(</span><span class="nb">min</span> <span class="nv">accumulator</span> <span class="nv">raw-sum</span><span class="p">)))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">delta</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">sum</span> <span class="nv">accumulator</span><span class="p">)))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">assert</span> <span class="p">(</span><span class="nb">&gt;=</span> <span class="nv">delta</span> <span class="nv">compensation</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">values</span> <span class="nv">sum</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">compensation</span> <span class="nv">delta</span><span class="p">))))</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">ftype</span> <span class="p">(</span><span class="k">function</span> <span class="p">(</span><span class="k">&amp;rest</span> <span class="kt">double-float</span><span class="p">)</span>
</span><span class="line">                          <span class="p">(</span><span class="nb">values</span> <span class="kt">double-float</span> <span class="kt">double-float</span> <span class="k">&amp;optional</span><span class="p">))</span>
</span><span class="line">                <span class="nv">sum-up</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">sum-up</span> <span class="p">(</span><span class="k">&amp;rest</span> <span class="nb">values</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Conservative upper bound for the sum of values, with a Kahan</span>
</span><span class="line"><span class="s">   summation loop.&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="k">let</span> <span class="p">((</span><span class="nv">acc</span> <span class="mf">0d0</span><span class="p">)</span>
</span><span class="line">        <span class="p">(</span><span class="nv">err</span> <span class="mf">0d0</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">dolist</span> <span class="p">(</span><span class="nv">value</span> <span class="nb">values</span> <span class="p">(</span><span class="nv">sum-update-finish</span> <span class="nv">acc</span> <span class="nv">err</span><span class="p">))</span>
</span><span class="line">      <span class="p">(</span><span class="nb">setf</span> <span class="p">(</span><span class="nb">values</span> <span class="nv">acc</span> <span class="nv">err</span><span class="p">)</span>
</span><span class="line">            <span class="p">(</span><span class="nv">sum-update-up</span> <span class="nv">acc</span> <span class="nv">err</span> <span class="nv">value</span><span class="p">)))))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We need one last thing to implement \(\log {n \choose a}\), and then
Robbins’s confidence sequence: a safely rounded floating-point value
approximation of  \(-\log \sqrt{2 \pi}\). I precomputed one with 
<a href="https://github.com/tarballs-are-good/computable-reals">computable-reals</a>:</p>

<pre><code>CL-USER&gt; (computable-reals:-r
          (computable-reals:log-r
           (computable-reals:sqrt-r computable-reals:+2pi-r+)))
-0.91893853320467274178...
CL-USER&gt; (computable-reals:ceiling-r 
          (computable-reals:*r *
                               (ash 1 53)))
-8277062471433908
-0.65067431749790398594...
CL-USER&gt; (* -8277062471433908 (expt 2d0 -53))
-0.9189385332046727d0
CL-USER&gt; (computable-reals:-r (rational *)
                              ***)
+0.00000000000000007224...
</code></pre>

<p>We can safely replace \(-\log\sqrt{2\pi}\) with
<code>-0.9189385332046727d0</code>, or, equivalently, 
<code>(scale-float -8277062471433908.0d0 -53)</code>, for an upper bound.
If we wanted a lower bound, we could decrement the integer significand
by one.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>log-choose </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
</pre></td><td class="code"><pre><code class="lisp"><span class="line"><span></span><span class="c1">;;; Upper bound for log c(n, s).</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">type</span> <span class="kt">double-float</span> <span class="vg">*minus-log-sqrt-2pi*</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defvar</span> <span class="vg">*minus-log-sqrt-2pi*</span> <span class="mf">-0.9189385332046727d0</span>
</span><span class="line">  <span class="s">&quot;Smallest double precision value &gt; -log sqrt(2pi).&quot;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line"><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">ftype</span> <span class="p">(</span><span class="k">function</span> <span class="p">((</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">)</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">))</span>
</span><span class="line">                          <span class="p">(</span><span class="nb">values</span> <span class="kt">double-float</span> <span class="kt">double-float</span> <span class="k">&amp;optional</span><span class="p">))</span>
</span><span class="line">                <span class="nv">robbins-log-choose</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">robbins-log-choose</span> <span class="p">(</span><span class="nv">n</span> <span class="nv">s</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Compute a conservative upper bound on log c(n, s) based on</span>
</span><span class="line"><span class="s">   Robbins&#39;s bounds for k!.&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">n</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">))</span> <span class="c1">;; ensure 53 bit arith is exact.</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">s</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">assert</span> <span class="p">(</span><span class="nb">&lt;=</span> <span class="mi">0</span> <span class="nv">s</span> <span class="nv">n</span><span class="p">))</span>
</span><span class="line">  <span class="c1">;; Handle easy cases, where c(n, s) is 1 or n.</span>
</span><span class="line">  <span class="p">(</span><span class="nb">when</span> <span class="p">(</span><span class="nb">or</span> <span class="p">(</span><span class="nb">=</span> <span class="nv">n</span> <span class="nv">s</span><span class="p">)</span>
</span><span class="line">            <span class="p">(</span><span class="nb">zerop</span> <span class="nv">s</span><span class="p">))</span>
</span><span class="line">    <span class="p">(</span><span class="k">return-from</span> <span class="nv">robbins-log-choose</span> <span class="p">(</span><span class="nb">values</span> <span class="mf">0d0</span> <span class="mf">0d0</span><span class="p">)))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">when</span> <span class="p">(</span><span class="nb">or</span> <span class="p">(</span><span class="nb">=</span> <span class="nv">s</span> <span class="mi">1</span><span class="p">)</span>
</span><span class="line">            <span class="p">(</span><span class="nb">=</span> <span class="nv">s</span> <span class="p">(</span><span class="nb">1-</span> <span class="nv">n</span><span class="p">)))</span>
</span><span class="line">    <span class="p">(</span><span class="k">return-from</span> <span class="nv">robbins-log-choose</span> <span class="p">(</span><span class="nb">values</span> <span class="p">(</span><span class="nv">log-up</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">n</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">                                            <span class="mf">0d0</span><span class="p">)))</span>
</span><span class="line">  <span class="p">(</span><span class="k">let*</span> <span class="p">((</span><span class="nv">n</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">n</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">s</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">s</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">n-s</span> <span class="p">(</span><span class="nb">float</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">n</span> <span class="nv">s</span><span class="p">)</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">l1</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">*</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">n</span> <span class="mf">.5d0</span><span class="p">)</span> <span class="p">(</span><span class="nv">log-up</span> <span class="nv">n</span><span class="p">))))</span> <span class="c1">; (+ n .5d0) is exact.</span>
</span><span class="line">         <span class="p">(</span><span class="nv">l2</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">-</span> <span class="p">(</span><span class="nb">*</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">s</span> <span class="mf">.5d0</span><span class="p">)</span> <span class="p">(</span><span class="nv">log-down</span> <span class="nv">s</span><span class="p">)))))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">l3</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">-</span> <span class="p">(</span><span class="nb">*</span> <span class="p">(</span><span class="nb">+</span> <span class="nv">n-s</span> <span class="mf">.5d0</span><span class="p">)</span> <span class="p">(</span><span class="nv">log-down</span> <span class="nv">n-s</span><span class="p">)))))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">r1</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">/</span> <span class="p">(</span><span class="nb">*</span> <span class="mf">12d0</span> <span class="nv">n</span><span class="p">))))</span>          <span class="c1">; (* 12d0 n) is exact.</span>
</span><span class="line">         <span class="p">(</span><span class="nv">r2</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">-</span> <span class="p">(</span><span class="nb">/</span> <span class="p">(</span><span class="nb">1+</span> <span class="p">(</span><span class="nb">*</span> <span class="mf">12d0</span> <span class="nv">s</span><span class="p">))))))</span> <span class="c1">; also exact.</span>
</span><span class="line">         <span class="p">(</span><span class="nv">r3</span> <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">-</span> <span class="p">(</span><span class="nb">/</span> <span class="p">(</span><span class="nb">1+</span> <span class="p">(</span><span class="nb">*</span> <span class="mf">12d0</span> <span class="nv">n-s</span><span class="p">)))))))</span>
</span><span class="line">    <span class="p">(</span><span class="nv">sum-up</span> <span class="vg">*minus-log-sqrt-2pi*</span>
</span><span class="line">            <span class="nv">l1</span> <span class="nv">l2</span> <span class="nv">l3</span>
</span><span class="line">            <span class="nv">r1</span> <span class="nv">r2</span> <span class="nv">r3</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We can quickly check against an exact implementation with
<code>computable-reals</code> and a brute force factorial.</p>

<pre><code>CL-USER&gt; (defun cr-log-choose (n s)
           (computable-reals:-r
            (computable-reals:log-r (alexandria:factorial n))
            (computable-reals:log-r (alexandria:factorial s))
            (computable-reals:log-r (alexandria:factorial (- n s)))))
CR-LOG-CHOOSE
CL-USER&gt; (computable-reals:-r (rational (robbins-log-choose 10 5))
                              (cr-log-choose 10 5))
+0.00050526703375914436...
CL-USER&gt; (computable-reals:-r (rational (robbins-log-choose 1000 500))
                              (cr-log-choose 1000 500))
+0.00000005551513197557...
CL-USER&gt; (computable-reals:-r (rational (robbins-log-choose 1000 5))
                              (cr-log-choose 1000 5))
+0.00025125559085509706...
</code></pre>

<p>That’s not obviously broken: the error is pretty small, and always positive.</p>

<p>Given a function to over-approximate log-choose, the Confidence
Sequence Method’s stopping criterion is straightforward.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>csm </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
</pre></td><td class="code"><pre><code class="lisp"><span class="line"><span></span><span class="p">(</span><span class="nb">declaim</span> <span class="p">(</span><span class="k">ftype</span> <span class="p">(</span><span class="k">function</span> <span class="p">((</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">)</span>
</span><span class="line">                           <span class="p">(</span><span class="nc">real</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line">                           <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">)</span>
</span><span class="line">                           <span class="nc">real</span><span class="p">)</span>
</span><span class="line">                          <span class="p">(</span><span class="nb">values</span> <span class="kt">boolean</span> <span class="kt">double-float</span> <span class="k">&amp;optional</span><span class="p">))</span>
</span><span class="line">                <span class="nv">csm</span><span class="p">))</span>
</span><span class="line"><span class="p">(</span><span class="nb">defun</span> <span class="nv">csm</span> <span class="p">(</span><span class="nv">n</span> <span class="nv">alpha</span> <span class="nv">s</span> <span class="nv">log-eps</span><span class="p">)</span>
</span><span class="line">  <span class="s">&quot;Given n trials and s sucesses, are we reasonably sure that the</span>
</span><span class="line"><span class="s">  success rate is *not* alpha (with a false positive rate &lt; exp(log-eps))?</span>
</span><span class="line">
</span><span class="line"><span class="s">  Answer that question with Ding, Gandy, and Hahn&#39;s confidence</span>
</span><span class="line"><span class="s">  sequence method (CSM). The second return value is an estimate of the</span>
</span><span class="line"><span class="s">  false positive target rate we would need to stop here. This value</span>
</span><span class="line"><span class="s">  should only be used for reporting; the target rate eps should always</span>
</span><span class="line"><span class="s">  be fixed before starting the experiment.&quot;</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">n</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">alpha</span> <span class="p">(</span><span class="nc">real</span> <span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">s</span> <span class="p">(</span><span class="kt">unsigned-byte</span> <span class="mi">49</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="nb">check-type</span> <span class="nv">log-eps</span> <span class="nc">real</span><span class="p">)</span>
</span><span class="line">  <span class="p">(</span><span class="nb">assert</span> <span class="p">(</span><span class="nb">&lt;=</span> <span class="mi">0</span> <span class="nv">s</span> <span class="nv">n</span><span class="p">))</span>
</span><span class="line">  <span class="p">(</span><span class="k">let*</span> <span class="p">((</span><span class="nv">log-choose</span> <span class="p">(</span><span class="nv">robbins-log-choose</span> <span class="nv">n</span> <span class="nv">s</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">n</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">n</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">alpha</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">alpha</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">s</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">s</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">log-eps</span> <span class="p">(</span><span class="nb">float</span> <span class="nv">log-eps</span> <span class="mf">1d0</span><span class="p">))</span>
</span><span class="line">         <span class="p">(</span><span class="nv">log-level</span> <span class="p">(</span><span class="nv">sum-up</span> <span class="p">(</span><span class="nv">log-up</span> <span class="p">(</span><span class="nb">1+</span> <span class="nv">n</span><span class="p">))</span>
</span><span class="line">                            <span class="nv">log-choose</span>
</span><span class="line">                            <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">*</span> <span class="nv">s</span> <span class="p">(</span><span class="nv">log-up</span> <span class="nv">alpha</span><span class="p">)))</span>
</span><span class="line">                            <span class="p">(</span><span class="nv">next</span> <span class="p">(</span><span class="nb">*</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">n</span> <span class="nv">s</span><span class="p">)</span> <span class="p">(</span><span class="nv">log1p-up</span> <span class="p">(</span><span class="nb">-</span> <span class="nv">alpha</span><span class="p">)))))))</span>
</span><span class="line">    <span class="p">(</span><span class="nb">values</span> <span class="p">(</span><span class="nb">&lt;</span> <span class="nv">log-level</span> <span class="nv">log-eps</span><span class="p">)</span> <span class="nv">log-level</span><span class="p">)))</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The other, much harder, part is computing credible (Bayesian)
intervals for the Beta distribution. I won’t go over the code,
but the <a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L465">basic strategy</a> is to invert the CDF, a monotonic function,
by <a href="https://en.wikipedia.org/wiki/Bisection_method">bisection</a><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, and to assume we’re looking for improbable 
(\(\mathrm{cdf} &lt; 0.5\)) thresholds. This assumption lets us pick a
<a href="http://dlmf.nist.gov/8.17#ii">simple hypergeometric series</a> that is
normally useless, but
<a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L381">converges well for \(x\) that correspond to such small cumulative probabilities</a>;
when the series converges too slowly,
<a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L482">it’s always conservative to assume that \(x\) is too central</a> (not
extreme enough).</p>

<p>That’s all we need to demo the code. Looking at the distribution of
fill rates for the 1000 bins @ 30K ball/bin facet in</p>

<p><a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid.png">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid-small.png" />
</a></p>

<p>it looks like we almost always hit at least 97.5% global density,
let’s say with probability at least 98%. We can ask the CSM to tell us
when we have enough data to confirm or disprove that hypothesis, with
a 0.1% false positive rate.</p>

<p>Instead of generating more data on demand, I’ll keep things simple and
prepopulate a list with new independently observed fill rates.</p>

<pre><code>CL-USER&gt; (defparameter *observations* '(0.978518900
                                        0.984687300
                                        0.983160833
                                        [...]))
CL-USER&gt; (defun test (n)
           (let ((count (count-if (lambda (x) (&gt;= x 0.975))
                                  *observations*
                                  :end n)))
             (csm:csm n 0.98d0 count (log 0.001d0))))
CL-USER&gt; (test 10)
NIL
2.1958681996231784d0
CL-USER&gt; (test 100)
NIL
2.5948497850893184d0
CL-USER&gt; (test 1000)
NIL
-3.0115331544604658d0
CL-USER&gt; (test 2000)
NIL
-4.190687115879456d0
CL-USER&gt; (test 4000)
T
-17.238559826956475d0
</code></pre>

<p>We can also use the inverse Beta CDF to get a 99.9% credible
interval. After 4000 trials, we found 3972 successes.</p>

<pre><code>CL-USER&gt; (count-if (lambda (x) (&gt;= x 0.975))
                   *observations*
                   :end 4000)
3972
</code></pre>

<p>These values give us the following lower and upper bounds on the 99.9% CI.</p>

<pre><code>CL-USER&gt; (csm:beta-icdf 3972 (- 4000 3972) 0.001d0)
0.9882119750976562d0
1.515197753898523d-5
CL-USER&gt; (csm:beta-icdf 3972 (- 4000 3972) 0.001d0 t)
0.9963832682169742d0
2.0372679238045424d-13
</code></pre>

<p>And we can even re-use and extend the Beta proportion code from
earlier to generate this embeddable <a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/1k-30k-report.svg"><img src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/1k-30k-report.svg" width="100px" style="vertical-align: middle" /></a> SVG report.</p>

<p>There’s one small problem with the sample usage above: if we compute
the stopping criterion with a false positive rate of 0.1%, and do the
same for each end of the credible interval, our total false
positive (error) rate might actually be 0.3%! The next section will
address that, and the equally important problem of estimating power.</p>

<h1 id="monte-carlo-power-estimation">Monte Carlo power estimation</h1>

<p>It’s not always practical to generate data forever. For example, we
might want to bound the number of iterations we’re willing to waste in
an automated testing script. When there is a bound on the sample size,
the CSM is still correct, just conservative.</p>

<p>We would then like to know the probability that the CSM will stop
successfully when the underlying success rate differs from the
threshold rate \(p\) (<a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L291"><code>alpha</code> in the code</a>). The problem here is
that, for any bounded number of iterations, we can come up with an
underlying success rate so close to \(p\) (but still different) that
the CSM can’t reliably distinguish between the two.</p>

<p>If we want to be able to guarantee any termination rate, we need <em>two</em>
thresholds: the CSM will stop whenever it’s likely that the underlying
success rate differs from either of them. The hardest probability to
distinguish from both thresholds is close to the midpoint between them.</p>

<p>With two thresholds and the credible interval, we’re
running three tests in parallel. I’ll apply a 
<a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a>,
and use \(\varepsilon / 3\) for each of the two CSM tests, and
\(\varepsilon / 6\) for each end of the CI.</p>

<p>That <a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L542">logic is encapsulated in <code>csm-driver</code></a>.
We only have to pass a
success value generator function to the driver. In our case, the
generator is itself a call to <code>csm-driver</code>, with fixed thresholds
(e.g., 96% and 98%), and a Bernoulli sampler (e.g., return <code>T</code> with
probability 97%). We can see if the driver returns successfully <em>and
correctly</em> at each invocation of the generator function, with the
parameters we would use in production, and recursively compute
an estimate for that procedure’s success rate with CSM. The following
expression simulates a CSM procedure with thresholds at 96% and 98%,
the (usually unknown) underlying success rate in the middle, at 97%, a
false positive rate of at most 0.1%, and an iteration limit of ten thousand
trials. We pass that simulation’s result to <code>csm-driver</code>, and ask
whether the simulation’s success rate differs from 99%, while allowing
one in a million false positives.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
</pre></td><td class="code"><pre><code class=""><span class="line">CL-USER&gt; (labels ((bernoulli (i &amp;aux (p 0.97d0))
</span><span class="line">                    (declare (ignore i))
</span><span class="line">                    (&lt; (random 1d0) p))
</span><span class="line">                  (generator (i &amp;aux (p 0.97d0)
</span><span class="line">                                     (alpha 0.96d0) (alpha-hi 0.98d0)
</span><span class="line">                                     (eps 1d-3) (max-count 10000))
</span><span class="line">                    (declare (ignore i))
</span><span class="line">                    (multiple-value-bind (success success-hi estimate)
</span><span class="line">                        (csm:csm-driver #'bernoulli alpha eps
</span><span class="line">                                        :alpha-hi alpha-hi
</span><span class="line">                                        :max-count max-count)
</span><span class="line">                      ;; check that the CSM succeeds, and that it does so
</span><span class="line">                      ;; with correct estimates.
</span><span class="line">                      (let ((correct-alpha (if (&lt; p alpha)
</span><span class="line">                                               (&lt; estimate alpha)
</span><span class="line">                                               (&gt; estimate alpha)))
</span><span class="line">                            (correct-hi (if (&lt; p alpha-hi)
</span><span class="line">                                            (&lt; estimate alpha-hi)
</span><span class="line">                                            (&gt; estimate alpha-hi))))
</span><span class="line">                        (cond ((and success success-hi)
</span><span class="line">                               (and correct-alpha correct-hi))
</span><span class="line">                              (success
</span><span class="line">                               correct-alpha)
</span><span class="line">                              (success-hi
</span><span class="line">                               correct-hi)
</span><span class="line">                              (t
</span><span class="line">                               nil))))))
</span><span class="line">           (csm:csm-driver #'generator 0.99d0 1d-6))
</span><span class="line">T
</span><span class="line">T
</span><span class="line">1.0d0
</span><span class="line">2210
</span><span class="line">2210
</span><span class="line">0.993145939238895d0
</span><span class="line">0.9999999998869291d0</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We find that yes, we can expect the 96%/98%/0.1% false positive/10K
iterations setup to succeed more than 99% of the time. The 
<a href="https://github.com/pkhuong/csm/blob/47ecdd8f676a213dfb47ad0a329090a4c22b3125/csm.lisp#L690">code above is available as <code>csm-power</code></a>,
with a tighter outer false positive rate of 1e-9. If we only allow
1000 iterations, <code>csm-power</code> quickly tells us that, with one CSM
success in 100 attempts, we can expect the CSM success rate to be less
than 99%.</p>

<pre><code>CL-USER&gt; (csm:csm-power 0.97d0 0.96d0 1000 :alpha-hi 0.98d0 :eps 1d-3 :stream *standard-output*)
         1 0.000e+0 1.250e-10 10.000e-1 1.699e+0
        10 0.000e+0 0.000e+0 8.660e-1 1.896e+1
        20 0.000e+0 0.000e+0 6.511e-1 3.868e+1
        30 0.000e+0 0.000e+0 5.099e-1 5.851e+1
        40 2.500e-2 5.518e-7 4.659e-1 7.479e+1
        50 2.000e-2 4.425e-7 3.952e-1 9.460e+1
        60 1.667e-2 3.694e-7 3.427e-1 1.144e+2
        70 1.429e-2 3.170e-7 3.024e-1 1.343e+2
        80 1.250e-2 2.776e-7 2.705e-1 1.542e+2
        90 1.111e-2 2.469e-7 2.446e-1 1.741e+2
       100 1.000e-2 2.223e-7 2.232e-1 1.940e+2
100 iterations, 1 successes (false positive rate &lt; 1.000000e-9)
success rate p ~ 1.000000e-2
confidence interval [2.223495e-7, 0.223213    ]
p &lt; 0.990000    
max inner iteration count: 816

T
T
0.01d0
100
1
2.2234953205868331d-7
0.22321314110840665d0
</code></pre>

<h1 id="slo-ify-all-the-things-with-this-exact-test">SLO-ify all the things with this Exact test</h1>

<p>Until now, I’ve only used the Confidence Sequence Method (CSM) for
Monte Carlo simulation of phenomena that are naturally seen as boolean
success / failures processes. We can apply the same CSM to implement
an <a href="https://en.wikipedia.org/wiki/Exact_test">exact test</a> for null
hypothesis testing, with a bit of resampling magic.</p>

<p>Looking back at the balls and bins grid, the average fill rate seems
to be slightly worse for 100 bins @ 60K ball/bin, than for 1000 bins
@ 128K ball/bin. How can we test that with the CSM?</p>

<p><a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid.png">
<img class="center" src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/grid-small.png" />
</a></p>

<p>First, we should get a fresh dataset for the two setups we wish to
compare.</p>

<pre><code>CL-USER&gt; (defparameter *100-60k* #(0.988110167
                                   0.990352500
                                   0.989940667
                                   0.991670667
                                   [...]))
CL-USER&gt; (defparameter *1000-128k* #(0.991456281
                                     0.991559578
                                     0.990970109
                                     0.990425805
                                     [...]))
CL-USER&gt; (alexandria:mean *100-60k*)
0.9897938
CL-USER&gt; (alexandria:mean *1000-128k*)
0.9909645
CL-USER&gt; (- * **)
0.0011706948
</code></pre>

<p>The mean for 1000 bins @ 128K ball/bin is slightly higher than that
for 100 bins @ 60k ball/bin. We will now simulate the null hypothesis
(in our case, that the distributions for the two setups are
identical), and determine how rarely we observe a difference of
<code>0.00117</code> in means. I only use a null hypothesis where the
distributions are identical for simplicity; we could use the same
resampling procedure to simulate distributions that, e.g., have
identical shapes, but one is shifted right of the other.</p>

<p>In order to simulate our null hypothesis, we want to be as close to
the test we performed as possible, with the only difference being that
we generate data by reshuffling from our observations.</p>

<pre><code>CL-USER&gt; (defparameter *resampling-data* (concatenate 'simple-vector *100-60k* *1000-128k*))
*RESAMPLING-DATA*
CL-USER&gt; (length *100-60k*)
10000
CL-USER&gt; (length *1000-128k*)
10000
</code></pre>

<p>The two observation vectors have the same size, 10000 values; in
general, that’s not always the case, and we must make sure to
replicate the sample sizes in the simulation. We’ll generate our
simulated observations by shuffling the <code>*resampling-data*</code> vector,
and splitting it in two subvectors of ten thousand elements.</p>

<pre><code>CL-USER&gt; (let* ((shuffled (alexandria:shuffle *resampling-data*))
                (60k (subseq shuffled 0 10000))
                (128k (subseq shuffled 10000)))
           (- (alexandria:mean 128k) (alexandria:mean 60k)))
6.2584877e-6
</code></pre>

<p>We’ll convert that to a truth value by comparing the difference of
simulated means with the difference we observed in our real data,
\(0.00117\ldots\), and declare success when the simulated difference
is at least as large as the actual one. This approach gives us a
one-sided test; a two-sided test would compare the absolute
values of the differences.</p>

<pre><code>CL-USER&gt; (csm:csm-driver 
          (lambda (_)
            (declare (ignore _))
            (let* ((shuffled (alexandria:shuffle *resampling-data*))
                   (60k (subseq shuffled 0 10000))
                   (128k (subseq shuffled 10000)))
              (&gt;= (- (alexandria:mean 128k) (alexandria:mean 60k))
                  0.0011706948)))
          0.005 1d-9 :alpha-hi 0.01 :stream *standard-output*)
         1 0.000e+0 7.761e-11 10.000e-1 -2.967e-1
        10 0.000e+0 0.000e+0 8.709e-1 -9.977e-1
        20 0.000e+0 0.000e+0 6.577e-1 -1.235e+0
        30 0.000e+0 0.000e+0 5.163e-1 -1.360e+0
        40 0.000e+0 0.000e+0 4.226e-1 -1.438e+0
        50 0.000e+0 0.000e+0 3.569e-1 -1.489e+0
        60 0.000e+0 0.000e+0 3.086e-1 -1.523e+0
        70 0.000e+0 0.000e+0 2.718e-1 -1.546e+0
        80 0.000e+0 0.000e+0 2.427e-1 -1.559e+0
        90 0.000e+0 0.000e+0 2.192e-1 -1.566e+0
       100 0.000e+0 0.000e+0 1.998e-1 -1.568e+0
       200 0.000e+0 0.000e+0 1.060e-1 -1.430e+0
       300 0.000e+0 0.000e+0 7.207e-2 -1.169e+0
       400 0.000e+0 0.000e+0 5.460e-2 -8.572e-1
       500 0.000e+0 0.000e+0 4.395e-2 -5.174e-1
       600 0.000e+0 0.000e+0 3.677e-2 -1.600e-1
       700 0.000e+0 0.000e+0 3.161e-2 2.096e-1
       800 0.000e+0 0.000e+0 2.772e-2 5.882e-1
       900 0.000e+0 0.000e+0 2.468e-2 9.736e-1
      1000 0.000e+0 0.000e+0 2.224e-2 1.364e+0
      2000 0.000e+0 0.000e+0 1.119e-2 5.428e+0

NIL
T
0.0d0
2967
0
0.0d0
0.007557510165262294d0
</code></pre>

<p>We tried to replicate the difference 2967 times, and did not succeed
even once. The CSM stopped us there, and we find a CI for the
probability of observing our difference, under the null hypothesis, of
<code>[0, 0.007557]</code> (i.e., \(p &lt; 0.01\)). Or, for a graphical summary, <a href="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/exact-report.svg"><img src="/images/2018-07-06-testing-slo-type-properties-with-the-confidence-sequence-method/exact-report.svg" width="100px" style="vertical-align: middle" /></a>.
We can also test for a lower \(p\)-value by changing the
thresholds and running the simulation more times (around thirty
thousand iterations for \(p &lt; 0.001\)).</p>

<p>This experiment lets us conclude that the
difference in mean fill rate between 100 bins @ 60K ball/bin and
1000 @ 128K is probably not due to chance: it’s unlikely
that we observed an expected difference between data sampled from
the same distribution. In other words, “I’m confident that the fill rate for
1000 bins @ 128K ball/bin is greater than for 100 bins @ 60K
ball/bins, because it would be highly unlikely to observe a difference in means
that extreme if they had the same distribution (\(p &lt; 0.01\))”.</p>

<p>In general, we can use this exact test when we have two sets of
observations, \(X\sb{0}\) and \(Y\sb{0}\), and a statistic
\(f\sb{0} = f(X\sb{0}, Y\sb{0})\), where \(f\) is a pure
function (the extension to three or more sets of observations is
straightforward).</p>

<p>The test lets us determine the likelihood of observing 
\(f(X, Y) \geq f\sb{0}\) 
(we could also test for \(f(X, Y) \leq f\sb{0}\)), if
\(X\) and \(Y\) were taken from similar distributions, modulo simple
transformations (e.g., \(X\)’s mean is shifted compared to \(Y\)’s, or
the latter’s variance is double the former’s).</p>

<p>We answer that question by repeatedly sampling without replacement
from \(X\sb{0} \cup Y\sb{0}\) to generate \(X\sb{i}\) and
\(Y\sb{i}\), such that \(|X\sb{i}| = |X\sb{0}|\) and
\(|Y\sb{i}| = |Y\sb{0}|\) (e.g., by shuffling a vector and
splitting it in two). We can apply any simple transformation here
(e.g., increment every value in \(Y\sb{i}\) by \(\Delta\) to shift
its mean by \(\Delta\)). Finally, we check if 
\(f(X\sb{i}, Y\sb{i}) \geq f\sb{0} = f(X\sb{0}, Y\sb{0})\); if
so, we return success for this iteration, otherwise failure.</p>

<p>The loop above is a Bernoulli process that generates independent,
identically distributed (assuming the random sampling is correct)
truth values, and its success rate is equal to the probability of
observing a value for \(f\) “as extreme” as \(f\sb{0}\) under the
null hypothesis. We use the CSM with false positive rate
\(\varepsilon\) to know when to stop generating more
values and compute a credible interval for the probability under the
null hypothesis. If that probability is low (less than some
predetermined threshold, like \(\alpha = 0.001\)), we infer that the
null hypothesis does not hold, and declare that the difference in our
sample data points at a real difference in distributions. If we do
everything correctly (<em>cough</em>), we will have implemented an Atlantic
City procedure that fails with probability \(\alpha + \varepsilon\).</p>

<p>Personally, I often just set the threshold and the false positive
rate unreasonably low and handwave some Bayes.</p>

<h1 id="thats-all">That’s all!</h1>

<p>I pushed
<a href="https://github.com/pkhuong/csm">the code above, and much more, to github</a>,
in Common
Lisp, C, and Python (probably Py3, although 2.7 might work). Hopefully
anyone can run with the code and use it to test, not only 
<a href="https://landing.google.com/sre/book/chapters/service-level-objectives.html">SLO</a>-type
properties, but also answer more general questions, with an exact
test. I’d love to have ideas or contributions on the usability front.
I have some
<a href="https://github.com/pkhuong/csm/blob/master/attic/beta-distribution.lisp">throw-away code in <code>attic/</code></a>,
which I used to generate the SVG in this post, but it’s not great. I
also feel like I can do something to make it easier to stick the logic
in shell scripts and continuous testing pipelines.</p>

<p>When I passed around a first draft for this post, many readers that
could have used the CSM got stuck on the process of moving from
mathematical expressions to computer code; not just how to do it,
but, more fundamentally, why we can’t just transliterate Greek to
C or CL. I hope this revised post is clearer. Also, I hope it’s clear
that the reason I care so much about not introducing false positive
via rounding isn’t that I believe they’re likely to make a difference,
but simply that I want peace of mind with respect to numerical issues;
I really don’t want to be debugging some issue in my tests and have to
wonder if it’s all just caused by numerical errors.</p>

<p>The reason I care so much about making sure users can understand what
the CSM codes does (and why it does what it does) is that I strongly
believe we should minimise dependencies whose inner working we’re
unable to (legally) explore. Every abstraction leaks, and leakage is
particularly frequent in failure situations. We may not need to
understand magic if everything works fine, but, everything breaks
eventually, and that’s when expertise is most useful. When shit’s on
fire, we must be able to break the abstraction and understand how the
magic works, and how it fails.</p>

<p>This post only tests ideal SLO-type properties (and
regular null hypothesis tests translated to SLO properties),
properties of the form “I claim that this indicator satisfies
$PREDICATE x% of the time, with false positive rate y%” where the
indicator’s values are independent and identically distributed.</p>

<p>The last assumption is rarely <em>truly</em> satisfied in practice. I’ve seen
an interesting choice, where the <a href="https://landing.google.com/sre/book/chapters/service-level-objectives.html">service level objective</a> is defined in
terms of a sample of production requests, which can replayed, shuffled,
etc. to ensure i.i.d.-ness. If the nature of the traffic changes
abruptly, the SLO may not be representative of behaviour in
production; but, then again, how could the service provider have
guessed the change was about to happen? I like this approach because
it is amenable to predictive statistical analysis, and incentivises
communication between service users and providers, rather than users
assuming the service will gracefully handle radically new crap being
thrown at it.</p>

<p>Even if we have a representative sample of production, it’s not true
that the <a href="https://landing.google.com/sre/book/chapters/service-level-objectives.html">service level indicators</a>
for individual requests are distributed identically. There’s an easy
fix for the CSM and our credible intervals: generate
i.i.d. <em>sets of requests</em> by resampling (e.g., shuffle the
requests sample) and count successes and failures for individual
requests, but only test for CSM termination after each resampled set.</p>

<p>On a more general note, I see the Binomial and Exact tests as
instances of a general pattern that avoids intuitive functional decompositions
that create subproblems that are harder to solve than the original
problem. For example, instead of trying to directly determine how
frequently the SLI satisfies some threshold, it’s natural to
first fit a distribution on the SLI, and then compute percentiles on
that distribution. Automatically fitting an arbitrary distribution is
hard, especially with the weird outliers computer systems spit
out. Reducing to a Bernoulli process before applying statistics is
much simpler. Similarly, rather than coming up with analytical
distributions in the Exact test, we brute-force the problem by
resampling from the empirical data. I have more examples from online
control systems… I guess the moral is to be wary of decompositions
where internal subcomponents generate intermediate values that are
richer in information than the final output.</p>

<p><small>Thank you Jacob, Ruchir, Barkley, and Joonas for all the
editing and restructuring comments.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Proportions are unscaled probabilities that don’t have to sum or integrate to 1. Using proportions instead of probabilities tends to make calculations simpler, and we can always get a probability back by rescaling a proportion by the inverse of its integral. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Instead of a \(\mathrm{Beta}(a+1, b+1)\), they tend to bound with a \(\mathrm{Beta}(a, b)\). The difference is marginal for double-digit \(n\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>I used the bisection method instead of more sophisticated ones with better convergence, like <a href="https://en.wikipedia.org/wiki/Newton%27s_method">Newton’s  method</a> or the derivative-free <a href="https://en.wikipedia.org/wiki/Secant_method">Secant method</a>, because bisection already adds one bit of precision per iteration, only needs a predicate that returns “too high” or “too low,” and is easily tweaked to be conservative when the predicate declines to return an answer. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance tuning ~ writing an essay]]></title>
    <link href="https://www.pvk.ca/Blog/2014/10/19/performance-optimisation-~-writing-an-essay/"/>
    <updated>2014-10-19T20:05:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2014/10/19/performance-optimisation-~-writing-an-essay</id>
    <content type="html"><![CDATA[<p><a href="#trust-no-one">Skip to the meaty bits.</a></p>

<p>My work at <a href="http://www.appnexus.com/">AppNexus</a> mostly involves
performance optimisation, at any level from microarchitecture-driven
improvements to data layout and assembly code to improving the
responsiveness of our distributed system under load.  Technically,
this is similar to what I was doing as a lone developer on
research-grade programs.  However, the scale of our (constantly
changing) code base and collaboration with a dozen other coders mean
that I approach the task differently: e.g., rather than
single-mindedly improving throughput <em>now</em>, I aim to pick an evolution
path that improves throughput today without imposing too much of a
burden on future development or fossilising ourselves in a design
dead-end.  So, although numbers still don’t lie (hah), my current
approach also calls for something like judgment and taste, as well as
a fair bit of empathy for others.  Rare are the obviously correct
choices, and, in that regard, determining what changes to make and
which to discard as
<a href="http://fun.irq.dk/funroll-loops.org/">over-the-top ricing</a> feels like
I’m drafting a literary essay.</p>

<p>This view is probably tainted by the fact that, between English and
French classes, I spent something like half of my time in High School
critiquing essays, writing essays, or preparing to write one.
Initially, there was a striking difference between the two languages:
English teachers had us begin with the five paragraph format where one
presents multiple arguments for the same thesis, while French teachers
imposed a thesis/antithesis/synthesis triad (and never really let it
go until CÉGEP, but that’s another topic).  When I write that
performance optimisation feels like drafting essays, I’m referring to
the latter “Hegelian” process, where one exposes arguments and
counterarguments alike in order to finally make a stronger case.</p>

<p>I’ll stretch the analogy further.  Reading between the lines gives us
access to more arguments, but it’s also easy to get the context wrong and
come up with hilariously far-fetched interpretations.  When I try to
understand a system’s performance, the most robust metrics treat the
system as a black box: it’s hard to get throughput under production
data wrong.  However, I need finer grained information (e.g.,
performance counters, instruction-level profiling, or
application-specific metrics) to guide my work, and, the more useful
that information can be – like domain specific metrics that highlight
what we could do differently rather than how to do the same thing more
efficiently – the easier it is to measure incorrectly.  That’s not a
cause for despair, but rather a fruitful line of skepticism that helps
me find more opportunities.</p>

<p>Just two weeks ago, questioning our application-specific metrics
lead to an easy 10% improvement in throughput for our biggest
consumer of CPU cycles.  The consumer is an application that
determines whether internet advertising campaigns are eligible to bid
on an ad slot, and if so, which creative (ad) to show and at what bid
price.  For the longest time, the most time-consuming part of that
process was the first step, testing for campaign eligibility.
Consequently, we tracked the execution of that step precisely and
worked hard to minimise the time spent on ineligible campaigns,
without paying much attention to the rest of the pipeline.  However,
we were clearly hitting diminishing returns in that area, so I asked
myself how an adversary could use our statistics to mislead us.  The
easiest way I could think of was to have campaigns that are eligible
to bid, but without any creative compatible with the ad slot (e.g.,
because it’s the wrong size or because the website forbids Flash ads):
although the campaigns are technically eligible, they are unable to
bid on the ad slot.  We added code to track these cases and found that
almost half of our “eligible” campaigns simply had no creative in the
right size.  Filtering these campaigns early proved to be a
low-hanging fruit with an ideal code complexity:performance
improvement ratio.</p>

<h1 id="trust-no-one-not-even-performance-counters"><a href="#trust-no-one" name="trust-no-one">Trust no one, not even performance counters</a></h1>

<p>I recently learned that we also had to second-guess instruction level
profiles.  Contemporary x86oids are out of order, superscalar, and
speculative machines, so profiles are always messy: “blame” is
scattered around the real culprit, and some instructions (pipeline
hazards like conditional jumps and uncached memory accesses, mostly)
seem to account for more than their actual share.  What I never
realised is that, in effect, some instructions systematically mislead
and push their cycles to others.</p>

<p>Some of our internal spinlocks use <code>mfence</code>.  I expected that to be
suboptimal, since it’s
<a href="https://blogs.oracle.com/dave/resource/NHM-Pipeline-Blog-V2.txt">common</a>
<a href="http://shipilev.net/blog/2014/on-the-fence-with-dependencies/">knowledge</a>
that <code>lock</code>ed instruction are more efficient barriers: serialising
instructions like <code>mfence</code> have to affect streaming stores and other
weakly ordered memory accesses, and that’s a lot more work than just
preventing store/load reordering.  However, our profiles showed that
we spent very little time on spinlocking so I never gave it much thought…
until eliminating a set of spinlocks had a much better impact on
performance than I would have expected from the profile.  Faced with
this puzzle, I had to take a closer look at the way <code>mfence</code> and
locked instructions affect hardware-assisted instruction profiles on
our production Xeon E5s (Sandy Bridge).</p>

<p>I came up with a simple synthetic microbenchmark to simulate locking
on my E5-4617: the loop body is an adjustable set of memory accesses
(reads and writes of out-of-TLB or uncached locations) or computations
(divisions) bracketed by pairs of normal stores, <code>mfence</code>, or <code>lock
inc/dec</code> to cached memory (I would replace the fences with an
increment/decrement pair and it looks like all read-modify-write
instructions are implemented similarly on Intel).  Comparing runtimes
for normal stores with the other instructions helps us gauge their
overhead.  I can then execute each version under <code>perf</code> and estimate
the overhead from the instruction-level profile.  If <code>mfence</code> is
indeed extra misleading, there should be a greater discrepancy between
the empirical impact of the <code>mfence</code> pair and my estimate from the
profile.</p>

<p>You can find the
<a href="/images/2014-10-19-performance-optimisation-~-writing-an-essay/fence.c">super crufty code here</a>,
<a href="/images/2014-10-19-performance-optimisation-~-writing-an-essay/cycle.h">along with an <code>rdtscp</code> version of cycle.h</a>.</p>

<p>With <code>lock</code>ed instructions and random reads that miss the L3 cache,
the (cycle) profile for the microbenchmark loop is:</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ perf annotate -s cache_misses
</span><span class="line">[...]
</span><span class="line">    0.06 :        4006b0:       and    %rdx,%r10
</span><span class="line">    0.00 :        4006b3:       add    $0x1,%r9
</span><span class="line">    ;; random (out of last level cache) read
</span><span class="line">    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
</span><span class="line">   30.37 :        4006bb:       mov    %rcx,%r10
</span><span class="line">    ;; foo is cached, to simulate our internal lock
</span><span class="line">    0.12 :        4006be:       mov    %r9,0x200fbb(%rip)        # 601680 &lt;foo&gt;
</span><span class="line">    0.00 :        4006c5:       shl    $0x17,%r10
</span><span class="line">    [... Skipping arithmetic with &lt; 1% weight in the profile]
</span><span class="line">    ;; locked increment of an in-cache "lock" byte
</span><span class="line">    1.00 :        4006e7:       lock incb 0x200d92(%rip)        # 601480 &lt;private+0x200&gt;
</span><span class="line">   21.57 :        4006ee:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    ;; random out of cache read
</span><span class="line">    0.00 :        400704:       xor    (%rsi,%r10,8),%rbp
</span><span class="line">   21.99 :        400708:       xor    %r9,%r8
</span><span class="line">    [...]
</span><span class="line">    ;; locked in-cache decrement
</span><span class="line">    0.00 :        400729:       lock decb 0x200d50(%rip)        # 601480 &lt;private+0x200&gt;
</span><span class="line">   18.61 :        400730:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    0.92 :        400755:       jne    4006b0 &lt;cache_misses+0x30&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Looking at that profile, I’d estimate that the two random reads
account for ~50% of runtime, and the pair of <code>lock inc/dec</code> for ~40%.</p>

<p>The picture is completely different for <code>mfence</code>.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
</pre></td><td class="code"><pre><code class=""><span class="line">$ perf annotate -s cache_misses
</span><span class="line">[...]
</span><span class="line">    0.00 :        4006b0:       and    %rdx,%r10
</span><span class="line">    0.00 :        4006b3:       add    $0x1,%r9
</span><span class="line">    ;; random read
</span><span class="line">    0.00 :        4006b7:       mov    (%rsi,%r10,8),%rbp
</span><span class="line">   42.04 :        4006bb:       mov    %rcx,%r10
</span><span class="line">    ;; store to cached memory (lock word)
</span><span class="line">    0.00 :        4006be:       mov    %r9,0x200fbb(%rip)        # 601680 &lt;foo&gt;
</span><span class="line">    [...]
</span><span class="line">    0.20 :        4006e7:       mfence 
</span><span class="line">    5.26 :        4006ea:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    ;; random read
</span><span class="line">    0.19 :        400700:       xor    (%rsi,%r10,8),%rbp
</span><span class="line">   43.13 :        400704:       xor    %r9,%r8
</span><span class="line">    [...]
</span><span class="line">    0.00 :        400725:       mfence 
</span><span class="line">    4.96 :        400728:       add    $0x1,%rax
</span><span class="line">    0.92 :        40072c:       add    $0x1,%rax
</span><span class="line">    [...]
</span><span class="line">    0.36 :        40074d:       jne    4006b0 &lt;cache_misses+0x30&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>It looks like the loads from uncached memory represent ~85% of the
runtime, while the <code>mfence</code> pair might account for <em>at most</em> ~15%, if
I include all the noise from surrounding instructions.</p>

<p>If I trusted the profile, I would worry about eliminating <code>lock</code>ed
instructions, but not so much for <code>mfence</code>.  However, runtimes (in
cycles), which is what I’m ultimately interested in, tell a different
story.  The same loop of LLC load misses takes 2.81e9 cycles for 32M
iterations without any atomic or fence, versus 3.66e9 for <code>lock
inc/dec</code> and 19.60e9 cycles for <code>mfence</code>.  So, while the profile for
the <code>mfence</code> loop would let me believe that only ~15% of the time is
spent on synchronisation, the <code>mfence</code> pair really represents 86%
\(((19.6 - 2.81) / 19.6)\) of the runtime for that loop!  Inversely,
the profile for the <code>lock</code>ed pair would make me guess that we spend
about 40% of the time there, but, according to the timings, the real
figure is around 23%.</p>

<p>The other tests all point to the same conclusion: the overhead of
<code>mfence</code> is strongly underestimated by instruction level profiling,
and that of <code>lock</code>ed instructions exaggerated, especially when
adjacent instructions write to memory.</p>

<pre><code>  setup     cycles   (est. overhead)  ~actual overhead

div [ALU] (100 Mi iterations)
 atomic: 20153782848   (20%)          ~ 3.8%
 mfence: 28202315112   (25%)          ~31.3%
vanilla: 19385020088

Reads:

TLB misses (64Mi iterations)
 atomic:  3776164048   (80%)          ~39.3%
 mfence: 12108883816   (50%)          ~81.1%
vanilla:  2293219400 

LLC misses (32Mi iterations)
 atomic:  3661686632   (40%)          ~23.3%
 mfence: 19596840824   (15%)          ~85.7%
vanilla:  2807258536

Writes:

TLB (64Mi iterations)
 atomic:  3864497496   (80%)          ~10.4%
 mfence: 13860666388   (50%)          ~75.0%
vanilla:  3461354848

LLC (32Mi iterations)
 atomic:  4023626584   (60%)          ~16.9%
 mfence: 21425039912   (20%)          ~84.4%
 vanilla: 3345564432
</code></pre>

<p>I can guess why we observe this effect; it’s not like Intel is
intentionally messing with us.  <code>mfence</code> is a full pipeline flush: it
slows code down because it waits for all in-flight instructions to
complete their execution.  Thus, while it’s flushing that slows us
down, the profiling machinery will attribute these cycles to the
instructions that are being flushed.  Locked instructions instead
affect stores that are still queued.  By forcing such stores to
retire, locked instructions become responsible for the extra cycles
and end up “paying” for writes that would have taken up time anyway.</p>

<p>Losing faith in hardware profiling being remotely representative of
reality makes me a sad panda; I now have to double check <code>perf</code>
profiles when hunting for misleading metrics.  At least I can tell
myself that knowing about this phenomenon helps us make better
informed – if less definite – decisions and ferret out more easy
wins.</p>

<p>P.S., if you find this stuff interesting, feel free to send an email
(pkhuong at $WORK.com).  My team is hiring both experienced developers
and recent graduates (:</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K-ary heapsort: more comparisons, less memory traffic]]></title>
    <link href="https://www.pvk.ca/Blog/2014/04/13/k-ary-heapsort/"/>
    <updated>2014-04-13T12:25:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2014/04/13/k-ary-heapsort</id>
    <content type="html"><![CDATA[<p><em>This post first appeared on <a href="http://techblog.appnexus.com/2014/k-ary-heapsort-more-comparisons-less-memory-traffic/">AppNexus’s tech blog</a>.</em></p>

<p>The impetus for this post was a max heap routine I had to write
because libc, unlike the STL, does not support
<a href="http://www.sgi.com/tech/stl/make_heap.html">incremental</a> or even
<a href="https://www.sgi.com/tech/stl/partial_sort.html">partial sorting</a>.
After staring at the standard implicit binary heap for a while, I
realised how to
<a href="http://www.pvk.ca/Blog/2014/04/13/number-systems-for-implicit-data-structures/">generalise it to arbitrary arity</a>.  The
routine will be used for medium size elements (a couple dozen bytes)
and with a trivial comparison function; in that situation, it makes
sense to implement a high arity heap and perform fewer swaps in return
for additional comparisons.  In fact, the trade-off is interesting
enough that a heapsort based on this routine is competitive with
<a href="http://fxr.watson.org/fxr/source/stdlib/merge.c?v=FREEBSD-LIBC">BSD</a>
and
<a href="https://sourceware.org/git/?p=glibc.git;a=blob;f=stdlib/qsort.c;h=04c25b984f74a8f738233cc6da8a738b6437833c;hb=HEAD">glibc</a>
sorts.  This post will present the k-ary heap code and explore the
impact of memory traffic on the performance of a few classical sort
routines.  The worst performing sorts in BSD’s and GNU’s libc overlook
swaps and focus on minimising comparisons.  I argue this is rarely the
correct choice, although our hands are partly tied by POSIX.</p>

<h1 id="a-k-ary-max-heap">A k-ary max heap</h1>

<p>The heart of a heap – and of heapsort – is the sift-down function.
I explored the indexing scheme below in details
<a href="http://www.pvk.ca/Blog/2014/04/13/number-systems-for-implicit-data-structures/">somewhere else</a>, but the gist of it is
that we work with regular 0-indexed arrays and the children of
<code>heap[i]</code> lie at indices <code>heap[way * i + 1]</code> to <code>heap[way * i + way]</code>,
inclusively.  Like sifting for regular binary heaps, the routine
restores the max-heap property under the assumption that only the root
(head) may be smaller than any of its children.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cp">#define REF(I) ((void *)((char *)base + ((I) * size)))</span>
</span><span class="line">
</span><span class="line"><span class="cp">#define ALIGNED_SIZE 16</span>
</span><span class="line">
</span><span class="line"><span class="k">typedef</span> <span class="nf">int</span> <span class="p">(</span><span class="o">*</span><span class="n">comparator</span><span class="p">)(</span><span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="p">);</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Similar arguments to qsort. way is the arity of the heap and</span>
</span><span class="line"><span class="cm"> * head the index from which to sift down.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="k">static</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">nheap_sift_inner</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">head</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">                 <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">aligned</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span><span class="line">                <span class="p">(((</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">base</span> <span class="o">-</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">while</span> <span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">void</span> <span class="o">*</span><span class="n">parent</span><span class="p">,</span> <span class="o">*</span><span class="n">max</span><span class="p">;</span>
</span><span class="line">                <span class="kt">size_t</span> <span class="n">children_offset</span> <span class="o">=</span> <span class="n">head</span> <span class="o">*</span> <span class="n">way</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
</span><span class="line">                <span class="kt">size_t</span> <span class="n">max_head</span> <span class="o">=</span> <span class="n">head</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">children_offset</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="cm">/* We&#39;re at a leaf. */</span>
</span><span class="line">                        <span class="k">return</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Find the max of parent and all children. */</span>
</span><span class="line">                <span class="n">parent</span> <span class="o">=</span> <span class="n">max</span> <span class="o">=</span> <span class="n">REF</span><span class="p">(</span><span class="n">head</span><span class="p">);</span>
</span><span class="line">                <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">way</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="kt">void</span> <span class="o">*</span><span class="n">child</span><span class="p">;</span>
</span><span class="line">                        <span class="kt">size_t</span> <span class="n">j</span> <span class="o">=</span> <span class="n">children_offset</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">                        <span class="k">if</span> <span class="p">(</span><span class="n">j</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                                <span class="k">break</span><span class="p">;</span>
</span><span class="line">                        <span class="p">}</span>
</span><span class="line">
</span><span class="line">                        <span class="n">child</span> <span class="o">=</span> <span class="n">REF</span><span class="p">(</span><span class="n">j</span><span class="p">);</span>
</span><span class="line">                        <span class="k">if</span> <span class="p">(</span><span class="n">compar</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">max</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                                <span class="n">max</span> <span class="o">=</span> <span class="n">child</span><span class="p">;</span>
</span><span class="line">                                <span class="n">max_head</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span>
</span><span class="line">                        <span class="p">}</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">max</span> <span class="o">==</span> <span class="n">parent</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="k">return</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="cm">/* Swap the parent down and re-iterate. */</span>
</span><span class="line">                <span class="n">swap</span><span class="p">(</span><span class="n">parent</span><span class="p">,</span> <span class="n">max</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">aligned</span><span class="p">);</span>
</span><span class="line">                <span class="n">head</span> <span class="o">=</span> <span class="n">max_head</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The expected use case for this routine is that <code>compar</code> is a trivial
function (e.g., comparing two longs) and <code>size</code> relatively large, up
to fifty or even a few hundred bytes.  That’s why we do some more
work to not only minimise the number of swaps, but also to accelerate
each swap.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="k">typedef</span> <span class="kt">int</span> <span class="n">vector</span> <span class="nf">__attribute__</span><span class="p">((</span><span class="n">vector_size</span><span class="p">(</span><span class="n">ALIGNED_SIZE</span><span class="p">)));</span>
</span><span class="line">
</span><span class="line"><span class="cp">#define CHUNK_SIZE sizeof(long)</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">swap</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">aligned</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">chunk</span><span class="p">[</span><span class="n">CHUNK_SIZE</span><span class="p">];</span>
</span><span class="line">        <span class="kt">size_t</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">aligned</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                        <span class="n">vector</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span>
</span><span class="line">                        <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">);</span>
</span><span class="line">                        <span class="o">*</span><span class="p">(</span><span class="n">vector</span> <span class="o">*</span><span class="p">)(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">                <span class="p">}</span>
</span><span class="line">
</span><span class="line">                <span class="k">return</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">+</span> <span class="n">CHUNK_SIZE</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">CHUNK_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When the data will always be aligned to 16-byte boundaries, we swap
vector registers.  Otherwise, we resort to register-size calls to
<code>memcpy(3)</code> – decent compilers will turn them into unaligned accesses
– and handle any slop with byte-by-byte swaps.</p>

<p>It’s a quick coding job to wrap the above in a linear-time <code>heapify</code>
and a heapsort.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>nheap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Given a way-ary heap, sift the head&#39;th element down to restore the</span>
</span><span class="line"><span class="cm"> * heap property.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">nheap_sift</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">head</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">           <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">way</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">head</span> <span class="o">&gt;=</span> <span class="n">nmemb</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">3</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Turn the nmemb elements of size bytes in base into a compar-max</span>
</span><span class="line"><span class="cm"> * way-ary heap.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span> <span class="nf">nheapify</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">             <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">way</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="o">-</span><span class="mi">2</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">nmemb</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="cm">/* ceiling(nmemb, way) is a leaf. */</span>
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">nmemb</span> <span class="o">/</span> <span class="n">way</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">--&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span><span class="line">
</span><span class="line"><span class="cm">/*</span>
</span><span class="line"><span class="cm"> * Like BSD heapsort(3), except that the first argument specifies the</span>
</span><span class="line"><span class="cm"> * arity of the internal max-heap.</span>
</span><span class="line"><span class="cm"> */</span>
</span><span class="line"><span class="kt">int</span>
</span><span class="line"><span class="nf">nheapsort</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">way</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">base</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">nmemb</span><span class="p">,</span>
</span><span class="line">          <span class="kt">size_t</span> <span class="n">size</span><span class="p">,</span> <span class="n">comparator</span> <span class="n">compar</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">aligned</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span>
</span><span class="line">                <span class="p">(((</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="n">base</span> <span class="o">-</span> <span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="p">)</span><span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">ALIGNED_SIZE</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line">        <span class="kt">int</span> <span class="n">r</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="n">r</span> <span class="o">=</span> <span class="n">nheapify</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">nmemb</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">r</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="k">return</span> <span class="n">r</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="n">nmemb</span><span class="p">;</span> <span class="n">i</span> <span class="o">--&gt;</span> <span class="mi">1</span><span class="p">;</span> <span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">swap</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">REF</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">size</span><span class="p">,</span> <span class="n">aligned</span><span class="p">);</span>
</span><span class="line">                <span class="n">nheap_sift_inner</span><span class="p">(</span><span class="n">way</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">compar</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This heap implementation proved more than good enough for our
production use.  In fact, it is so efficient that <code>nheapsort</code> is
competitive with battle-tested sort implementations, particularly when
sorting medium or large structs.  However, these classical
implementations were primarily tuned with comparison counts in mind…
which I argue is now inappropriate.  In the remainder of this post,
I’ll explore the impact of swaps on the performance of sort routines.</p>

<h1 id="meet-the-competition">Meet the competition</h1>

<p>I will compare with the sort routines in two libc: GNU’s glibc and
BSD’s (FreeBSD and OS X, at least, and other *BSD I expect) libc.
All are more than 20 years old.</p>

<p>The <code>qsort(3)</code> routine in glibc is usually a mergesort that allocates
scratch space.  It includes some tests to try and detect out of memory
conditions, but I doubt the logic is very effective on contemporary
hardware and with Linux’s overcommit by default.  BSD manpages specify
that <code>qsort(3)</code> is in-place.  GNU’s don’t… because glibc’s qsort is not
a quicksort.</p>

<p>I can see many arguments for glibc’s design choice.  For one, a
nicely tuned mergesort may be quicker than a safe (guaranteed
\(\mathcal{O}(n \log n)\) time) quicksort.  It also enables msort’s
indirect sorting strategy for large elements: when elements span more
than 32 chars, the sort phase only manipulates pointers and a final
linear-time pass shuffles the actual data in place.</p>

<p>On the other hand, I would be annoyed if qsort triggered the 
<a href="http://linux-mm.org/OOM_Killer">OOM killer</a>.</p>

<p>For this reason, I’ll explicitly consider the real in-place
quicksort that gets called when it’s clear that allocating more memory
is infeasible.  Sadly, I doubt that quicksort received much
attention since it was tuned for a
<a href="http://en.wikipedia.org/wiki/Sun-4">Sun 4/260</a>.  It’s clearly a
product of the 80’s.  It has an explicit stack, the recursion order
guarantees logarithmic stack depth, the pivot is always chosen with a
median of three, and the base case (a partition of 4 or fewer
elements) switches to insertion sort.</p>

<p>The code and the very design of the routine seem to completely
disregard memory traffic.  The swap loop is always char-by-char, and
the base case (insertion sort) isn’t actually called when recursion
stops at a leaf.  Instead, small unsorted partitions are left as is.
Insertion sort is only executed at the end, over the whole array:
unsorted partitions contain at most 4 elements and are otherwise
ordered correctly, so each insertion sort inner loop iterates at most
4 times.  This strategy wastes the locality advantage of
divide-and-conquer sorts: the insertion sort pass is almost streaming,
but it would likely operate on warm data if leaf partitions were
sorted on the fly.</p>

<p>It also includes unsafe code like</p>

<pre><code>char *const end_ptr = &amp;base_ptr[size * (total_elems - 1)];
char *tmp_ptr = base_ptr;
char *thresh = min(end_ptr, base_ptr + max_thresh);
</code></pre>

<p>I can only pray that compilers never becomes smart enough to exploit
<code>base_ptr + max_thresh</code> to determine that this routine will never sort
arrays of fewer than 4 elements.</p>

<p>glibc’s implementation of inserts only aggravates the problem.  After
finding a sorted position for the new element, insertion sort must
insert it there.  Usually one would implement that by copying the new
element (one past the end of the sorted section) to a temporary
buffer, sliding the tail of the sorted array one slot toward the end,
and writing the new element in its expected position.  Unfortunately,
we can’t allocate an arbitrary-size temporary buffer if our quicksort
is to be in-place.  The insertion loop in glibc circumvents the
problem by doing the copy/slide/write dance… one char at a time.
That’s the worst implementation I can think of with respect to memory
access patterns.  It operates on a set of contiguous addresses (the
tail of the sorted array and the new element at its end) with a double
loop of strided accesses that still end up touching each of these addresses.
When the element size is a power of two, it might even hit aliasing
issues and exhaust the associativity of data caches.</p>

<p>The quicksort in *BSD is saner, perhaps because it is still called by
<code>qsort(3)</code>.  While glibc pays lip service to <a href="http://dl.acm.org/citation.cfm?id=172710">Bentley and McIlroy’s “Engineering a Sort Function”,</a> BSD actually uses their implementation.
The swap macro works one <code>long</code> at a time if possible, and the
recursive (with tail recursion replaced by a <code>goto</code>) function switches
to insertion sort as soon as the input comprises fewwer than 7
elements.  Finally, unlike GNU’s insertion sort, BSD’s rotates in
place with a series of swaps.  This approach executes more writes than
glibc’s strided rotations, but only accesses pairs of contiguous addresses.
There’s one tweak that I find worrying: whenever a pivoting step
leaves everything in place, quicksort directly switches to insertion
sort, regardless of the partition size.  This is an attempt to detect
pre-sorted subsequences, but malicious or unlucky input can easily
turn BSD’s quicksort into a quadratic-time insertion sort.</p>

<p>GNU’s quicksort finishes with insertion sort because the latter copes
well with almost sorted data, including the result of its partial
quicksort.  BSD’s qsort is a straight, cache-friendly,
divide-and-conquer implementation.  It looks like it’d make sense to
replace its base case with a selection sort: selection sort will
perform more comparisons than insertion sort (the latter could even
use binary search), but, with a single swap per iteration, will move
much less data around.  I tried it out and the change had little to no
impact, even for large elements.</p>

<p>For completeness’s sake, I’ll also include results with BSD libc’s
heapsort and mergesort.  The heapsort doesn’t look like it’s been
touched in a long while; it too gives compilers licence to kill code,
via</p>

<pre><code>/*
 * Items are numbered from 1 to nmemb, so offset from size bytes
 * below the starting address.
 */
base = (char *)vbase - size;
</code></pre>

<p>Even libcs can’t get <a href="http://blog.regehr.org/archives/767">undefined behaviour</a> straight.</p>

<h1 id="and-the-test-range">And the test range</h1>

<p>Ideally, we could expose the effect of both element size and element
count on the performance of these sort routines.  However, even if I
were to measure runtimes on the cross product of a range of element
size, range of element count, and a set of sort routine, I don’t know
how I would make sense of the results.  Instead, I’ll let element
sizes vary from one word to a few hundred bytes, and bin element
counts in a few representative ranges:</p>

<ol>
  <li>Tiny arrays: 4-7 elements.</li>
  <li>X-Small arrays: 8-15 elements;</li>
  <li>Small arrays: 16-31 elements</li>
  <li>Medium arrays: 32-64 elements.</li>
</ol>

<p>The test program times each sort routine on the same randomly
generated input (sampling with replacement from <code>random(3)</code>), for each
element size.  I try and smooth out outliers by repeating this process
(including regenerating a fresh input) 20 times for each element size and
count.</p>

<p>Regardless of the element size, the comparison function is the same:
it compares a single <code>unsigned</code> field at the head of the member.  I
chose this comparator to reflect real world workloads in which
comparisons are trivial.  When comparisons are slow, element size
becomes irrelevant and classical performance analyses apply.</p>

<p>The idea is to track, within each count bin, relative speedup/slowdown
compared to BSD’s quicksort: any difference in multiplicative factors,
as a function of element size, should show itself.  The element count
bins can instead highlight differences in additive factors.</p>

<h1 id="pretty-pictures">Pretty pictures</h1>

<p>A test program logged runtimes for this cross product of all element
sizes from 8 to 512 bytes (inclusively), all element counts (4 to 64,
inclusively), and various sort routines.  For each size/count pair,
the program regenerated a number of <code>random(3)</code> input; each sort
routine received a copy of the same input.  Given such random input,
we can expect all runtimes to scale with \(\Theta(n\log n)\), and it
makes sense to report cycle counts scaled by a baseline (BSD’s
quicksort).</p>

<p>I compiled each sort routine (with gcc 4.8.2 at <code>-O2</code>) separately from
the benchmark driver.  This prevented fancy interprocedural
optimisation or specialisation from taking place, exactly like we
would expect from calling to libc.  Everything below reports (scaled)
cycle counts on my machine, an E5-4617.  I executed the benchmark with
four processes pinned to different sockets, so the results should
reflect a regular single-threaded setting.  I definitely could have
run the benchmark on more machines, but I doubt that the relative
computational overhead of copying structs versus comparing two machine
words have varied much in the past couple years.</p>

<h2 id="finding-the-perfect-k">Finding the perfect k</h2>

<p>I first tested our k-ary heapsort with many values for <code>k</code>: 2-9, 12,
15-17.</p>

<p>When sorting 8-byte values, <code>k</code> doesn’t have too much of an impact.
Nevertheless, it’s clear that large k (12 and over) are slower than even
binary heapsort.  The sweetspot seems to be around 4 to 7.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_8s.png" /></p>

<p>Larger elements (32 and 64 bytes) show that the usual choice of 
<code>k = 2</code> causes more than a 30% slowdown when swaps are slow.  The
ideal <code>k</code> falls somewhere between 5 and 9 and between 6 and 17.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_32s.png" />
<img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_64s.png" /></p>

<p>Finally, at the extreme, with 512 byte elements, binary heapsort is
almost twice as slow as 7-ary to 17-ary heapsorts.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/heap_size_512s.png" /></p>

<p>The best choice of <code>k</code> will vary depending on how much slower it is to
swap two elements than to compare them.  A fixed value between 5 and 7
should be close to optimal for elements of 8 to 512 bytes.</p>

<h2 id="tweaks-to-gnu-quicksort">Tweaks to GNU quicksort</h2>

<p>I already mentioned that GNU’s quicksort has a slow swap macro, and
that it ends with an insertion sort pass rather than completely
sorting recursion leaves.  I tested versions of that quicksort with a
different swap function:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>swap.c </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
</pre></td><td class="code"><pre><code class="c"><span class="line"><span></span><span class="cp">#define CHUNK_SIZE 8</span>
</span><span class="line">
</span><span class="line"><span class="k">static</span> <span class="kr">inline</span> <span class="kt">void</span>
</span><span class="line"><span class="nf">SWAP</span><span class="p">(</span><span class="kt">char</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">size_t</span> <span class="n">n</span><span class="p">)</span>
</span><span class="line"><span class="p">{</span>
</span><span class="line">        <span class="kt">char</span> <span class="n">chunk</span><span class="p">[</span><span class="n">CHUNK_SIZE</span><span class="p">];</span>
</span><span class="line">        <span class="kt">size_t</span> <span class="n">i</span><span class="p">;</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">+</span> <span class="n">CHUNK_SIZE</span> <span class="o">&lt;=</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">CHUNK_SIZE</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">                <span class="n">memcpy</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">CHUNK_SIZE</span><span class="p">);</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line">
</span><span class="line">        <span class="k">for</span> <span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span><span class="line">                <span class="kt">char</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</span><span class="line">                <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</span><span class="line">        <span class="p">}</span>
</span><span class="line"><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I also tested versions that only stops recursing on inputs of size 1
(with <code>#define MAX_THRESH 1</code>).  This is a simple-minded way to
implement quicksort, but avoids the final insertion sort pass (which
proved too hairy to convert to the new swap function).</p>

<p>When elements are small (8 bytes), the trivial recursion base case
(GQL) is a bad idea; it’s slower than the original GNU quicksort (GQ).
The new swap function (GQS), however, is always useful.  In fact, the
combination of trivial base cases and improved swap (GQLS) is pretty
much identical to the original GNU quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_8s.png" /></p>

<p>Switching to larger elements (16, 32, 64 or 512 bytes) shakes things
up a bit.  The impact of faster swaps increases, and GQLS – the
version with faster swap and trivial base case – is markedly quicker
than the other variants.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_16s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_32s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_64s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/gnu_qs_512s.png" /></p>

<p>In the sequel, I will only consider the original GNU quicksort (GQ)
and the variant with trivial base case and faster swaps.  From now on,
I’ll refer to that variant as GQ’.</p>

<h2 id="sanity-checking-the-size-bins">Sanity checking the size bins</h2>

<p>I proposed to partition array lengths in 4 bins.  For that to work,
the behaviour of the sort routines must be fairly homogeneous within
each bin.</p>

<p>The graph below fixes the element size at 8 bytes and presents
normalised times for a few sort routines and a range of array sizes.
We report the (geometric) average of 20 runtimes, scaled by the
runtime for BSD quicksort on the same input, on a logarithmic scale.
The logarithmic scale preserves the symmetry between a routine that is
twice as slow as BSD quicksort (log ratio = 1) and one that is twice
as fast (log ratio = -1).  The sort routines are:</p>

<ul>
  <li>GM: GNU mergesort;</li>
  <li>GQ: GNU quicksort;</li>
  <li>GQ’: tweaked GNU quicksort;</li>
  <li>BM: BSD mergesort;</li>
  <li>BH: BSD heapsort;</li>
  <li>NH5: 5-ary heapsort;</li>
  <li>NH7: 7-ary heapsort.</li>
</ul>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_8s.png" /></p>

<p>The graph is pretty noisy for smaller arrays, but there’s no obvious
discrepancy within size bins.  The same holds for larger element sizes
(16, 64 and 512 bytes).</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_16s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_64s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/slowdown_size_512s.png" /></p>

<p>We can already see that BSD’s quicksort is hard to beat on the element
sizes considered above.</p>

<h2 id="impact-of-element-size-on-tiny-arrays">Impact of element size on tiny arrays</h2>

<p>The next graph shows the evolution of average (log) normalised sort
times for tiny arrays (4-7 elements), as the size of each element
grows from 8 to 512 bytes.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-512s.png" /></p>

<p>There’s some overplotting, but it’s clear that there are actually two
sets of curves: the main one, and another that only comprises sizes
that are multiples of 8 (word-aligned).  I believe this is caused by
special optimisations for nice element sizes in some (but not all) sort
routines, including our baseline, BSD’s quicksort.</p>

<p>For word-unaligned sizes, GNU’s mergesort and our tweaked GNU
quicksort are never slower than the baseline; for large items, they
are around 4x as quick.  On the same input, BSD’s mergesort and our
k-ary heapsorts are on par with one another and slightly slower than
the baseline, while GNU’s quicksort is slightly quicker.  The only
sort routine that’s markedly slower than BSD’s quicksort is BSD’s
binary heapsort (around 2-3x as slow).</p>

<p>BSD’s word-alignment optimisation makes sense in practice: larger
structs will often include at least one field that requires
non-trivial alignment.  I think it makes sense to look at arbitrary
sizes only up to a small limit (e.g., 32 bytes), and then only
consider word-aligned sizes.</p>

<p>For small element sizes, all but two routines are comparable to the
baseline.  BSD’s heapsort is markedly slower, and our tweaked GNU
quicksort somewhat faster.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-32s.png" /></p>

<p>When element sizes are word-aligned, BSD’s quicksort is hard to beat.
The only routine that manages to be quicker is GNU’s mergesort, for
very large element sizes: the routine exploits dynamic storage to sort
pointers and only permute elements in-place during a final linear-time
pass.  Our k-ary heapsorts remain in the middle of the pack, slightly
slower than BSD’s mergesort and GNU’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_4-128s.png" />
<img class="center" src="/images/2014-04-13-k-ary-heapsort/tiny_128-512s.png" /></p>

<h2 id="impact-of-element-size-on-extra-small-arrays">Impact of element size on extra small arrays</h2>

<p>The results in the previous section were noisy: that’s expected when
sorting arrays of 4 to 7 elements.  This section looks at arrays of 8
to 15 elements; there’s less random variations and the graphs are much
clearer.</p>

<p>The graph for overall speeds is similar to the one for tiny arrays.
However, it’s clear that, for general element sizes, our k-ary
heapsorts are slightly faster than BSD’s mergesort and a bit slower
than GNU’s quicksort.  Again, GNU’s mergesort and our tweaked
quicksort are even quicker than BSD’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_4-512s.png" /></p>

<p>When we only consider word-aligned element sizes, the picture changes.
The most important change is that our baseline is a lot quicker, and
even GNU’s mergesort is slightly slower than BSD’s quicksort.  Our
heapsorts are now slower than BSD’s mergesort, which is now comparable
to GNU’s quicksort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_4-128s.png" /></p>

<p>It’s only when we consider almost ridiculous element sizes that GNU’s
mergesort edges out BSD’s quicksort: GNU’s indirect sorting strategy
finally pays off.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/xsmall_128-512s.png" /></p>

<h2 id="finally-small-and-medium-arrays">Finally, small and medium arrays</h2>

<p>Small and medium arrays lead to the same conclusion as small ones,
only more clearly.  For arbitrary element sizes, GNU’s mergesort and
tweaked quicksorts are much quicker than everything else.  As for
k-ary heapsorts, they are comparable to BSD’s quicksort and quicker
than BSD’s mergesort and heapsort, except for small elements (fewer
than a dozen bytes each).</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_4-512s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_4-512s.png" /></p>

<p>For word-aligned element sizes, GNU’s quicksort is still only
outperformed by GNU’s indirect mergesort, while k-ary heapsorts are
still slightly slower than GNU’s quicksort and BSD’s mergesort, but
always quicker than the latter’s heapsort.</p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_4-128s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/small_128-512s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_4-128s.png" /></p>

<p><img class="center" src="/images/2014-04-13-k-ary-heapsort/medium_128-512s.png" /></p>

<h1 id="wrap-up">Wrap-up</h1>

<p>I’d say our results show that heapsort gets a bad rap because we
always implement binary heaps.  Higher arity (around 7-way) heaps are
just as easy to implement, and much more efficient when comparators
are trivial and data movement slow.  For small arrays of medium-size
elements, k-way heapsorts are competitive with BSD’s mergesort and
GNU’s quicksort, while guaranteeing \(\mathcal{O}(n \log n)\) worst-case performance
and without allocating storage; they’re almost magical when we only
need partial or incremental sorts.</p>

<p>There’s a more general conclusion from these experiments: <code>qsort(3)</code>’s
very interface is a bad fit for a lot of workloads.  <code>qsort</code> lets us
implement arbitrary comparisons with a callback, but must rely on
generic swap routines.  This probably reflects a mindset according to
which comparisons (logic) are slow and swaps (accessing data)
trivially quick; GNU’s quicksort was definitely written with these
assumptions in mind.  The opposite is often the case, nowadays.  Most
comparators I’ve written are lexicographic comparisons of machine
words, floating point values, and, very rarely, strings.  Such
comparators could be handled with a few specialised sort routines:
lexicographic sorts can either exploit stability (à la bottom-up radix
sort), or recursion (top-down radix sort).  However, there were some
cases when I could have implemented swaps more cleverly than with
<code>memcpy(3)</code>, and I could always have passed a size-specialised swap
routine.  There were also cases when I wanted to sort multiple arrays
with respect to the same key, and had to allocate temporary storage,
transpose the data, sort, and transpose back.  Mostly, this happened
because of hybrid
<a href="https://software.intel.com/en-us/articles/how-to-manipulate-data-structure-to-optimize-memory-use-on-32-bit-intel-architecture">struct of arrays</a>
data layout imposed by memory performance considerations, but
implementing the
<a href="http://c2.com/cgi/wiki?SchwartzianTransform">Schwartzian transform</a>
in C (which avoids recomputing keys) is basically the same thing.  A
swap function argument would be ideal: it’s quicker than generic
swaps, supports memory-optimised layouts, and helps express a classic
Perl(!) idiom for sorting on derived keys.</p>

<p>Behold, the modern sort interface:</p>

<pre><code>struct comparison {
        void *base;
        size_t stride; 
        union { void *ptr; unsigned long mask; } auxiliary;
        enum comparison_kind kind;
};

void
generic_sort(const struct comparison *cmp, size_t ncmp, size_t length,
        void (*swap)(void *, size_t i, size_t j), void *swap_data);
</code></pre>

<p>Interpreted DSLs for logic and compiled data shuffling; it’s the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How bad can 1GB pages be?]]></title>
    <link href="https://www.pvk.ca/Blog/2014/02/18/how-bad-can-1gb-pages-be/"/>
    <updated>2014-02-18T00:06:00-05:00</updated>
    <id>https://www.pvk.ca/Blog/2014/02/18/how-bad-can-1gb-pages-be</id>
    <content type="html"><![CDATA[<p>I joined the ad server team at <a href="http://www.appnexus.com/">AppNexus</a>
two weeks ago.  It’s a new problem domain for me, one that I find
pretty interesting so far: the workload is definitely on the
branchy/pointer-chasing side of things, and, although aggregate
throughput matters, we work under stringent latency goals.  There are
a few hash table lookups on the code path I’ve been working on, and
some micro-benchmarking revealed that 1GB pages could easily shave 25%
off the latency for lookups in large (hundreds of MB) tables.</p>

<p>That’s not too surprising.  Given any change, it’s usually easy to
design a situation that makes it shine.  I find it just as important to
ask the opposite question: what’s the worst slowdown I can get from
switching from regular 4KB pages to huge 1GB pages?  This way, I can
make more robust decisions, by taking into account both a good (if not
best) and a worst case. By the end of this post, I’ll multiply runtime
by 250% for the same operation, simply by switching from 4KB to 1GB
pages… with a setup is so contrived that it seems unlikely to occur by
accident.</p>

<p>But first, a quick overview of the benefits and downsides of huge
pages.</p>

<h1 id="why-are-huge-pages-interesting">Why are huge pages interesting?</h1>

<p>In short, manufacturers are adding huge pages because translating
virtual addresses to physical ones is slow.</p>

<p>On x86-64, the mapping from virtual to physical pages is represented
with a trie; each level dispatches on 9 bits (i.e., each node has 512
children), and leaves correspond to 4KB pages. There are 4 levels from
the root node to the leaves, which covers the (currently) standard 48-bit
virtual address space.</p>

<p>The address translation table is (mostly) stored in normal memory and
is too large to fit in cache. Thus, translating a virtual address
requires four reads, any of which can hit uncached memory.</p>

<p>This is where the translation lookaside buffer (TLB) comes in. On my
E5-4617, each core has 64 entries for regular 4KB pages in its L1
dTLB, and 512 (shared) entries in its L2 TLB. I don’t know if the TLBs
are exclusive or inclusive, but even if they’re exclusive, that’s only
enough for 2.25MB worth of data. Assuming that the working set is
completely contiguous (i.e., the best case), the TLB space for 4KB
pages is less than the total cache/core (2.5MB L3/core + 256 KB L2 +
32 KB L1D).</p>

<p>2MB and 1GB “huge pages” address this imbalance: nine 2MB pages
suffice to cover more address space than all the caches in a 6-core
E5-4617. However, there are only 32 L1dTLB entries for 2MB pages –
and 4 entries for 1GB pages – on my E5.</p>

<p>In addition to covering more address space in the TLB, huge pages
offer secondary benefits: there are fewer page table entries, and the
trie is shallower. Fewer page table entries means that a larger
fraction of memory can be used by data, rather than metadata, and that
the page table walk is more likely to stay in cache. Moreover, larger
pages are closer to the trie’s root: while the processor traverses 4
levels to get to a 4KB page, it only traverses 3 levels to reach a 2MB
page and 2 levels to for a 1GB page. These two effects compound to
make TLB misses quicker to handle.</p>

<h1 id="now-the-downsides">Now, the downsides…</h1>

<p>This idea that one must cover as much address space as possible with
the TLB is most relevant in two settings: trivially, if the working
set is completely covered by the (L1d)TLB, or, more interestingly,
when the access patterns show a lot of locality.  Examples of the
latter case are BLAS routines: with appropriate blocking, they can
usually access each page once or twice, but read almost every byte in
a page before switching to the next.</p>

<p>The opposite, worst, case would be something like lookups in a large
(too big for the TLB) hash table: we choose a virtual address
pseudorandomly and painstakingly translate it to a physical address,
only to read a handful of words from that page. In that situation, we
want as many TLB entries as possible, regardless of the address space
each one covers… and that’s where 4KB pages ought to shine. Taking
into account both the L1DTLB and the L2TLB, each core has 576 TLB
entries for 4KB (data) pages, versus 64x2MB and 4x1GB. Now, I don’t
know if the TLBs are exclusive or not, so I’ll assume the worst case
and work with 512*4KB entries.</p>

<p>The thing is, 512 TLB entries aren’t that many. If, by chance, our
hash table lookups keep hitting the same 512 pages, a contrived
microbenchmark will show that 4KB pages are a big win (but really, a
software cache might be a better way to exploit the situation). It’s
more likely that it’ll be hard to avoid TLB misses regardless of page
size, and huge pages then become useful because each TLB miss is
handled more quickly. Regardless, I’ll try to approximate this
worst-case behaviour to see how bad things can get.</p>

<h1 id="a-first-stab-at-pessimising-huge-pages">A first stab at pessimising huge pages</h1>

<p>Ideally, I would want to read from 512 (or a bit fewer) locations 1GB
apart, but I don’t have that much RAM. In the interest of realism, I
decided to “only” allocate 24GB.</p>

<p>My first microbenchmark follows: I allocate 24GB, divide that space in
512 chunks, and read the first word of each chunk in a loop. At first,
I didn’t even randomise the traversal order (so as to abuse LRU), but
there seems to be some prefetching for 1GB pages.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define _GNU_SOURCE
</span><span class="line">#include &lt;assert.h&gt;
</span><span class="line">#include &lt;stddef.h&gt;
</span><span class="line">#include &lt;stdlib.h&gt;
</span><span class="line">#include &lt;stdio.h&gt;
</span><span class="line">#include &lt;string.h&gt;
</span><span class="line">#include &lt;sys/mman.h&gt;
</span><span class="line">
</span><span class="line">#include "cycle.h"
</span><span class="line">
</span><span class="line">#ifndef MAP_HUGETLB
</span><span class="line"># define MAP_HUGETLB 0x40000
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">#ifndef MAP_HUGE_1GB
</span><span class="line"># define MAP_HUGE_1GB (30 &lt;&lt; 26)
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">#if defined(ONE_G)
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB | MAP_HUGE_1GB
</span><span class="line">#elif defined(TWO_M)
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB
</span><span class="line">#else
</span><span class="line"># define FLAGS MAP_ANONYMOUS | MAP_PRIVATE
</span><span class="line">#endif
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/512;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;20, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;20; i++) {
</span><span class="line">                size_t x = 512.0*random()/RAND_MAX;
</span><span class="line">                indices[i] = x*stride;
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 7; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;20; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The results: 1.13e10 cycles for 4KB pages, 1.60e10 for 2MB and 1.56e10
for 1GB. That’s only 40% more cycles… it’s bad, but not horrible. The
reason is that the data vector spans only 24x1GB, so 1/6th of the
random lookups will hit the 1GB TLB. Instead, let’s try and load from
each of these 24 pages, in random order. 24 pages will easily fit in
the L1DTLB for 4KB pages, but not in the 4 slots for 1GB pages.</p>

<h1 id="takes-two-to-six">Takes two to six</h1>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define NCHUNKS 24
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;20, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;20; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                indices[i] = (x*stride) % (24ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 7; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;20; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The results are even worse (better)! 4.82e9 cycles for 4KB pages,
versus 3.96e9 and 2.84e9 for 2MB and 1GB pages!</p>

<p>The problem is aliasing. The TLB on my E5 has limited way-ness (4-way,
I believe), so, by aligning everything to a 1GB boundary, the
effective size of the 4KB page TLB is 4 entries (same for 2MB). In a
way, this highlights the effect of page size when TLBs are useless
(random accesses to dozens or hundreds of GBs): 2MB pages shave 18%
off the runtime, and 1GB pages another 30%, for a total of 60% as much
time to handle a 1GB TLB miss versus 4KB.</p>

<p>Let’s try again, with <code>indices[i] = (x*stride + (x*4096)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);</code> on line 19.  I now find 1.14e9, 6.18e9 and 2.65e9 cycles. Much better!</p>

<p>For fun, I also tried to offset by 2MB increments, with <code>indices[i] =
(x*stride + (x&lt;&lt;21)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);</code>, and found 2.76e9,
1.30e9, and 2.85e9 cycles.</p>

<p>Finally, I tried</p>

<pre><code>            size_t offset = 4096 + (1ul&lt;&lt;21);
            indices[i] = (x*stride + (x*offset)%(1ul&lt;&lt;30)) % (24ul &lt;&lt; 30);
</code></pre>

<p>so that neither 4KB nor 2MB pages would alias, and got 1.13e9, 1.08e9
and 2.65e9 cycles. That’s 234% as much time for 1GB pages as for 4KB.</p>

<p>We’re close: this setup is such that 1GB pages cause a lot of TLB
misses, but neither 4KB nor 2MB pages do.  However, <code>perf stat</code> shows
there’s a lot of cache misses, and that probably reduces the
difference between 4KB and 1GB pages.</p>

<p>Let’s try one last thing, with <code>size_t offset = 4096 + (1ul&lt;&lt;21) +
64;</code> (to avoid aliasing at the data cache level), and a smaller index
vector that fits in cache.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
</pre></td><td class="code"><pre><code class=""><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (24ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 24ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 24ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;10, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;10; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                size_t offset = 4096 + (1ul&lt;&lt;21) + 64;
</span><span class="line">                indices[i] = (x*stride + ((x*offset)%(1ul&lt;&lt;30))) % (24ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 17; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;10; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We get 1.06e9, 9.94e8, and 2.62e9 cycles, i.e., 250% as much time with
1GB pages than 4KB ones.</p>

<p>We can easily turn this around: we just have to loop over more than 4
4KB-aligned locations in a 4GB space. For example, with</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
</pre></td><td class="code"><pre><code class=""><span class="line">#define NCHUNKS 4096
</span><span class="line">
</span><span class="line">int main (int argc, char **argv)
</span><span class="line">{
</span><span class="line">        (void)argc;
</span><span class="line">        (void)argv;
</span><span class="line">
</span><span class="line">        char acc = 0;
</span><span class="line">        size_t stride = (4ul &lt;&lt; 30)/NCHUNKS;
</span><span class="line">        char *data = mmap(NULL, 4ul &lt;&lt; 30,
</span><span class="line">                          PROT_READ | PROT_WRITE, FLAGS,
</span><span class="line">                          -1, 0);
</span><span class="line">        assert(data != MAP_FAILED);
</span><span class="line">        memset(data, 0, 4ul &lt;&lt; 30);
</span><span class="line">
</span><span class="line">        size_t *indices = calloc(1ul&lt;&lt;10, sizeof(size_t));
</span><span class="line">        for (size_t i = 0; i &lt; 1ul&lt;&lt;10; i++) {
</span><span class="line">                size_t x = NCHUNKS*random()/RAND_MAX;
</span><span class="line">                size_t offset = 64;
</span><span class="line">                indices[i] = (x*stride + ((x*offset)%(1ul&lt;&lt;30))) % (4ul &lt;&lt; 30);
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        ticks begin = getticks();
</span><span class="line">        for (size_t i = 0; i &lt; 1ul &lt;&lt; 17; i++) {
</span><span class="line">                for (size_t j = 0; j &lt; 1ul&lt;&lt;10; j++) {
</span><span class="line">                        acc += data[indices[j]];
</span><span class="line">                }
</span><span class="line">        }
</span><span class="line">        ticks end = getticks();
</span><span class="line">
</span><span class="line">        printf("%g %i\n", elapsed(end, begin), acc);
</span><span class="line">        return acc;
</span><span class="line">}</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With the above, I find 7.55e9 cycles for 4KB pages, 3.35e9 for 2MB and
1.09e9 for 1GB pages. Here, 4KB pages are almost 7x as slow as 1GB
pages.  If I instead let <code>size_t offset = 4096 + 64;</code> (to avoid
aliasing in the 4KB TLB), I get 4.72e9 cycles for 4KB pages, so still
433% as much time.</p>

<p>We can also play the same trick over 32*2MB = 64MB.  On my E5, I find
3.23e9 cycles for 4KB pages, versus 1.09e9 for 2MB and 1GB pages.
Eliminating page-level aliasing only brings the 4KB case down to
3.02e9 cycles, and doesn’t affect the other two cases.</p>

<h1 id="so-are-1gb-pages-generally-useful">So, are 1GB pages generally useful?</h1>

<p>The following table summarises the runtimes of all the variations above with
2MB and 1GB pages (as a fraction of the number of cycles for 4KB
pages).</p>

<center>
<table style="border-collapse: collapse;">
<col style="border:1px solid #000000;" />
<col style="border:1px solid #000000;" />
<tr><td>2MB/4KB&nbsp;</td> <td>&nbsp;1GB/4KB</td></tr>
<tr><td>1.42</td> <td>1.38</td></tr>
<tr><td>0.82</td> <td>0.59</td></tr>
<tr><td>5.42</td> <td>2.32</td></tr>
<tr><td>0.47</td> <td>1.03</td></tr>
<tr><td>0.96</td> <td>2.34</td></tr>
<tr><td>0.94</td> <td>2.47</td></tr>
<tr><td>0.44</td> <td>0.14</td></tr>
<tr><td>0.72</td> <td>0.23</td></tr>
<tr><td>0.34</td> <td>0.34</td></tr>
<tr><td>0.36</td> <td>0.36</td></tr>
</table>
</center>

<p>Overall, I think that I wouldn’t automatically switch to 2MB pages,
but that 1GB pages are a solid choice for machines that basically run
a single process at a time. When the data fits in 4GB, 1GB pages
completely eliminate TLB misses. When the data is even larger, 2MB and
1GB pages make page table walks quicker (by 18% and 40%,
respectively). It takes a very contrived situation – in which a
program keeps hitting fewer than 512 4KB-pages that are spread out
across multiple GBs – for smaller pages to be preferable.  The worst
I managed was 250% as much time for 1GB pages vs 4KB; in the other
direction, I achieved 693% as much time for 4KB pages versus 1GB, and
433% with a realistic situation (e.g., repeated lookups in a 4GB hash
table). Plus, there’s another interesting benefits from larger pages
that did not show up in this post: we get more control over aliasing
in data caches.</p>

<p>With multiple processes in play, there are fragmentation issues, and
things aren’t as clear-cut… especially given that 1GB pages must
currently be allocated at boot-time, on Linux.</p>

<p>I’m also still unsure how 1GB pages interact with NUMA. I’m
particularly worried about interleaving: interleaving at a 1GB
granularity seems unlikely to smooth out the ratio of local:remote
accesses as much as doing it at a 4KB granularity.</p>
]]></content>
  </entry>
  
</feed>
