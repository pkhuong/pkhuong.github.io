<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Approximation | Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/Blog/categories/approximation/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2025-08-24T19:57:31-04:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[The eight useful polynomial approximations of `sinf(3)']]></title>
    <link href="https://www.pvk.ca/Blog/2012/10/07/the-eight-useful-polynomial-approximations-of-sinf-3/"/>
    <updated>2012-10-07T23:35:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2012/10/07/the-eight-useful-polynomial-approximations-of-sinf-3</id>
    <content type="html"><![CDATA[<p>I just spent a few CPU-months to generate
<a href="https://github.com/pkhuong/polynomial-approximation-catalogue">these text files</a>.
They catalogue all the “interesting” (from an efficiency and accuracy
point of view) polynomial approximations of degree 16 or lower for a
couple transcendental functions, over small but useful ranges, in
single and double float arithmetic.  This claim seems to raise many
good questions when people hear it.</p>

<p>What’s wrong with Taylor approximations? Why the need to specify a
range?</p>

<p>Why are the results different for single and double floating point
arithmetic? Doesn’t rounding each coefficient to the closest float
suffice?</p>

<p>Why do I deem only certain approximations to be interesting and
others not, and how can there be so few?</p>

<p>In this post, I attempt to provide answers to these interrogations,
and sketch how I exploited classic
<a href="http://www.scienceofbetter.org/">operations research (OR)</a> tools to
enter the numerical analysts’ playground.</p>

<p>The final section describe how I’d interpret the catalogue when coding
quick and slightly inaccurate polynomial approximations.  Such lossy
approximations seem to be used a lot in machine learning, game
programming and signal processing: for these domains, it makes sense
to allow more error than usual, in exchange for faster computations.</p>

<p>The CL code is all up in the repository for
<a href="https://github.com/pkhuong/rational-simplex/tree/master/demo/branch-and-cut-fit">rational-simplex</a>,
but it’s definitely research-grade. Readers beware.</p>

<h1 id="minimax-approximations">Minimax approximations</h1>

<p><span class="pullquote-right" data-pullquote="Taylor approximations are usually easy to compute, but only provide good approximations over an infinitesimal range.">
The natural way to approximate functions with polynomials
(particularly if one spends time with physicists or engineers) is to
use truncated Taylor series.  Taylor approximations are usually easy to compute, but only provide good approximations over an infinitesimal range.  For example, the degree-1 Taylor approximation for \(\exp\)
centered around 0 is \(1 + x\).  It’s also obviously suboptimal, if
one wishes to minimise the worst-case error: the exponential function
is convex, and gradients consistently under-approximate convex functions.
</span></p>

<p><img class="center" src="/images/2012-10-07-the-eight-useful-polynomial-approximations-of-sinf-3/minimax.png" />
<!-- ggplot(data.frame(x=c(-1, 1)), aes(x)) + stat_function(fun=exp, aes(colour='exp')) + stat_function(fun=function(x) { 1+x}, aes(color='1 + x')) + stat_function(fun=function(x) { 1.26+1.18*x}, aes(colour='1.26 + 1.18x')) + scale_colour_manual("Function", value=c("black","blue", "red"), breaks=c("exp", "1 + x", "1.26 + 1.18x")) --></p>

<p>Another affine function, \(1.26 + 1.18x\), intersects \(\exp\) in
two points and is overall much closer over \([-1, 1]\).  In fact,
this latter approximation (roughly) minimises the maximal absolute
error over that range: it’s clearly not as good as the Taylor
approximation in the vicinity of 0, but it’s also much better in the
worst case (all bets are off outside \([-1, 1]\)).  That’s why I’m
(or anyone’s libm, most likely) not satisfied by Taylor polynomials,
and instead wish to compute approximations that minimise the error
over a known range; function-specific identities can be exploited to
reduce any input to such a range.</p>

<h2 id="computing-minimax-polynomials">Computing minimax polynomials</h2>

<p>As far as I know, the typical methods to find polynomial
approximation that minimise the maximal error (minimax polynomials)
are iterative algorithms in the style of the <a href="http://en.wikipedia.org/wiki/Remez_algorithm">Remez exchange algorithm</a>.
These methods exploit real analysis results to reduce the problem to
computing minimax polynomials over very few points (one per coefficient,
i.e. one more than the degree): once an approximation is found by
solving a linear equation system, error extrema are computed and used
as a basis to find the next approximation.  Given arbitrary-precision
arithmetic and some properties on the approximated function, the
method converges.  It’s elegant, but depends on high-precision
arithmetic.</p>

<p>Instead, I
<a href="http://pvk.ca/Blog/2012/05/24/fitting-polynomials-by-generating-linear-constraints/">reduce the approximation problem to a sequence of linear optimisation programs</a>.
Exchange algorithms solve minimax subproblems over exactly as many
points as there are coefficients: the fit can then be solved as a
linear equation.  I find it simpler to use many more constraints than
there are coefficients, and solve the resulting optimisation problem
subject to linear inequalities directly, as a
<a href="http://en.wikipedia.org/wiki/Linear_programming">linear program</a>.</p>

<p>There are obviously no cycling problems in this cutting planes
approach (the set of points grows monotonically), and all the
machinery from exchange algorithms can be reused: there is the same
need for a good set of initial points and for determining error
extrema.  The only difference is that points can always be added to
the subproblem without having to remove any, and that we can restrict
points to correspond to floating values (i.e. values we might actually
get as input) without hindering convergence.  The last point seems
pretty important when looking at high-precision approximations.</p>

<p>For example, let’s approximate the exponential function over \([-1, 1]\)
with an affine function.  The initial points could
simply be the bounds, -1 and 1.  The result is the line that passes by
\(\exp(-1)\) and \(\exp(1)\), approximately \(1.54 + 1.18x\).
The error is pretty bad around 0; solving for the minimax line over
three points (-1, 0 and 1) yields a globally optimal solution,
approximately \(1.16 + 1.18x\).</p>

<p><img class="center" src="/images/2012-10-07-the-eight-useful-polynomial-approximations-of-sinf-3/cutting-planes.png" />
<!-- ggplot(data.frame(x=c(-1, 1)), aes(x)) + stat_function(fun=exp, aes(colour='exp')) + stat_function(fun=function(x) { 1.54+1.18*x}, aes(color='1.54 + 1.18x')) + stat_function(fun=function(x) { 1.26+1.18*x}, aes(colour='1.26 + 1.18x')) + scale_colour_manual("Function", value=c("black","blue", "red"), breaks=c("exp", "1.54 + 1.18x", "1.26 + 1.18x")) --></p>

<p>There’s a lot of meat to wrap around this bone.  I use a
<a href="https://bitbucket.org/tarballs_are_good/computable-reals">computable reals</a>
package in Common Lisp to pull arbitrary-precision rational
approximations for arithmetic expressions; using libm directly would
approximate an approximation, and likely cause strange results.  A
<a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/branch-and-cut-fit/newton.lisp">variant of Newton’s algorithm</a>
(with bisection steps) converges to error extrema (points which can be
added to the linear subproblem); arbitrary precision real arithmetic
is very useful to ensure convergence down to machine precision here.
Each linear subproblem is solved with an
<a href="https://github.com/pkhuong/rational-simplex">exact simplex algorithm in rational arithmetic</a>,
and convergence is declared when the value of the error extrema found
in the Newton steps correspond to that estimated by the subproblem.
Finally, the fact that the input are floating-point values is
exploited by ensuring that all the points considered in the linear
subproblems correspond exactly to FP values: rather than only rounding
a point to the nearest FP value, its two immediately neighbouring
(predecessor and successor) FP values were also added to the
subproblem.  Adding immediate neighbours helps skip iterations in
which extrema move only by one <a href="http://en.wikipedia.org/wiki/Unit_in_the_last_place">ULP</a>.</p>

<p>Initialising the method with a good set of points is essential to
obtain reasonable performance.  The
<a href="http://en.wikipedia.org/wiki/Chebyshev_nodes">Chebyshev nodes</a> are
known to yield
<a href="http://www.uta.edu/faculty/rcli/papers/li2004.pdf">nearly optimal [PDF]</a>
initial approximations.  The LP-based approach can exploit a large
number of starting points, so I went with very many initial Chebyshev
nodes (256, for polynomials of degree at most 16), and, again,
adjoined three neighbouring FP values for each point.  It doesn’t seem
useful to me to determine the maximal absolute error very precisely,
and I declared convergence when the value predicted by the LP
relaxation was off by less than 0.01%.  Also key to the performance
were tweaks in the polynomial evaluation function to avoid rational
arithmetic until the very last step.</p>

<h1 id="exactly-represented-coefficients">Exactly represented coefficients</h1>

<p><span class="pullquote-right" data-pullquote="rounding coefficients can result in catastrophic error blowups.">
The previous section gives one reason why there are different tables
for single and double float approximations: the ranges of input
considered during optimisation differ.  There’s another reason that’s
more important, particularly for single float coefficients:
rounding coefficients can result in catastrophic error blowups.
</span></p>

<p>For example, rounding \(2.5 x\sp{2}\) to the nearest integer
coefficient finds either \(2 x\sp{2}\) or \(3 x\sp{2}\).  However \(2
x\sp{2} + x\sp{3}\) is also restricted to integer coefficients, but
more accurate over \([0, .5]\).  Straight coefficient-wise
rounding yields an error of \(\frac{1}{2} x\sp{2}\), versus
\(\frac{1}{2} x\sp{2} - x\sp{3}\) for the degree-3 approximation.
As the following graph shows, the degree-3 approximation is much
closer for the range we’re concerned with.</p>

<p><img class="center" src="/images/2012-10-07-the-eight-useful-polynomial-approximations-of-sinf-3/rounding.png" />
<!-- ggplot(data.frame(x=c(0, .5)), aes(x)) + stat_function(fun=function(x){.5*x^2}, aes(colour='|2.5 x^2 - 2 x^2|')) + stat_function(fun=function(x) { .5*x^2-x^3}, aes(color='|2.5 x^2 - (2 x^2 + x^3)|')) + scale_colour_manual("Error", breaks=c('|2.5 x^2 - 2 x^2|', '|2.5 x^2 - (2 x^2 + x^3)|'), value=c('red', 'blue')) --></p>

<p>Floating point values are more densely distributed than the integers
for the range of values we usually encounter in polynomial
approximations, but still discrete, and the same phenomenon crops up,
at a smaller scale.</p>

<p>The
<a href="http://www.marc.mezzarobba.net/m2/summary_chevillard.pdf">state of the art</a>
for this version of the approximation problem seems to be based on an
initial Remez step, followed by a reduction to a
<a href="http://en.wikipedia.org/wiki/Lattice_problem#Closest_vector_problem_.28CVP.29">CVP</a>.</p>

<p>The cutting planes approach points to a natural solution from the OR
world: a branch and cut method.  The issue with the cutting planes
solution is that, while I only generate points that correspond to FP
values, the decision variables (coefficients) are free to take
arbitrary rational values.</p>

<p>Branching can be used to split up a decision variable’s range and
eliminate from consideration a range of non-FP values.  The decision
variables’ upper and lower bounds are tightened in a large number of
subproblems; solutions gradually converge to all-FP values and their
optimality is then proven.</p>

<p>Embedding the cutting planes method in a branch and bound lets us find
polynomial approximations with float coefficients that minimise the
maximal error over float arguments, with correctly rounded powers.
The only remaining simplification is that we assume that the dot
product in the polynomial evaluation is error-free.  Sadly, removing
this simplification results in a horrible discrete optimisation
problem with a very bumpy objective function.  I’m not sure that any
exact approach can solve this in reasonable time.  Still, there are
ways to evaluate FP polynomials very accurately, and I’m mostly
interested in approximations to trade accuracy for speed, so rounding
errors may well be negligible compared to approximation errors.</p>

<h2 id="a-branch-and-cut-method-for-polynomial-approximations">A branch-and-cut method for polynomial approximations</h2>

<p>As in all
<a href="http://en.wikipedia.org/wiki/Branch_and_bound">branch and bound</a>
methods, a problem is split in subproblems by restricting the range of
some coefficient to exclude some infeasible (non-FP) values.  For
example, in the previous example (in which we’re looking for integer
coefficients), the 2.5 coefficient would lead to two subproblems, one
in which the degree-2 coefficient is at most 2 \((\lfloor 2.5\rfloor)\),
and another in which it’s at least 3 \((\lceil2.5 \rceil)\),
excluding all the fractional values between 2 and 3.  For floats,
we restrict to the closest floats that under- or over- approximate
the current value.</p>

<p>However, instead of solving the full continuous relaxation (which is
impractical, given the large number of potential argument values), our
subproblems are solved with cutting planes.  The trick is that cuts
(constraints, which correspond to argument values) from any branch can
be used everywhere else.  Thus, the global point pool is shared
between all subproblems, rather than re-generating it from scratch for
each subproblem.  In practice, this lifting of cutting planes to the
root problem seems essential for efficiency; that’s certainly what it
took for branch-and-cut MIP solvers to take off.</p>

<p>I generate cuts at the root, but afterwards only when a subproblem
yields all-FP values.  This choice was made for efficiency reasons.
Finding error extrema is relatively slow, but, more importantly,
adding cuts really slows down re-optimisation: the state of the
simplex algorithm can be preserved between invocations, and
warm-starting tends to be extremely efficient when only some
variables’ bounds have been modified.  However, it’s also necessary to
add constraints when an incumbent is all-FP, lest we prematurely
declare victory.</p>

<p>There are two really important choices when designing branch and bound
algorithms: how the branching variable is chosen, and the order in
which subproblems are explored.  With polynomial approximations, it
seems to make sense to branch on the coefficient corresponding to the
lowest degree first: I simply scan the solution and choose the
least-degree coefficient that isn’t represented exactly as a float.
Nodes are explored in a hybrid depth-first/best-first order: when a
subproblem yields children (its objective value is lower than the
current best feasible solution, and it isn’t feasible itself), the
next node is its child corresponding to the bound closest to the
branching variable’s value, otherwise the node with the least
predicted value is chosen.  The depth-first/closest-first dives
quickly converge to decent feasible solutions, while the best-first
choice will increase the lower bound, bringing us closer to proving
optimality.</p>

<p>A randomised rounding heuristic is also used to provide initial
feasible (all-FP) solutions, and the incumbent is considered close
enough to optimal when it’s less than 5% off from the best-known lower
bound.  When the method returns, we have a polynomial approximation
with coefficients that are exactly representable as single or double
floats (depending on the setting), and which (almost) minimises the
approximation error on float arguments.</p>

<h1 id="interesting-approximations">Interesting approximations</h1>

<p>The branch and cut can be used to determine an (nearly-)
optimally accurate FP polynomial approximation given a maximum degree.
However, the degree isn’t the only tunable to accelerate polynomial
evaluation: some coefficients are nicer than others.  Multiplying by 0
is obviously very easy (nothing to do), while multiplication by +/-1
is pretty good (no multiplication), and by +/- 2 not too bad either
(strength-reduce the multiplication into an addition).  It would be
possible to consider other integers, but it doesn’t seem to make
sense: on current X86, floating point multiplication is only 33-66%
slower (more latency) than FP addition, and fiddling with the exponent
field means ping-ponging between the FP and integer domains.</p>

<p>There are millions of such approximations with a few “nice”
coefficients, even when restricting the search to low degrees (e.g. 10
or lower).  However, the vast majority of them will be wildly
inaccurate.  I decided to only consider approximations that are at
least as accurate as the best approximation of degree three lower:
e.g. a degree-3 polynomial with nice coefficients is only
(potentially) interesting if it’s at least as accurate as a constant.
Otherwise, it’s most likely even quicker to just use a lower-degree
approximation.</p>

<p><span class="pullquote-right" data-pullquote="what&#8217;s the point in looking at a degree-4 polynomial with one coefficient equal to 0 if there&#8217;s a degree-3 with one zero that&#8217;s just as accurate?">
That’s not enough: this filter still leaves thousands of polynomials.
Most of the approximations will be dominated by another; what’s the point in looking at a degree-4 polynomial with one coefficient equal to 0 if there’s a degree-3 with one zero that’s just as accurate?  The relative importance
of the degree, and the number of zeroes, ones and twos will vary
depending on the environment and evaluation technique.  However, it
seems reasonable to assume that zeroes are always at least as quick to
work with as ones, and ones as quick as twos.  The constant
offset ought to be treated distinctly from other coefficients.  It’s
only added rather than multiplied, so any speed-up is lower, but an
offset of 0 is still pretty nice, and, on some architectures, there is
special support to load constants like 1 or 2.
</span></p>

<p>This lets me construct a simple but robust performance model: a
polynomial is more quickly evaluated than another if it’s of lower or
same degree, doesn’t have more non-zero, non-{zero, one} or non-{zero,
one, two} multipliers, and if its constant offset is a nicer integer
(0 is nicer than +/- 1 is nicer than +/- 2 is nicer than arbitrary
floats).</p>

<p>With these five metrics, in addition to accuracy, we have a
multi-objective optimisation problem.  In some situations, humans may
be able to determine a weighting procedure to bring the dimension down
to a scalar objective value, but the procedure would be highly
domain-specific.  Instead, we can use this partial order to report
solutions on the Pareto front of accuracy and efficiency: it’s only
worth reporting an approximation if there is no approximation that’s
better or equal in accuracy and in all the performance metrics.  The
performance and accuracy characteristics are strongly correlated
(decreasing the degree or forcing nice coefficients tends to decrease
the accuracy, and a zero coefficient is also zero-or-one, etc.), so
it’s not too surprising that there are so few non-dominated solutions.</p>

<h2 id="enumerating-potentially-interesting-approximations">Enumerating potentially-interesting approximations</h2>

<p>The branch and cut can be used to find the most accurate
approximation, given an assignment for a few values (e.g. the constant
term is 1, or the first degree coefficient 0).  I’ll use it as a
subproblem solver, in a more exotic branch and bound approach.</p>

<p>We wish to enumerate all the partial assignments that correspond to
not-too-horrible solutions, and save those.  A normal branch and bound
can’t be applied directly, as one of the choices is to leave a given
variable free to take any value.  However, bounding still works: if a
given partial assignment leads to an approximation that’s too
inaccurate, the accuracy won’t improve by fixing even more
coefficients.</p>

<p>I started with a search in which children were generated by adjoining
one fixed value to partial assignments.  So, after the root node,
there could be one child with the constant term fixed to 0, 1 or 2
(and everything else free), another with the first coefficient fixed
to 0, 1 or 2 (everything else left free), etc.</p>

<p>Obviously, this approach leads to a search graph: fixing the constant
term to 0 and then the first coefficient to 0, or doing in the reverse
order leads to the same partial assignment.  A hash table ensures that
no partial assignment is explored twice.  There’s still a lot of
potential for wasted computation: if bounding lets us determine that
fixing the constant coefficient to 0 is worthless, we will still
generate children with the constant fixed to 0 in other branches!</p>

<p>I borrowed a trick from the SAT solving and constraint programming
communities,
<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.30.2870">nogood recording</a>.
Modern SAT solvers go far beyond strict branching search: in practice,
it seems that good branching orders will lead to quick solutions,
while a few bad ones take forever to solve.  Solvers thus frequently
reset the search tree to a certain extent.  However, information is
still communicated across search trees via learned clauses.  When the
search backtracks (an infeasible partial assignment is found), a nogood
set of conflicting partial assignments can be computed: no feasible
solution will include this nogood assignment.</p>

<p>Some time ago, Vasek Chvatal introduced
<a href="http://dimacs.rutgers.edu/TechnicalReports/abstracts/1995/95-14.html">Resolution Search</a>
to port this idea to 0/1 optimisation, and Marius Posta, a friend at
CIRRELT and Université de Montréal,
<a href="https://www.cirrelt.ca/DocumentsTravail/CIRRELT-2009-16.pdf">extended it to general discrete optimisation [PDF]</a>.
The complexity mostly comes from the desire to generate partial
assignments that, if they’re bad, will merge well with the current set
of learned clauses.  This way, the whole set of nogoods (or, rather,
an approximation that suffices to guarantee convergence) can be
represented and queried efficiently.</p>

<p>There’s no need to be this clever here: each subproblem involves an
exact (rational arithmetic) branch and cut.  Simply scanning the set
of arbitrary nogoods to look for a match significantly accelerates the
search.  The process is sketched below.</p>

<p><a href="/images/2012-10-07-the-eight-useful-polynomial-approximations-of-sinf-3/search-tree.jpg"><img class="center" src="/images/2012-10-07-the-eight-useful-polynomial-approximations-of-sinf-3/search-tree-small.jpg" /></a></p>

<p>The search is further accelerated by executing multiple branch and
cuts and nogood scans in parallel.</p>

<p>The size of the search graph is also reduced by only fixing
coefficients to a few values.  There’s no point in forcing a
coefficient to take the value of 0, 1 or 2 (modulo sign) if it already
does.  Thus, a coefficient with a value of 0 is left free; otherwise a
child extending the partial assignment with 0 is created.  Similarly,
a child extended with a value of 1 is only generated if the
coefficient isn’t already at 0 or 1 (we suppose that multiplication by
0 is at least as efficient as by 1), and similarly for 2.  Finally,
coefficients between 0 and 1 are only forced to 0 or 1, and those
between 1 and 3 to 0, 1 or 2.  If a coefficient takes a greater
absolute value than 3, fixing it most probably degrades the
approximation too strongly, and it’s left free – then again such
coefficients only seem to happen on really hard-to-approximate
functions like \(\log\) over \([1, 2]\).  Also, the last
coefficient is never fixed to zero (that would be equivalent to
looking at lower-degree approximations, which was done in previous
searches).  Of course, with negative coefficients, the fixed values
are negated as well.</p>

<p>This process generates a large number of potentially interesting
polynomial approximations.  The non-dominated ones are found with a
straight doubly nested loop, with a slight twist: accuracy is computed
with machine floating point arithmetic, thus taking rounding into
account.  The downside is that it’s actually approximated, by sampling
fairly many (the FP neighbourhood of 8K Chebyshev nodes) points;
preliminary testing indicates that’s good enough for a relative error
(on the absolute error estimate) lower than 1e-5.  There tends to be a
few exactly equivalent polynomials (all the attributes are the same,
including accuracy – they only differ by a couple ULP in a few
coefficients); in that case, one is chosen arbitrarily.  There’s
definitely some low-hanging fruit to better capture the performance
partial order; the error estimate is an obvious candidate.  The hard
part was generating all the potentially interesting approximations,
though, so one can easily re-run the selection algorithm with tweaked
criteria later.</p>

<h1 id="exploiting-the-approximation-indices">Exploiting the approximation indices</h1>

<p>I’m not sure what functions are frequently approximated over what
ranges, so I went with the obvious ones: \(\cos\) and \(\sin\)
over \([-\pi/2, \pi/2]\), \(\exp\) and arctan over \([-1, 1]\)
or \([0, 1]\), \(\log\) over \([1, 2]\), and \(\log 1+x\) and
\(\log\sb{2} 1+x\) over \([0, 1]\).  This used up a fair amount of
CPU time, so I stopped at degree 16.</p>

<p>Each file reports the accuracy and efficient metrics, then the
coefficients in floating point and rational form, and a hash of the
coefficients to identify the approximation.  The summary columns are
all aligned, but each line is very long, so the files are best read
without line wrapping.</p>

<p>For example, if I were looking for a fairly good approximation of
degree 3 for \(\exp\) in single floats, I’d look at
<a href="https://github.com/pkhuong/polynomial-approximation-catalogue/blob/master/single/exp-degree-lb_error-non_zero-non_one-non_two-constant-error">exp-degree-lb_error-non_zero-non_one-non_two-constant-error</a>.</p>

<p>The columns report the accuracy and efficiency metrics, in the order
used to sort approximations lexicographically:</p>

<ol>
  <li>the approximation’s degree;</li>
  <li>the floor of the negated base-2 logarithm of the maximum error
(roughly bits of absolute accuracy, rounded up);</li>
  <li>the number of non-zero multipliers;</li>
  <li>the number of non-{-1, 0, 1} multipliers;</li>
  <li>whe number of non-{-2, -1, 0, 1, 2} multipliers;</li>
  <li>whether the constant’s absolute value is 0, 1, 2, or other (in which
case the value is 3); and</li>
  <li>the maximum error.</li>
</ol>

<p>After that, separated by pipes, come the coefficients in float form,
then in rational form, and the MD5 hash of the coefficients in a float
vector (in a contiguous vector, in increasing order of degree, with
X86’s little-endian sign-magnitude representation).  The hash might
also be useful if you’re worried that your favourite implementation
isn’t parsing floats right.</p>

<p>There are three polynomials with degree 3, and they all offer
approximately the same accuracy (lb_error = 10).  I’d choose between
the most accurate polynomial</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">exp(x) = 0.9994552 + 1.0166024 x + 0.42170283 x**2 + 0.2799766 x**3 # exp-74F7B9B7E0E73A804ABF6AC6C006BD98</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>or one with a nicer multiplier that doesn’t even double the maximum error</p>
<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">exp(x) = 1.0009761 + x + 0.4587815 x**2 + 0.2575481 x**3 # exp-D4C349D8F2C45EC0BE2154D1052EAA03</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>On the other hand, if I were looking for an accurate-enough
approximation of \(\log 1+x\), I’d open
<a href="https://github.com/pkhuong/polynomial-approximation-catalogue/blob/master/single/log1px-lb_error-degree-error-non_zero-non_one-non_two-constant">log1px-lb_error-degree-error-non_zero-non_one-non_two-constant</a>.</p>

<p>The columns are in the order used for the lexicographic sort:</p>

<ol>
  <li>the number of bits of accuracy;</li>
  <li>the approximation’s degree;</li>
  <li>the maximum error;</li>
  <li>the number of non-zero multipliers;</li>
  <li>the number of non-{-1, 0, 1} multipliers;</li>
  <li>the number of non-{-2, -1, 0, 1, 2} multipliers; and</li>
  <li>whether the constant’s absolute value is 0, 1, 2, or other (in which
case the value is 3).</li>
</ol>

<p>An error around 1e-4 would be reasonable for my needs, and
<code>log1px-6AE509</code> seems interesting: maximum error is around 1.5e-4,
it’s degree 4, the constant offset is 0 and the first multiplier 1.
If I needed a bit more accuracy (7.1e-5), I’d consider <code>log1px-E8200B</code>:
it’s degree 4 as well, and the constant is still 0.</p>

<p>It seems to me optimisation tools like approximation generators are
geared toward fire and forget usage.  I don’t believe that’s a
realistic story: very often, the operator will have a fuzzy range of
acceptable parameters, and presenting a small number of solutions with
fairly close matches lets them exploit domain-specific insights.  In
this case, rather than specifying fixed coefficients and degree or
accuracy goals, users can scan the indices and decide whether each
trade-off is worth it or not.  That’s particularly true of the single
float approximations, for which the number of possibilities tends to
be tiny (e.g. eight non-dominated approximations for
<a href="https://github.com/pkhuong/polynomial-approximation-catalogue/blob/master/single/sin-degree-lb_error-non_zero-non_one-non_two-constant-error">\(\sin\)</a>).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fitting polynomials by generating linear constraints]]></title>
    <link href="https://www.pvk.ca/Blog/2012/05/24/fitting-polynomials-by-generating-linear-constraints/"/>
    <updated>2012-05-24T17:35:00-04:00</updated>
    <id>https://www.pvk.ca/Blog/2012/05/24/fitting-polynomials-by-generating-linear-constraints</id>
    <content type="html"><![CDATA[<p><small>(There were some slight edits since the original publication.
The only major change is that the time to solve each linear program
probably grows cubically, not linearly, with the number of
points.)</small></p>

<p>This should be the first part in a series of post exploring the
generation of efficient and precise polynomial approximations.  Yes,
there is Lisp code hidden in the math (: See
<a href="https://github.com/pkhuong/rational-simplex">rational-simplex</a> and
<a href="https://github.com/pkhuong/rational-simplex/tree/master/demo/vanilla-fit">vanilla-fit</a>.</p>

<p>Here’s the context: we want to approximate some non-trivial function,
like \(\log\), \(\exp\) or \(\sin\) on a computer, in floating
point arithmetic.  The textbook way to do this is to use a truncated
Taylor series: evaluating polynomials is pretty quick on computers.
<a href="http://lol.zoy.org/blog/2011/12/21/better-function-approximations">Sam Hocevar</a>’s
post on the topic does a very good job of illustrating how
wrong-headed the approach is in most cases.  We should instead try to
find an approximation that minimises the maximal error over a range
(e.g. \([-\pi,\pi]\) for \(\sin\)).  He even has
<a href="http://lol.zoy.org/wiki/oss/lolremez">LolRemez</a> to automatically
compute such approximations for given functions.  (While we’re kind of
on the topic, the very neat
<a href="http://www2.maths.ox.ac.uk/chebfun/">Chebfun</a> applies closely-related
ideas to try and be to continuous functions what floating point
numbers are to reals.)</p>

<p>The LolRemez docs point out two clever tricks to get more efficient
approximations:
<a href="http://lol.zoy.org/wiki/doc/maths/remez/tutorial-changing-variables">forcing odd or even polynomials</a>,
and
<a href="http://lol.zoy.org/wiki/doc/maths/remez/tutorial-fixing-parameters">fixing parameters to simple values</a>.
Could we automate this process?</p>

<p>The goal of this series is to show that we can, by applying Operations
Research (<a href="http://www.scienceofbetter.org/">“The Science of Better”</a>)
to this micro-optimisation challenge.  OR is what I do during the day,
as a PhD student, and I believe that there’s a lot of missed
opportunities in nifty program optimisation or analysis.  (I also
think that language design has a lot to offer to OR people who cope
with modeling languages that have only changed incrementally since the
seventies. :)</p>

<h2 id="problem-statement">Problem statement</h2>

<p>For now, let’s try and solve the original problem: find, in a family
of functions \(F\) (e.g. polynomials of degree 4), an approximation
\(\tilde{f}\) of a function \(f\) over some bounded range
\(B\subset R\) such that \(\tilde{f}\) minimises the maximal error
over that range:</p>

<div>
$$\tilde{f} = \mathop{\arg\min}_{g \in F}\max_{x\in B} |g(x)-f(x)|.$$
</div>

<p>\(B\) is a dense set, with infinitely, if not uncountably, many
elements, so it’s far from obvious that we can solve this, even up to
negligible errors.</p>

<h2 id="the-remez-algorithm">The Remez algorithm</h2>

<p>The
<a href="http://en.wikipedia.org/wiki/Remez_algorithm">Remez exchange algorithm</a>
shows one way to solve the minimax approximation problem, from the realm
of mathematical analysis.</p>

<p>Let’s pretend we do not know \(\tilde{f}\), but do know the points
where its error from \(f\) is maximised.  In fact, if we are looking
for polynomials of degree \(n\), we only need \(n+2\) such points.
Let’s further impose that the errors alternate in sign: i.e., if the
points are ordered (\(x_0 &lt; \ldots &lt; x_i &lt; \ldots &lt; x_{n+1}\)), then
\(\tilde{f}(x_0) &lt; f(x_0)\), \(\tilde{f}(x_1) &gt; f(x_1)\), etc., or
vice versa.  Nifty analysis results tell us that these points exist,
and that they exhibit the same absolute error \(|\epsilon|\).  We
can now find the coefficients \(a_0,a_1,\ldots,a_n\) of
\(\tilde{f}\) by solving a small linear system:</p>

<div>
$$\sum_{j=0}^n a_j x_0^j = f(x_0) + \epsilon$$
$$\sum_{j=0}^n a_j x_1^j = f(x_1) - \epsilon$$
<p><center>etc. for each \(x_i\).</center></p>
</div>

<p>There are \(n+2\) variables (the \(n+1\) coefficients \(a_j\)
and \(\epsilon\)), and \(n+2\) constraints (one for each
extremum), so it’s a straight linear solve.</p>

<p>Of course, we don’t actually know these extremal error points
\(x_i\).  The Remez algorithm works because we can initialise them
with an educated guess, find the corresponding coefficients, construct
a new approximation from these coefficients, and update (some) of the
extremal points from the new approximation.  In theory this process
converges (infinitely) toward the optimal coefficients.  In practice,
numerical issues abound: solving a linear system is usually reasonably
stable, but finding exact function extrema is quite another issue.
The upside is that smart people like
<a href="http://lol.zoy.org/wiki/oss/lolremez">Sam</a> and the
<a href="http://www.boost.org/doc/libs/1_36_0/libs/math/doc/sf_and_dist/html/math_toolkit/toolkit/internals2/minimax.html">boost::math team</a>
have already done the hard work for us.</p>

<h2 id="a-linear-minimax-formulation">A linear minimax formulation</h2>

<p>The minimax objective can be reformulated as an optimisation problem
over reals (or rationals, really) with linear constraints and
objective, a
<a href="http://en.wikipedia.org/wiki/Linear_programming">linear program</a>:</p>
<div> $$\min_{e\in\mathbb{R}, a\in\mathbb{R}^{n+1}} e$$
subject to $$\sum_{i=0}^n a_ix^i \leq f(x) + e\qquad\forall x\in B$$
$$\sum_{i=0}^n a_ix^i \geq f(x) - e\qquad\forall x\in B$$ </div>

<p>In words, we’re trying to find an error value \(e\) and \(n+1\)
coefficients, such that the error \(|\tilde{f}(x)-f(x)|\quad\forall x\in
B\) is at most \(e\): \(-e \leq \tilde{f}(x)-f(x)\leq e\).  The
goal is to minimise that error value \(e\).</p>

<p>We’re still left with the issue that \(B\) is very large: infinite,
or even uncountable.  It doesn’t make sense to have two constraints
for each \(x\in B\)!  Our goal is to find approximations in floating
point arithmetic, which helps a bit: there’s actually a finite number
of floating point (single, double or otherwise) values in any range.
That number, albeit finite, still tends to be impractically large.
We’ll have to be clever and find a good-enough subset of constraints.
Note that, unlike the Remez algorithm, we can have as many constraints
as we want; we never have to remove constraints, so the quality of the
approximation can only improve.</p>

<p>This method is an interesting alternative to the Remez algorithm
because it always converges, finitely, even with (or rather, due to)
floating-point arithmetic.  It will also makes our future work on a
branch-and-cut method marginally easier.</p>

<h2 id="rational-simplex-for-exact-solutions-to-linear-programs">Rational simplex for exact solutions to linear programs</h2>

<p>We’ve reformulated our problem as a finite linear program, assuming
that we know for which point \(x\in B\) we should generate
constraints.  How can we find solutions?</p>

<p>LPs are usually solved with variants of the
<a href="http://en.wikipedia.org/wiki/Simplex_algorithm">simplex algorithm</a>, a
<em>combinatorial</em> algorithm.  This is the key part: while most of the
algorithm is concerned with numerical computations (solving linear
systems, etc.), these are fed to predicates that compare values, and
the results of the comparisons drive each iteration of the algorithm.
The key insight in the simplex algorithm is that there exists at least
one optimal solution in which all but a small set of variables (the
basis) are set to zero, and the value of the basic variables can then
be deduced from the constraints.  Better: every suboptimal (but
feasible) basis can be improved by only substituting one variable in
the basis.  Thus, each iteration of the algorithm moves from one basis
to a similar, but better, one, and numerical operations only determine
whether a basis is feasible and is an improvement.  Very little
information has to persist across iterations: only the set of basic
variables.  This means that there is no inherent accumulation of
round-off errors, and that we can borrow tricks from the computational
geometry crowd.</p>

<p><a href="http://www.dii.uchile.cl/~daespino">Daniel Espinoza</a> implemented
that, and much more, for his PhD thesis on exact linear and integer
program solvers.
<a href="http://www.dii.uchile.cl/~daespino/ESolver_doc/main.html">QSopt-Exact</a>
is a GPL fork of a fairly sophisticated simplex and branch and cut
program.  It exploits the fact that the simplex algorithm is
combinatorial to switch precision on the fly, using hardware floats
(single, double and long), software quad floats, multiprecision
floats, and rational arithmetic.  Unfortunately, the full solver
segfaults when reporting solution values on my linux/x86-64 machine.
Still, the individual simplex solvers (with double floats, quad
floats, multiprecision floats and rationals) work fine.  What I
decided to do was to sidestep the issue and call the solvers in
increasing order of precision, while saving the basis between calls.
The double float implementation is executed first, and when it has
converged (within epsilon), a more precise solver is called, starting
from the basis on which the previous solver converged, etc.  The final
rational solver is exact, but very slow.  Hopefully, previous solvers,
while inexact, will have converged to very nearly-optimal bases,
leaving only a couple iterations for the exact solver.</p>

<p><a href="https://github.com/pkhuong/rational-simplex">Rational-simplex</a> is a
one-file CL system that wraps QSopt-Exact and offers a trivial
modeling language.  We’ll use it a lot in the sequel.</p>

<h2 id="solving-the-minimax-lp-with-rational-simplex">Solving the minimax LP with rational-simplex</h2>

<p><a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/linf-fit.lisp">linf-fit.lisp</a>
implements a minimax linear fit in 60 LOC (plus a couple kLOC in QSopt
:).</p>

<p>Each
<a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/linf-fit.lisp#L20">point</a>
is defined by <code>loc</code>, the original value for <code>x</code>, a sequence of
parameters (in this case, powers of <code>x</code> as we’re building
polynomials), and the <code>value</code> to approximate, \(f(x)\).</p>

<p><a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/linf-fit.lisp#L41">solve-fit</a>
takes a sequence of such points, and solves the corresponding LP with
QSopt-Exact:</p>

<pre><code>(defun solve-fit (points)
  (lp:with-model (:name "linf fit" :sense :minimize)
    (let ((diff  (lp:var :name "diff" :obj 1))
          (coefs (loop for i below *dimension* collect
                       (lp:var :name (format nil "coef ~A" i)
                               :lower nil))))
      (map nil (lambda (point)
                 (let ((lhs (linexpr-dot coefs
                                         (point-parameters point)))
                       (rhs (point-value point)))
                   (lp:post&lt;= lhs (lp:+ rhs diff))
                   (lp:post&gt;= lhs (lp:- rhs diff))))
           points)
      (multiple-value-bind (status obj values)
          (lp:solve)
        (assert (eql status :optimal))
        (values obj (map 'simple-vector
                         (lambda (var)
                           (gethash var values 0))
                         coefs))))))
</code></pre>

<p>With a fresh model in scope in which the goal is to minimise the
objective function, a variable, <code>diff</code>, is created to represent the
error term \(e\), and one variable for each coefficient in the
polynomial (<code>coefs</code>) list; each coefficient is unbounded both from
below and above.</p>

<p>Then, for each point, the two corresponding constraints are added to
the current model.</p>

<p>Finally, the model is solved, and a vector of optimal coefficients is
extracted from the <code>values</code> hash table.</p>

<p>We can easily use <code>solve-fit</code> to <em>exactly</em> solve minimax instances
with a couple thousands of points in seconds:</p>

<pre><code>CL-USER&gt; (let ((*dimension* 5) ; degree = 4
               (*loc-value* (lambda (x)
                              (rational (exp (float x 1d0))))))
           ;; function to fit: EXP. Converting the argument to
           ;; a double-float avoid the implicit conversion to
           ;; single, and we translate the result into a rational
           ;; to avoid round-off
           (solve-fit (loop for x from -1 upto 1 by 1/2048
                       ;; create a point for x in 
                       ;; -1 ... -1/2048,0,1/2048, ... 1
                            collect (make-point x))))
dbl_solver: 0.510 sec 0.0 (optimal)      ; solve with doubles
ldbl_solver: 0.260 sec 0.0 (optimal)     ;   with long doubles
float128_solver: 0.160 sec 0.0 (optimal) ;   with 128 bit quads
mpf_solver: 1.110 sec 0.0 (optimal)      ;   with multi-precision floats
mpq_solver: 0.570 sec 0.0 (optimal)      ;   with rationals
3192106304786396545997180544707814405/5839213465929014357942937289929760178176
#(151103246476511404268507157826110201499879/151089648430913246511773502376932544610304
  28253082057367857587607065964265169406677/28329309080796233720957531695674852114432
  13800443618507633486386515045130833755/27665340899215071993122589546557472768
  9582651302802548702442907342912775/54033868943779437486567557708120064
  2329973989305264632365349185439/52767450140409606920476130574336)
</code></pre>

<p>Or, as more readable float values:</p>

<pre><code>CL-USER&gt; (values (float * 1d0) (map 'simple-vector (lambda (x)
                                                     (float x 1d0))
                                     (second /)))
5.466671707434376d-4 ; (estimated) error bound
#(1.0000899998493569d0 0.997309252293765d0 0.49883511895923116d0
  0.177345274179293d0 0.04415551600665573d0) ; coefficients
</code></pre>

<p>So, we have an approximation that’s very close to what
<a href="http://lol.zoy.org/wiki/doc/maths/remez/tutorial-exponential">LolRemez</a>
finds for \(\exp\) and degree 4.  However, we’re reporting a maximal
error of 5.466672e-4, while LolRemez finds 5.466676e-4.  Who’s wrong?</p>

<h2 id="tightening-the-model">Tightening the model</h2>

<p>As is usually the case, we’re (slightly) wrong! The complete LP
formulation has constraints for each possible argument \(x\in B\).
We only considered 4k equidistant such values in the example above.
The model we solved is missing constraints: it is a
<a href="http://en.wikipedia.org/wiki/Relaxation_%28approximation%29">relaxation</a>.
We’re solving that relaxation exactly, so the objective value it
reports is a lower bound on the real error, and on the real optimal
error.  At least, we don’t have any design constraint, and the
coefficients are usable directly.</p>

<p>We shall not settle for an approximate solution.  We must simply add
constraints to the model to make it closer to the real problem.</p>

<p>Obviously, adding constraints that are already satisfied by the
optimal solution to the current relaxation won’t change anything.
Adding constraints can only remove feasible solutions, and the current
optimal solution remains feasible; since it was optimal before
removing some candidates, it still is after.</p>

<p>So, we’re looking for constraints that are violated to add them to our
current model.  In the optimisation lingo, we’re doing constraint (or
row, or cut) generation.  In our problem, this means that we’re
looking for points such that the error from the current approximation
is greater than the estimate reported as our solution value.  We’ll
take a cue from the Remez algorithm and look for extremal violation
points.</p>

<p>Extrema of differentiable functions are found where the first
derivative is zero.  Our approximation function is a polynomial and is
trivially differentiated.  We’ll assume that the user provides the
derivative of the function to approximate.  The (signed) error term is
a difference of differentiable functions \(\tilde{f}-f\), and its
first derivative is simply the difference of the derivatives.</p>

<p>Assuming that the derivative is continuous, we can use the
<a href="http://en.wikipedia.org/wiki/Intermediate_value_theorem">intermediate value theorem</a>
to find its roots: for each root, there is a (non-empty, open)
neighbourhood such that the derivative is negative to the left of the
root and positive to the right, or inversely.</p>

<p>This is what
<a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/find-extrema.lisp">find-extrema.lisp</a>
implements.  In optimisation-speak, that’s our separation algorithm:
we use it to find (maximally-) violated constraints that we ought to
add to our relaxation.</p>

<p><a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/find-extrema.lisp#L71">find-root</a>
takes a function, a lower bound, and an upper bound on the root, and
first performs a
<a href="http://en.wikipedia.org/wiki/Bisection_method">bisection search</a>
until the range is down to 256 distinct double values.  Then, the 256
values in the range are scanned linearly to find the minimal absolute
value.  I first tried to use
<a href="http://en.wikipedia.org/wiki/Newton's_method">Newton’s method</a>, but
it doesn’t play very well with rationals: although convergence is
quick, denominators grow even more quickly.  There are also potential
issues with even slightly inexact derivatives.  This is why the final
step is a linear search.  The method will work as long as we have
correctly identified a tiny interval around the extremum; the interval
can be explored exhaustively without involving the derivative.</p>

<p><a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/find-extrema.lisp#L90">%approximation-error-extrema</a>
finds candidate ranges.  The sign of the derivative at each pair of
consecutive points currently considered in the model is examined; when
they differ, the range is passed to <code>find-root</code>.  If we find a new
extremum, it is pushed on a list.  Once all the pairs have been
examined, the maximal error is returned, along with the list of new
extrema, and the maximal distance between new extrema and the
corresponding bounds.  As the method converges, extrema should be
enclosed more and more tightly by points already considered in our
constraints.</p>

<p>There are frequent calls to
<a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/utility.lisp#L18">round-to-double</a>:
this function is used to take an arbitrary real, convert it to the
closest double value, and convert that value back in a rational.  The
reason we do that is that we don’t want to generate constraints that
correspond to values that cannot be represented as double floats: not
only are they superfluous, but they also tend to have large
denominators, and those really slow down the exact solver.</p>

<p>Finally, the two pieces are put together in
<a href="https://github.com/pkhuong/rational-simplex/blob/master/demo/vanilla-fit/driver.lisp">driver.lisp</a>.
The constraints are initialised from 4096 equidistant points in the
range over which we optimise.  Then, for each iteration, the
relaxation is solved, new extrema are found and added to the
relaxation, until convergence.  Convergence is easy to determine: when
the error reported by the LP (the relaxation) over the constraint
subset is the same as the actual error, we are done.</p>

<pre><code>CL-USER&gt; (time
          (let ((*trace-output* (make-broadcast-stream)))
            ;; muffle the simplex solver log
            (find-approximation 4 -1 1
                                (lambda (x)
                                  (rational (exp (float x 1d0))))
                                (lambda (x)
                                  (rational (exp (float x 1d0)))))))
          ;  predicted error  actual       difference           log distance of new points
Iteration    1: 5.4666716e-4 5.466678e-4 [6.5808034e-10] (4 new extrema, delta 40.42 bit)
Iteration    2: 5.4666746e-4 5.4666775e-4 [3.25096e-10] (4 new extrema, delta 41.15 bit)
Iteration    3: 5.466676e-4 5.466677e-4 [9.041601e-11] (4 new extrema, delta 40.89 bit)
Iteration    4: 5.4666764e-4 5.4666764e-4 [2.4131829e-11] (4 new extrema, delta 38.30 bit)
Iteration    5: 5.4666764e-4 5.4666764e-4 [5.3669834e-12] (4 new extrema, delta 39.04 bit)
Iteration    6: 5.4666764e-4 5.4666764e-4 [2.1303722e-12] (4 new extrema, delta 36.46 bit)
Iteration    7: 5.4666764e-4 5.4666764e-4 [8.5113746e-13] (4 new extrema, delta 37.19 bit)
Iteration    8: 5.4666764e-4 5.4666764e-4 [5.92861e-14] (4 new extrema, delta 36.93 bit)
Iteration    9: 5.4666764e-4 5.4666764e-4 [8.7291163e-14] (4 new extrema, delta 34.34 bit)
Iteration   10: 5.4666764e-4 5.4666764e-4 [2.7814624e-14] (4 new extrema, delta 35.08 bit)
Iteration   11: 5.4666764e-4 5.4666764e-4 [1.22935355e-14] (4 new extrema, delta 33.82 bit)
Iteration   12: 5.4666764e-4 5.4666764e-4 [6.363092e-15] (4 new extrema, delta 34.56 bit)
Iteration   13: 5.4666764e-4 5.4666764e-4 [2.6976767e-15] (4 new extrema, delta 31.97 bit)
Iteration   14: 5.4666764e-4 5.4666764e-4 [6.2273127e-16] (4 new extrema, delta 31.71 bit)
Iteration   15: 5.4666764e-4 5.4666764e-4 [3.1716093e-16] (4 new extrema, delta 32.44 bit)
Iteration   16: 5.4666764e-4 5.4666764e-4 [8.9307225e-17] (4 new extrema, delta 29.86 bit)
Iteration   17: 5.4666764e-4 5.4666764e-4 [2.986885e-17] (4 new extrema, delta 29.60 bit)
Iteration   18: 5.4666764e-4 5.4666764e-4 [8.047477e-17] (4 new extrema, delta 30.33 bit)
Iteration   19: 5.4666764e-4 5.4666764e-4 [0.0e+0] (4 new extrema, delta 27.75 bit)
Evaluation took:
  153.919 seconds of real time    ; includes time spent in the LP solvers
  43.669636 seconds of total run time (39.078488 user, 4.591148 system)
  [ Run times consist of 2.248 seconds GC time, and 41.422 seconds non-GC time. ]
  28.37% CPU
  245,610,372,128 processor cycles
  4,561,238,704 bytes consed
  
4046879766238553594956027226378928284211150354828640757733860080712233114910799/7402816194767429570294393430906636383377554724586974324818463099198772886301048832
#(925435306122623901577988826638313570454904960173764226165130358296546098660735739/925352024345928696286799178863329547922194340573371790602307887399846610787631104
  6557329860867414742326673804552068829044003331586895788335382835917/6575021589199030076761681434499916661609079970484769650357093605918
  1639925832159462899796569909691517124754835951733786891469617405863/3287510794599515038380840717249958330804539985242384825178546802959
  583024503858777554811473359621837746265838166280012537010086877260/3287510794599515038380840717249958330804539985242384825178546802959
  145161740826348143502214435074903924097159484388538508594721873208/3287510794599515038380840717249958330804539985242384825178546802959)
</code></pre>

<p>Note that we found new extrema in the last iteration.  However, the
error for those extrema wasn’t actually larger than for the points we
were already considering, so the solution was still feasible and
optimal.</p>

<p>The fractions will be more readable in floats:</p>

<pre><code>CL-USER&gt; (values (float * 1d0) (map 'simple-vector
                                    (lambda (x) (float x 1d0))
                                    (second /)))
5.466676005138464d-4
#(1.0000900001021278d0 0.9973092516744465d0 0.4988351170902357d0
  0.177345274368841d0 0.044155517622880315d0)
</code></pre>

<p>Now, we have nearly exactly (up to a couple bits) the same values as
<a href="http://lol.zoy.org/wiki/doc/maths/remez/tutorial-exponential">LolRemez’s example</a>.
Each iteration takes less than 10 seconds to execute; we could easily
initialise the relaxation with even more than 4096 points.  We should
expect the impact on the solution time to be cubic: most of the
simplex iterations take place in the floating point solvers, and the
number of iterations is usually linear in the number of variables and
constraints, but the constraint matrix is dense so the complexity of
each iteration should grow almost quadratically.  What really slows
down the rational solver isn’t the number of variables or constraints,
but the size of the fractions.  On the other hand, we see that some of
the points considered by our constraints differ by \(\approx 30\)
bit in their double representation: we would need a very fine grid
(and very many points) to hit that.</p>

<p>In addition to being more easily generalised than the Remez algorithm,
the LP minimax has no convergence issue: we will always observe
monotone progress in the quality of the approximation and, except for
potential issues with the separation algorithm, we could use the
approach on any function.  There is however (in theory, at least) a
gaping hole in the program: however improbable, all the extrema could
be sandwiched between two points for which the derivative has the same
sign.  In that case, the program will finish (there are no constraint
to adjoin), but report the issue.  A finer initial grid would be one
workaround.  A better solution would be to use a more robust
root-finding algorithm.</p>

<h2 id="whats-next">What’s next?</h2>

<p>The minimax polynomial fitting code is on github, as a
<a href="https://github.com/pkhuong/rational-simplex/tree/master/demo/vanilla-fit">demo</a>
for the
<a href="https://github.com/pkhuong/rational-simplex">rational-simplex</a>
system.  The code can easily be adapted to any other basis (e.g. the
Fourier basis), as long as the coefficients are used linearly.</p>

<p>In the next instalment, I plan to use the cut (constraint) generation
algorithm we developed here in a
<a href="http://en.wikipedia.org/wiki/Branch_and_cut">branch-and-cut</a>
algorithm.  The new algorithm will let us handle side-constraints on
the coefficients: e.g., fix as many as possible to zero, 1, -1 or
other easy multipliers.</p>

<p>If you found the diversion in Linear programming and cut generation
interesting, that makes be very happy!  One of my great hopes is to
see programmers, particularly compiler writers, better exploit the
work we do in mathematical optimisation.  Sure, “good enough” is nice,
but wouldn’t it be better to have solutions that are provably optimal
or within a few percent of optimal?</p>

<p>If you want more of that stuff, I find that Chvatal’s book
<a href="http://books.google.com/books?id=DN20_tW_BV0C">Linear Programming</a> is
a very nice presentation of linear programming from both the usage and
implementation points of view; in particular, it avoids the trap of
simplex tableaux, and directly exposes the meaning of the operations
(its presentation also happens to be closer to the way real solvers
work).  Works like Wolsey’s
<a href="http://books.google.com/books?id=x7RvQgAACAAJ">Integer Programming</a>
build on the foundation of straight linear programs to tackle more
complex problems, and introduce techniques we will use (or have used)
here, like branch-and-bound or cut generation.  The classic
undergraduate-level text on operations research seems to be Hillier
and Lieberman’s
<a href="http://books.google.com/books?id=SrfgAAAAMAAJ">Introduction to operations research</a>
(I used it, and so did my father nearly 40 years ago)… I’m not sure
that I’m a fan though.</p>
]]></content>
  </entry>
  
</feed>
