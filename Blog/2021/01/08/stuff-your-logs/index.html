
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Stuff your logs - Paul Khuong: some Lisp</title>
  <meta name="author" content="Paul Khuong">
  <meta name="description" content="Paul Khuong's personal blog. Some Lisp, some optimisation, mathematical or computer.">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://www.pvk.ca/Blog/2021/01/08/stuff-your-logs/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Paul Khuong: some Lisp" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Poller+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Germania+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fontdiner+Swanky&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Lato&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Cardo&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Sorts+Mill+Goudy&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=EB+Garamond&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Della+Respira&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=UnifrakturMaguntia&subset=all&display=fallback" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Arimo|EB+Garamond|PT+Sans+Caption&subset=latin,cyrillic&display=fallback' rel='stylesheet' type='text/css'>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
    Macros: {
     sp: "^",
     sb: "_"
    }
  }});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<meta name="twitter:dnt" content="on">

</head>

<body >
  <header role="banner"><hgroup>
  <h1><a href="/">Paul Khuong: some Lisp</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/Blog/archives">Archives</a></li>
  <li><a href="/atom.xml" title="subscribe via RSS">RSS</a></li>
</ul>

<br>

      
        <form action="https://google.com/search" method="get">
          <fieldset role="search">
            <input type="hidden" name="q" value="site:https://www.pvk.ca" />
      
      
            <input class="search" type="text" name="q" results="0" placeholder="Search" aria-label="Search"/>
          </fieldset>
        </form>
  
</nav>
  <div id="main">
    <div id="content">
      
      <div>
<article class="hentry" role="article">
  
  <header>
    
      
        <h1 class="entry-title" style="font-family: ">Stuff your logs</h1>
      
    
    
      <p class="meta">
        





Jan
  
8th, 
2021




        
         | <a href="#disqus_thread"
              data-disqus-identifier="https://www.pvk.ca/Blog/2021/01/08/stuff-your-logs/"
	      >Comments</a>
        
        
      </p>
    
  </header>


<p><em>This is a draft (essai ;) post.  Feel free to share it with people, but I would prefer to avoid aggregators.  Draft posts do not have stable URLs, and some may never make it out of that stage; you may instead want to link to the <a href="/Blog/drafts/index.html">draft category page</a>.</em></p>


<div class="entry-content" style="font-family: ; font-size: "><p>Having more than satisfied the <a href="https://wiki.c2.com/?RuleOfThree">rule of three</a>, I feel comfortable claiming
that one’s representation of first resort for binary logs
(write-ahead, recovery, replay, or anything else that may be
consumed by a program) should use a 
<a href="https://en.wikipedia.org/wiki/Self-synchronizing_code">self-synchronising code</a>.</p>

<p>At <a href="https://www.backtrace.io">Backtrace</a>, we’ve settled on a variant of 
<a href="http://www.stuartcheshire.org/papers/COBSforToN.pdf">consistent overhead byte stuffing (COBS)</a>
that improves encoding and decoding speed at the expense of higher buffering
requirements (up to 64KB instead of 254 bytes).  We target servers and
always access full records at a time, so this downside is a non-issue for us.</p>

<p>I’ll describe the format later in this post.  Bur first, why do I
make that claim?</p>

<h2 id="why-is-self-synchronisation-important">Why is self-synchronisation important?</h2>

<p>Logs are never more useful than when things go wrong, so it’s crucial
that they be robust and simple to write. 
<a href="https://en.wikipedia.org/wiki/Self-synchronizing_code">Self-synchronising encodings</a>
like <a href="https://en.wikipedia.org/wiki/Consistent_Overhead_Byte_Stuffing">COBS</a>
can not only recover from overwritten data (e.g., when a network
switch flipped a bit, or weak error handling in a filesystem results
in silently zero-filling a page), but also from garbage inserted in
the middle of a stream, and even from vanishing bytes (e.g., a
couple bytes lost to incorrect ad hoc backup logic).</p>

<p>A COBS encoding for log records achieves all that by unambiguously
separating records with a reserved byte (e.g., 0), and encoding each
record to avoid that separator byte.  A reader can thus assume that a
potential record starts at a log file’s first byte, that a record
ends at the last byte, and otherwise look for separator bytes
to know where to cut records.  When data is lost or corrupt, the only
records affected are those that immediately surround the bad data: the
reader’s re-synchronises to a good state as soon as it finds a
valid separator byte in the stream of log byte.</p>

<p>This encoding does not reliably detect invalid records; it only
guarantees that readers can always identify where valid records begin and
end, when these records are not corrupt nor next to corrupt
(overwritten, missing, or inserted) data.  Corruption (e.g., a
duplicated page) may result in hallucinating framed data; higher-level
code should compare checksums once potential records are identified,
and the log consuming logic itself should handle duplicate data.</p>

<p>On the write side, the encoding logic is simple (couple dozen lines of
C code), and uses a predictable amount of space, as expected from an
algorithm suitable for microcontrollers.</p>

<p>Actually writing encoded data is also simple: on POSIX filesystems, we can
make sure each record is delimited (e.g., prefixed with the
delimiter byte), and issue a
<a href="https://pubs.opengroup.org/onlinepubs/007904875/functions/write.html">regular <code>O_APPEND</code> write(2)</a>.
Vectored writes can even insert delimiters without copying in
userspace.</p>

<p>When a write errors out, we can blindly (maybe once or twice) try
again: the encoding is independent of the output file’s state.  When a
write is cut short, we can just issue the same<sup id="fnref:suffix" role="doc-noteref"><a href="#fn:suffix" class="footnote">1</a></sup> write call: the
encoding and the read-side logic already protect against that kind of
corruption.</p>

<p>What if multiple threads or processes write to the same log file? If
we <a href="https://pubs.opengroup.org/onlinepubs/007904875/functions/open.html">open with <code>O_APPEND</code></a>, the operating system can handle the rest.  This
doesn’t make contention disappear, but at least we’re not adding
a bottleneck in userspace on top of what is necessary to append to
the same file.</p>

<p>This simplicity also plays well 
<a href="https://kernel.dk/io_uring.pdf">high-throughput I/O primitives like <code>io_uring</code></a>, and with 
<a href="https://docs.microsoft.com/en-us/rest/api/storageservices/append-block">blob stores that support appends</a>:
independent workers can concurrently queue up blind append requests, and
retry on failure.  There’s no need for application-level concurrency
control.</p>

<h2 id="fun-tricks-with-robust-readers">Fun tricks with robust readers</h2>

<p>Our log encoding lets readers recover from missing bytes; we also
assume that readers rejec invalid records, and that the log processing
logic itself handles duplicate records.  These are table stakes for a
reliable log consumer.</p>

<p>What I find interesting is that we can turn this robustness into
new capabilities.</p>

<p>It’s often useful to process the tail of a log at a regular cadence.
For example, I once maintained a system that regularly tailed hourly
logs to maintain approximate views up to date, and re-computed
everything from scratch once the data was old enough, to make sure the
views were eventually accurate. One could support that use case with
length footers on each record… however, with COBS framing, we can
also start scanning for a valid record from an arbitrary byte
location, and then process the rest of the data.</p>

<p>When logs grow large enough, we want to process them in parallel.  A
simple solution is to shard log streams, but that unfortunately
couples the parallelisation and storage strategies, and
adds complexity to the write side.  With COBS framing, we can partition a
data file arbitrarily (e.g., in fixed size chunks) for independent
workers.  A worker will scan for the first valid record starting at or
after its start byte, and handle every record that <em>starts</em> in its
assigned range; this last condition guarantees we’ll process records that
straddle chunk boundaries.</p>

<p>In fact, random access even lets us implement a form of binary or
interpolation search on raw unindexed logs, when we known the records
are (k-)sorted on the search key!</p>

<p>Eventually, we might also want to truncate our logs.  We can do so by
simply punching a hole at the head of log files, or
<a href="https://man7.org/linux/man-pages/man2/fallocate.2.html">collapsing old data away</a>.
Filesystems can only execute these operations at coarse granularities,
but it’s safe to leave garbage behind for readers to ignore, as long
as they implement logic to skip sparse holes.</p>

<h2 id="backtraces-variant-of-byte-stuffing"><a href="https://www.backtrace.io">Backtrace</a>’s variant of byte stuffing</h2>

<p><a href="http://www.stuartcheshire.org/papers/COBSforToN.pdf">Cheshire and Baker’s original byte stuffing scheme</a>
targets small machines and slow transport (amateur radio and
phone lines).  That’s why it bounds the amount of buffering needed to
254 bytes for writers and one 1-byte counter for readers, and
attempts to minimise space overhead, beyong its worst-case bound of
0.4%.</p>

<p>The algorithm is also reasonable. A writer buffers data until it
encounters a forbidden 0 (delimiter) “stuff” byte, or there’s 254
bytes of buffered data.  Whenever a writer stops buffering, it outputs a
block, whose contents are described by its first byte.  If the
writer stopped buffering because it found a “stuff” byte, it emits one
byte with <code>buffer_size + 1</code> before writing and clearing the
buffer.  Otherwise, it outputs 255 (one more than the buffer size),
followed by the buffer’s contents.  On the read side, we know that the
first byte we read describes the current block of data (255 means
254 bytes of literal data, any other value is one more than the number
of literal bytes to copy, followed by a “stuff” byte).  We denote the
end of a record with an implicit delimiter: when we run out of data to
decode, we should have just decoded an extra delimiter byte that’s not
really part of the data.</p>

<p>We have different goals for the software we write at <a href="https://www.backtrace.io">Backtrace</a>.
For our logging use cases, we pass wround fully constructed records,
and we want to issue a single atomic write syscall per record.
Buffering is baked in, so there’s no point in making sure we can work
with a small write buffer.  We also don’t care as much about the space
overhead (the worst-case bound is already pretty good) as much as we
do about encoding and decoding speed.</p>

<p>These different design goals lead us to an updated hybrid word/byte
stuffing scheme.</p>

<p>In the past, I’ve seen
<a href="https://issues.apache.org/jira/browse/AVRO-27">“word” stuffing schemes</a>
that aim to reduce the runtime overhead of COBS codecs by scaling up
the COBS loops to work on two or four bytes at a time.  However,
searching for a byte is trivial to vectorise, and there is no
guarantee that frameshift corruption will be aligned to word
boundaries (for example, POSIX allows short writes to stop after an
arbitrary number of bytes).</p>

<p>Our hybrid word-stuffing instead looks for a forbidden two-byte
delimiter sequence at arbitrary byte offsets.  We must still
conceptually process bytes one at a time, but we can now encode
wider spans of literal data (up to almost \(2^{16} =
64\textrm{KB}\)), and that’s a win on larger records: short
variable-length <code>memcpy</code> aren’t great.</p>

<p>Delimiting with a pair of bytes instead of only one byte also makes it
easier to carefully select a delimiter that’s unlikely to appear in our data.
Cheshire and Baker aim for the opposite, and suggest to use a frequent
byte (0) to eliminate the space overhead in the common case.  We care
a lot more about encoding and decoding speed, so an unlikely delimiter
makes more sense for us.  We picked <code>0xfe 0xfd</code> because that sequence 
doesn’t appear in small integer (unsigned, two’s complement, varint)
regardless of endianness, nor in valid UTF-8 data.</p>

<p>With a 2-byte forbidden sequence, we can encode the size of each block
in radix 253 (<code>0xfd</code>), which goes up to \(253^2 - 1 = 64008\).
That’s a reasonable size for <code>memcpy</code>.  With this radix conversion, we
don’t need any off-by-one weirdness: that part of the original COBS
algorithm actually convert values in \([0, 254]\) to one byte while
avoiding the stuff byte, 0.</p>

<p>A two-byte size prefix is a bit ridiculous for small records (our
records tend to be on the order of 30-50 bytes). We encode the first
block specially, with a single radix-253 byte for the size prefix.
Since the stuff sequence <code>0xfe 0xfd</code> is unlikely to appear in our
data, the encoding for short record often boils down to adding a byte-sized 
length prefix.</p>

<p>The encoding algorithm is only slightly more complex than the original
COBS scheme.</p>

<p>Assume the data to encode has been appended with the 2-byte stuff
sequence <code>0xfe 0xfd</code>.</p>

<p>For the first block, look for the stuff sequence in the first 252
bytes.  If we find it, emit its position (must be less than 251) in
one byte, then all the data bytes up to but not including the stuff
sequenceg, and resume regular encoding after the stuff sequence.  If
the sequence isn’t in the first block, emit <code>252</code>, followed
by 252 bytes of data, and resume regular encoding after those bytes.</p>

<p>For regular (all but the first) blocks, look for the stuff sequence in
the next 64008 bytes.  If we find it, emit the sequence’s byte offset
(must be less than 64008) in little endian radix 253, followed by the
data up to but not including the stuff sequence, and resume encoding
after the sequence.  If we don’t find the stuff sequence, emit 64008
in radix 253 (<code>0xfc 0xfc</code>), copy the next 64008 bytes of data, and
resume encoding immediately after the data.</p>

<p>Remember that we conceptually padded the data with a stuff sequence at
the end.  This means we’ll always observe that we fully consumed the
input data at a block boundary.  When we encode the block for the
final stuff sequence, we stop (and append a stuff sequence to
delimit the end of the record).</p>

<p>You can find our <a href="https://gist.github.com/pkhuong/8736396cee3c54f0a12840e5b8978697#file-word_stuff-h">MIT-licensed implementation in this gist</a>.</p>

<h2 id="a-resilient-record-stream-on-top-of-word-stuffing">A resilient record stream on top of word stuffing</h2>

<p>The stuffing scheme only provides resilient framing.  That’s good, but
not enough for a record abstraction.  At the very least, we need a
checksum, to detect invalid records that happen to be correctly
encoded (e.g., a non-stuff literal byte was overwritten with another
non-stuff byte).</p>

<p>Our (pre-stuffed) records start with the little-endian header</p>

<pre><code>struct record_header {
        uint32_t crc;
        uint32_t generation;
};
</code></pre>

<p>where <code>crc</code> is the <code>crc32c</code> of whole record, including the
header,<sup id="fnref:initialize-your-crc" role="doc-noteref"><a href="#fn:initialize-your-crc" class="footnote">2</a></sup> and the <code>generation</code> is a yet-unused
arbitrary 32-bit payload that we added for forward compatibility.
There is no size field: the framing already handles that.</p>

<p>The remaining bytes in a record are an arbitrary payload.  We use
protobuf messages to help with schema evolution (and keep them small
and flat for decoding performance), but there’s no special coupling
between the stream of word-stuffed records and the payload’s 
format.</p>

<p>For writers, <a href="https://gist.github.com/pkhuong/8736396cee3c54f0a12840e5b8978697#file-record_stream-h">our MIT-licensed implementation</a>
can write to buffered <code>FILE</code> streams, or directly to file descriptors.
Buffered streams offer higher write throughput, but are only safe
if synchronisation and flushing happens at a higher level; we use them
as part of a commit protocol that handles
<a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/fsync.html">fsync</a>
and publishes data with
<a href="https://pubs.opengroup.org/onlinepubs/009695399/functions/rename.html">atomic <code>rename</code> syscalls</a>.
During normal operations, we instead use file descriptors opened with
<code>O_APPEND</code> and a background fsync worker: in practice, the kernel is
more stable than our software, so it’s more important that log records
immediately reach the kernel than all the way to persistent storage.</p>

<p>For readers, we can either read from a buffer, or <code>mmap</code> a file
descriptor in, and read from the resulting buffer.  While we expose a
linear iterator interface, we can also override the start and stop
byte offset of an iterator; we use that capability to replay logs in
parallel.  Finally, when readers advance an iterator they choose to
receive a raw data buffer, or have it decoded with a protobuf descriptor.</p>

<h2 id="whats-next">What’s next?</h2>

<p>We have happily been using this log format to store metadata records
that we replay every time the <a href="https://www.backtrace.io">Backtrace</a> server restarts.
Decoupling writes from the parallel read strategy let us work on
startup times incrementally without any hard migration.  The use of a
real schema language (protocol buffers) also made it easier to start
small and incrementally add more optional metadata, before forcing a
hard switch-over.</p>

<p>This piece-meal approach let us transition from a length-prefixed
data format to one where all important metadata lives in a record
stream, without breaking backward compatibility.  We slowly added more
metadata to the record stream and parallelised loading from the
metadata record stream, while preserving backward compatibility.
Finally, nearly 6 months later, we flipped the switch and made the
new, more robust, format mandatory; the old length-prefixed data files
still exist, but are now treated as bags of arbitrary checksummed
data bytes, with all the control metadata in record streams.</p>

<p>By now, we’ve had a respectable amount of pleasant operational
experience with the format, and, while performance is more than good
enough for us (the parallel loading phase is bottlenecked on disk I/O
and <code>protobuf-c</code>), we also know there’s plenty of
headroom: short records can usually be decoded without any write, and
<em>all</em> records could be decoded in-place with a single byte of head
padding.</p>

<p>We’re now starting to work on distributing our single-node embedded
database and making it interact more easily with other data stores.
The first step will be generating a
<a href="https://materialize.com/change-data-capture-part-1/">change data capture stream</a>,
and re-using the word-stuffed record format was an obvious choice.</p>

<p>Word stuffing is simple, efficient, and robust.  If you can’t just
rely on a real database (maybe you’re trying to write one yourself)
for your data stream, give it a shot!  Feel free to use our
MIT-licensed code, or simply write your own implementation.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:suffix" role="doc-endnote">
      <p>If you append with the delimiter, it probably makes sense to special-case short writes and prepend with the delimiter after failures, to let readers resynchronise. <a href="#fnref:suffix" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:initialize-your-crc" role="doc-endnote">
      <p>We overwrite the <code>crc</code> field with <code>UINT32_MAX</code> before computing a checksum for the header and its trailing data.  It’s important to avoid zero prefixes because the result of crc-ing a 0 byte into a 0 state is… 0. <a href="#fnref:initialize-your-crc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>


  <footer class="page-footer">
    <p class="meta">
      
<span class="byline author vcard">Text authored by <span class="fn">Paul Khuong</span></span>


      





Jan
  
8th, 
2021




      
      


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
      
      
        <a class="basic-alignment left" href="/Blog/2020/10/31/nearly-double-the-ph-bits-with-one-more-clmul/" title="Previous Post: 1.5x the PH bits for one more CLMUL">&laquo; 1.5x the PH bits for one more CLMUL</a>
      
      
      
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>


</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Website copyright &copy; 2021 - <a href="mailto:pvk@pvk.ca">Paul Khuong</a> | <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/TheChymera/Koenigspress">Königspress</a></span>
</p>

</footer>
  

<script id="dsq-count-scr" src="//pvk.disqus.com/count.js" async></script>

  
<script type="text/javascript">
  var disqus_config = function () {
      this.page.url = 'https://www.pvk.ca/Blog/2021/01/08/stuff-your-logs/';
      this.page.identifier = 'https://www.pvk.ca/Blog/2021/01/08/stuff-your-logs/';
      this.page.title = 'Stuff your logs';
  };

  (function() {
      var d = document, s = d.createElement('script');

      s.src = '//pvk.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
  })();
</script>












<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-20468541-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
