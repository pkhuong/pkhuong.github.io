<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Paul Khuong: some Lisp]]></title>
  <link href="https://www.pvk.ca/atom.xml" rel="self"/>
  <link href="https://www.pvk.ca/"/>
  <updated>2022-07-11T22:40:06-04:00</updated>
  <id>https://www.pvk.ca/</id>
  <author>
    <name><![CDATA[Paul Khuong]]></name>
    <email><![CDATA[pvk@pvk.ca]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  
  <entry>
    <title type="html"><![CDATA[Plan B for UUIDs: double AES-128]]></title>
    <link href="https://www.pvk.ca/Blog/2022/07/11/plan-b-for-uuids-double-aes-128/"/>
    <updated>2022-07-11T22:38:02-04:00</updated>
    <id>https://www.pvk.ca/Blog/2022/07/11/plan-b-for-uuids-double-aes-128</id>
    <content type="html"><![CDATA[<p>It looks like internauts are having another go at the <a href="https://brandur.org/nanoglyphs/026-ids">“UUID as primary key”</a> debate,
where the fundamental problem is the tension between nicely structured primary keys that tend to improve spatial locality in the storage engine,
and unique but otherwise opaque identifiers that avoid running into <a href="https://www.hyrumslaw.com/">Hyrum’s law</a> when communicating with external entities and generally prevent <a href="https://en.wikipedia.org/wiki/German_tank_problem">unintentional information leakage</a>.<sup id="fnref:state-of-sin" role="doc-noteref"><a href="#fn:state-of-sin" class="footnote" rel="footnote">1</a></sup></p>

<p>I guess I’m lucky that the systems I’ve worked on mostly fall in two classes:<sup id="fnref:really-performance-sensitive" role="doc-noteref"><a href="#fn:really-performance-sensitive" class="footnote" rel="footnote">2</a></sup></p>

<ol>
  <li>
    <p>those with trivial write load (often trivial load in general), where the performance implications of UUIDs for primary keys are irrelevant.</p>
  </li>
  <li>
    <p>those where performance concerns lead us to heavily partition the data, by tenant if not more finely… making information leaks from sequentially allocation a minor concern.</p>
  </li>
</ol>

<p>Of course, there’s always the possibility that a system in the first class eventually handles a much higher load.
Until roughly 2016, I figured we could always sacrifice some opacity and switch to <a href="https://www.ietf.org/archive/id/draft-peabody-dispatch-new-uuid-format-04.html#section-1-6.1.1">one of the many k-sorted alternatives</a> created by <a href="https://github.com/segmentio/ksuid">web</a>-<a href="https://www.mongodb.com/docs/manual/reference/method/ObjectId/">scale</a> <a href="https://firebase.blog/posts/2015/02/the-2120-ways-to-ensure-unique_68">companies</a>.</p>

<p>By 2016-17, I felt comfortable assuming <a href="https://en.wikipedia.org/wiki/AES_instruction_set#x86_architecture_processors">AES-NI</a> was available on any x86 server,<sup id="fnref:also-arm" role="doc-noteref"><a href="#fn:also-arm" class="footnote" rel="footnote">3</a></sup> and that opens up a different option:
work with structured “leaky” keys internally, and encrypt/decrypt them at the edge (e.g., by printing a <a href="https://www.postgresql.org/docs/current/xtypes.html">user-defined type</a> in the database server).
Assuming we get the cryptography right, such an approach lets us 
have our cake (present structured keys to the database’s storage engine),
and eat it too (present opaque unique identifiers to external parties),
as long as the computation overhead of repeated encryption and decryption at the edge remains reasonable.</p>

<p>I can’t know why this approach has so little mindshare, but I think part of the reason must be that developers tend to have an outdated mental cost model for strong encryption like <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">AES-128</a>.<sup id="fnref:poll" role="doc-noteref"><a href="#fn:poll" class="footnote" rel="footnote">4</a></sup>
This quantitative concern is the easiest to address, so that’s what I’ll do in this post.
That leaves the usual hard design questions around complexity, debuggability, and failure modes… and new ones related to symmetric key management.</p>

<h2 id="a-short-intermission-for-questionswcomments">A short intermission for questions^Wcomments</h2>

<p><a href="https://brandur.org/nanoglyphs/026-ids">Brandur</a> compares sequential keys and UUIDs.
I’m thinking more generally about “structured” keys, which may be sequential in single-node deployments, or include a short sharding prefix in smaller (range-sharded) distributed systems.
Eventually, a short prefix will run out of bits, and fully random UUIDs are definitely more robust for range-sharded systems that might scale out to hundreds of nodes…
especially ones focused more on horizontal scalability than single-node performance.</p>

<p>That being said, design decisions that unlock scalability to hundreds or thousands of nodes have a tendency to also force you to <a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-mcsherry.pdf">distribute work over a dozen machines when a laptop might have sufficed</a>.</p>

<p>Mentioning cryptography makes people ask for a crisp threat model.
There isn’t one here (and the question makes sense outside cryptography and auth!).</p>

<p>Depending on the domain, leaky or <a href="https://cheatsheetseries.owasp.org/cheatsheets/Insecure_Direct_Object_Reference_Prevention_Cheat_Sheet.html">guessable external ids</a> can <a href="https://web.archive.org/web/20220601153759/https://www.wired.com/story/parler-hack-data-public-posts-images-video/#:~:text=Increase%20a%20value%20in%20a%20Parler%20post%20url%20by%20one%2C%20and%20you%27d%20get%20the%20next%20post%20that%20appeared%20on%20the%20site.">enable scraping</a>,
let competitors estimate the creation rate and number of accounts (or, similarly, activity) in your application,
or, more benignly, expose an accidentally powerful API endpoint that will be difficult to replace.</p>

<p>Rather than try to pinpoint the exact level of dedication we’re trying to foil, from curious power user to nation state actor, let’s aim for something that’s hopefully as hard to break as our transport (e.g., HTTPS).
AES should be helpful.</p>

<h2 id="hardware-assisted-aes-not-not-fast">Hardware-assisted AES: not not fast</h2>

<p>Intel shipped their first chip with AES-NI in 2010, and AMD in 2013.
A decade later, it’s anything but exotic, and is available even in low-power <a href="https://en.wikichip.org/wiki/intel/microarchitectures/goldmont">Goldmont Atoms</a>.
For <em>consumer</em> hardware, with a longer tail of old machines than servers, the May 2022 <a href="https://web.archive.org/web/20220619045520/https://store.steampowered.com/hwsurvey/">Steam hardware survey</a> shows 96.28% of the responses came from machines that support AES-NI (under “Other Settings”), an availability rate somewhere between those of AVX (2011) and SSE4.2 (2008).</p>

<p>The core of the AES-NI extension to the x86-64 instruction set is a pair of instructions to perform <a href="https://www.felixcloutier.com/x86/aesenc">one round of AES encryption (<code>AESENC</code>)</a> or <a href="https://www.felixcloutier.com/x86/aesdec">one round of decryption (<code>AESDEC</code>)</a> on a 16-byte block.
<a href="https://uops.info/table.html?search=aesenc&amp;cb_lat=on&amp;cb_tp=on&amp;cb_WSM=on&amp;cb_ICL=on&amp;cb_ADLP=on&amp;cb_ADLE=on&amp;cb_ZENp=on&amp;cb_ZEN3=on&amp;cb_measurements=on&amp;cb_aes=on">Andreas Abel’s uops.info</a> shows that the first implementation, in Westmere, had a 6-cycle latency for each round, and that Intel and AMD have been optimising the instructions to bring their latencies down to 3 (Intel) or 4 (AMD) cycles per round.</p>

<p>That’s pretty good (on the order of a multiplication), but each instruction only handles one round. The schedule for AES-128, the fastest option, consists of 10 rounds:
<a href="https://www.intel.com/content/dam/doc/white-paper/advanced-encryption-standard-new-instructions-set-paper.pdf#page=17">an initial whitening xor, 9 <code>aesenc</code> / <code>aesdec</code> and 1 <code>aesenclast</code> / <code>aesdeclast</code></a>.
Multiply 3 cycles per round by 10 “real” rounds, and we find a latency of 30 cycles (\(+ 1\) for the whitening <code>xor</code>) on recent Intels and \(40 + 1\) cycles on recent AMDs, assuming the key material is already available in registers or L1 cache.</p>

<p>This might be disappointing given that <a href="https://2013.diac.cr.yp.to/slides/gueron.pdf#page=20">AES128-CTR could already achieve more than 1 byte/cycle in 2013</a>.
There’s a gap between throughput and latency because pipelining lets contemporary x86 chips start <em>two</em> rounds per cycle, while prior rounds are still in flight (i.e., 6 concurrent rounds when each has a 3 cycle latency).</p>

<p>Still, 35-50 cycles latency to encrypt or decrypt a single 16-byte block with AES-128 is similar to a <a href="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server)#:~:text=50-70%20cycles%20latency">L3 cache hit</a>… 
really not that bad compared to executing a durable DML statement, or even a single lookup in a <a href="https://memcached.org/">big hash table stored in RAM</a>.</p>

<h2 id="a-trivial-encryption-scheme-for-structured-keys">A trivial encryption scheme for structured keys</h2>

<p>AES works on 16 byte blocks, and 16-byte randomish external ids are generally accepted practice.
The simplest approach to turn structured keys into something that’s provably difficult to distinguish from random bits probably goes as follows:</p>

<ol>
  <li>Fix a global AES-128 key.</li>
  <li>Let primary keys consist of a sequential 64-bit id and a randomly generated 64-bit integer.<sup id="fnref:not-strictly" role="doc-noteref"><a href="#fn:not-strictly" class="footnote" rel="footnote">5</a></sup></li>
  <li>Convert a primary key to an external id by encrypting the primary key’s 128 bits with AES-128, using the global key (each global key defines a unique permutation from 128 bits input to 128 bit output).</li>
  <li>Convert an external id to a potential primary key by decrypting the external id with AES-128, using the same global key.</li>
</ol>

<iframe width="800px" height="500px" src="https://godbolt.org/e?readOnly=true&amp;hideEditorToolbars=true#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAZi%2BkHAGTwGTAA5dwAjTGJvADZSAAdUBUJbBmc3D29fROSbAUDgsJZI6K84y0xrVKECJmICdPdPHwtMKzyGGrqCAtCIqNiLWvrGzJaFYZ6gvuKBsoBKC1RXYmR2DgBSACYvLBpggGpjAHEQuWMhAHk5ACVHbA2NAEFtryDkNywDja9HPBYWEECMQggA6BA/B7PV7vT6Yb6/IEEACe8UwCnBkMeLx2sNcXx%2Bjgm6HCqBcmK8UJxbwYH3x8MJxL2FKpMNpcIRRII%2BEELOxbLpBN%2BxNsfOhuPZ9M5ExBRjF1LxQqJyIUAHoFCrRLRaPKBRzCa4GHhibrnqYWFwtgAOPAHRjIYiojrGADWmGRGwArAAhLhcL0AER%2B3ux5stNoOWAdTtSrvdXt9/s9Qa8IehzxlrmsB3iIJYdWRcY90IA7GmXhoAJyuIExSTGAgHJQAR1c9swwf5VZrgjrDYODAEa07pZT5exmezmFUBCizFoxjw6H5Za71aBVv74WRs4UCa4MUDI5eJbHXaek8b09nxHni/QdtpGEwEEvObzBaLOZd8xX46rb7XnOYj3gcxCYAQx6PJWYbWras4sPEUHppWqoAFQHM4DAAG5RI2RAHLQhAEPQAC0Dh4IYBzbruoIHGhqprmwLDIPEyIQNsMQIfEpDfFsMTxC6vHJAAXpgqBUBA3HzL%2BqZrtxCIBkcALGKoJDGMk4ZSZgiG8fajrxM6boej6GiBrJ/7UrmTDAPmBzHI4jgHIaxAuLQBxcGZKH8MQByvngYn9raPxKf6qYHMFvweWZ4V4NsIZbN6v4odBCkhcpLDGEw6L2hpeBadxem0gZRnxj6cXJhZ56pTp8SKRlWU5eyTATHlBW1UV0aGbGxn7mZlXIZWzGsexnHgQQvGcYVTYBeJknjTJg3jSsDBgRBUGnueb65v8n7GZGlTPq%2BwJZleM7AQuS52kuyUnv%2BlbbR%2BjpfuNg2wRG3HIRWQ06SNHH8dNnGYEuwmzRJ2mIYtckoWlXhKcYKlqcQbXWhDPEHV1pUmd6/UBlV6ZWcQNl2Q5TkuW50Vrj5fmiZgQX1WF3oRZylOxfF8W3dBNWIfVCOZdlChRijVpo7xUYlT1ZXehVeNfdzdXpXzjWC5UBitZpqPTeLMYCEWfXmYN6GYQIuH1AcVCuSwhHEWRFFUTR6J0QxTG/Wx/0xONk0Ax1M1ieDC3499y23mtkHQyeQYExMTA2MgTa0/2wAQUTDDoKgLAQNhqBXWh4SuFQoOBY2edUPQDC8UC5sGMACic6uKHB6tGoKFqtAQEIACaQjGEnwKGGnLC8SXQ/52XvGlzZtcbZHFZPFnV1YCCuHGNr3W68ZmfZ%2Bgdf3avWP68m9X6TrDB66ZR7h9B1P%2BUXzPpYzd9RZ5wYReziWc1W0F75L2OeQcpGv0PorFSAt/jIGFhAY%2Ba9T69XKgbS%2BVZv7rylrjI%2BxUT5n19LjJaEEVrT3PPPB8CBpzoHcPECAaAGATAOMgBAdR6K5kwDQVQvFKHUMIfRdAMcmCFzpo2cu1EyTuWCAAdyIsEHea42GNkNMkYAwQHy0PobnHc6J6pcNqHLKseAqB%2BUYcwg4YAwDpTOP4fwH9KzQR2oISS2wtjbE9HuLYWxeL6LwKoQOVMSA01mvTdKMUmaRScgwF%2BcVEoczXFY2UBBbHOIcRoLYqg7FD1UXuOBA0EHfR0X5UR4jMAWKiUCWJ9jPSOBCc4zxVZG74IJpXfMQQICSJSvXb60EFAJ0bMAVAYdLJcyrF0xs6Ve4pwHu7KBWNeHg3GT/RYBx0ITxriFBiGhKmWO0bo6A3TDHGLhr7OakD0HQKLItFKn8qxMFJPURpg0NibQJl/KIeBl5IJge6a5mToLENUKQxC/0tjTIEAcYyIA%2BIuLXAcCFkKIVA0ORMvZUzYUzN4uhXJ0wlmqhOvkwaXyfnkLsS8oF7oQXJPBVCyFnEXlFkmZJSlxlZkoswGItFcMGKYsqd9R6u1nr7UEvVW5TMuC8UkN8TaCCHonSnOdW8IErr2mfA%2BdKcqsAQEEqszl%2BZuXugxvK9Rh1lVKswNvLRlZrExL%2BYJYlWxPR2OUDcAAkq4OsyS%2BLWucbah1Tq4mlPKS478oIWxtlpJgVxLpQSDiDasnFZC/kGvQJa31MK0BYHQNSg5SbDX0rQqi4I6K2WDVNcUqM8rLWuq2O6x1khnUOJtfaitdivRlOdUW5N/rMCtnbGLPVhqw1DixWKxuBwAkrhnhwRYtBOCel4J4DgWhSCoE4E5BQyxVgMh2DwUgBBNCjsWC6EAkhKygkrEe49J6T36E4JIKdW652cF4AoEAGgN1bsWHAWAMBEAoHTvEOgURyCUDQIhH90RkDAEtIKmgtAbz3ogOEa94QggFk4OugDbBBCXAYLQZE16sD5iMOIGdvB8DgSqLhe9BG/CqEqK4Wc16gRtGvURcIRNHTOCwNe4E/wkOjr4NXBQAA1PAjLLhomneu/gggRBiHYFIGQghFAqHUOR3QgqDBGBAKYYw5hGP3sgIsVA0CyOkUuFsf%2B%2BZ0R3uXWsPQwJMDrB4GOidV7yPzo4KoK0MRSJ1gOMAZAcdLSgi4H5RwvFcCEG8a8QVxtAP0F8hF%2BYvBN0EZkqQXd%2B7D2noy0e89HBL2kBYCAT0j7p2zpc3eh9T6kukFfR%2B5YBB4jUb/RQr9QGQisHWG5jzXmfN%2Ba2AF3ghqwsgjjeB2QknxAyfE/IJQahr3KdICIom8QuMOY4JO0gxXeAucuNR%2BrjYJIHA6554V3WPK9cCxAZw0Woh8S8FweLFWtDJeIUwRelAVu5fy4V9b17SsWHK4lx7O690Hsy5l7LXgnMldvQ97d2WtiQ829DgHsPTbJDsJIIAA%3D%3D"></iframe>

<p><small><a href="/images/2022-07-11-plan-b-for-uuids-double-aes-128/aes128.c">source: aes128.c</a></small></p>

<p>The computational core lies in the <code>encode</code> and <code>decode</code> functions, two identical functions from a performance point of view.
We can estimate how long it takes to encode (or decode) an identifier by executing <code>encode</code> in a tight loop, with a data dependency linking each iteration to the next;
the data dependency is necessary to prevent superscalar chips from overlapping multiple loop iterations.<sup id="fnref:latency-throughput" role="doc-noteref"><a href="#fn:latency-throughput" class="footnote" rel="footnote">6</a></sup></p>

<p><a href="https://bit.ly/3nMxIKV">uiCA predicts 36 cycles per iteration on Ice Lake</a>.
On my unloaded 2 GHz EPYC 7713, I observe 50 cycles/<code>encode</code> (without frequency boost), and 13.5 ns/<code>encode</code> when boosting a single active core.
That’s orders of magnitude less than a syscall, and <a href="https://en.wikichip.org/wiki/amd/cores/milan#:~:text=l3%20cache%20with%2046%20cycles%20average%20latency">in the same range as a slow L3 hit</a>.</p>

<iframe width="800px" height="400px" src="https://godbolt.org/e?readOnly=true&amp;hideEditorToolbars=true#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAZgDspBwAyeAyYAHLuAEaYxCCSAJykAA6oCoS2DM5uHt5%2Byak2AkEh4SxRMfEWmFYFDEIETMQEme6evpXV6XUNBEVhkdGxCQr1jc3ZbcPdvSVlgwCUFqiuxMjsHACkAExewchuWADU616OeCwswQTEwQB0CMfY6xoAgls7DHuuh8eOw%2BhYVDuDyer22u32mCOJz%2BlyBXkeLze4K%2BkJ%2BMNQcIRoPen2%2B0II6FoeAimJBSI%2BEKhvyuwWApMRL1MLC4mwAHHgDoxkMQAJ6JGrGADWmB56wArAAhLhccUAEWOErJL2GxFc1gOiWuLAaPKFIrJPkVDLirkuADZJMYCAclABHVxczAKpUm82W60MASrZ2InzyrxG17Kq5q62YVQEaLMWjGPDoA2Bp6uwSsq0HCI8yMKcVSs1yn2vP0FkGTGzIG0h9XhyPEaOx9Ccj4YTAQFWhjVanV6nkawVzBMuttViNRsT1g7ETAEYsaOJMlnsg6RliJGdBuIAegAVAdnAwAG7Ra1EA5EggEegAWgceEM6czmAUNwOW43LrYLGQiR5EC2ZuXiSkEcmxmokgpAakABemCoFQEAAXM/YBi6AFQrKBzGOcxiqCQxipAu8GYCuQFcry/LpN2OYaHKSGJoi2yakwwDagcADijiOAcrgMMQLi0AcXDUca/DEAcrZ4NBaYcsc6EygGBzSScAnUfJeBbIqmwSv2xpJqhMkYVhTCPlyeF4ARAEkR8ZECsKoqSmpYqyrRSrrnpXjoZhLDGEZSgUkwwymeZRGAY23J8jZ%2BqSoJNEznEH5fj%2Bf6TgQQF/hZNoSTBcHJYhsXJcsDATlOxZFgyzymEw57XBEriRqYUCesERIhIhFaquqmpnF2tkglyzbGLQqCoIkraVtanXary3a9hBmVppOiSYJV2mFnRJoMOkRyGi6SZDuNnZTbZvaxbtY2ciOtZjnGnJxidfoVpVqLuVta1JjcYFoTNO1FshZVJiJYlQZgUmfSpEoKVSRWLZVCoKep6krUmp2PTcmDXfpfVYKNKNgc5f2zvltYPZG72CiV8plaWeDlk1wSQqaggWlaILAFOVrU4KCjGFEwDBBADMEEz1pbmgK50NE3MNNc0SI9txoC14mxpoNQEIGpv1Buu/ksAc%2B4uJV4u/psmxfqa8Zio4DDio407Gy6BwO47TuO1smzEOgBAKMgrv287fsHCAwGbDJTCuxAg0LEHMnxsbEBq5HPuacQYci6gYv0MQkvENLxBzCAICJ2KxARKorupXbmxF8gpfG6Qvv%2B07rsfiQoq10HyDe8beProThVQALQtzGrVI/AcitzFtnGDeTLlUzTDDNfTbrMy8rMEOzyCc8YDgQHrcay2tCtK9aKsHGra5Jtruv6zY9BG27Hte6uFtWxbtvB8aDfO03qD7sBYpbDFEtVQ5cAGSmno5a2r8bY%2B0/l/F2xsWC/3/oAzA6AQEoMlOfSBL9rbv3rvA12ptbp2zgfAgORxwGoDlFHdyydY4R1Slghy6EfZ0LDvHfOhdiBMBrpscuwdK7uz4QIwBxcRFRyEdXMuBCv5NyIi3Mu7dO6bG7kmXuYl%2BbLwIEPRSjhR7j0nqeahGt1ilU1nPBSggDjIBXMYVwFoIBoAYMMa%2B10tz7gwc41xe8GweJ5AfF03jrQD3dC%2BVQn1PGxWCVxbRL4ez6X3KKUxxo8BUDEluCJMl9JbgCTtAmU4CoHDBi6DREBMmQ1yRPY4AAxA4l4uAULkomcxZJtgAjpgcUIAAlbAyhlIaEGUJbEHSQhdIAJIABVsDdIGcMkElwDjaj5oE40ljQlpiqEwRIShzYSh6X0/MKTkwEFTNaHqkogawQgKRcKFFbL9kgSk9cANxKSWtIpdCYMIajyuXBB5sM1KaQRvkuIFyJQsM%2BurOi654rfiNmaW55EBDdnLmaWys1oLXKRRFAJajhIkEBnND5oNAWQwOcoQF8NNLVLluuU67V9pdUOiKXsn0zHgzwEBaSP0YVI1nBs60PNgifTXhvLewqGAIrAjcO0DoPiYHxXEJMH0MZNiwANIaI0wJAVCFMmZSrL4KBYPfV2FDE4ShYGHXGhqBVxIcKKtmZYt473xfS2cWydloJzJCtVDZLzpkwLzK2xyzEU01kmW0CgSAEBuQYL16BMVZTjds3ZE8NwZSxXBT1uyqI0TrmQx2fyU0JrzY5SOtjEj2ItG6lV1xBBwULjcLw6TAHNtbZXdtUCZGFsdhALgNwNAvk5PG3NkoKUHAzYJaijlJ0TOmd0hYsiHb9sHcOnN3rx29P6Rm4Os6M16oXUu3tK6B1Dp3BuvZE6A2NIDROqdgyaEHv1YupVGiSm%2BnDRwBYtBOBil4J4DgWhSCoE4JxaNyxVjAS8DwUgBBNDfoWIKQYNw4hofQxhjD%2BhOCSAAwhkDnBeAKBABoODCGFhwFgEgUWiRxZkAoE4tOtGM4oGACyLgfA6A1mIxACI%2BGIjBB1JwWDos2CCAAPILx5PhrA2ojDiCA7wfAk5rB4EPMRxT/hVCYGQLVNYwHLhVHw0SCIPDeTOCwPhmkLBhPfr4AYYACgABqeBMAAHdxOLUA7B/gggRBiHYFIGQghFAqHUJp3QHGDBGBAKYYw5gTPEcgAsYaNQNOXnE5sep2pHxEaqDpmo9gGBOBcC0PQgQ6YzAGBxvIaQBBjE8DVlIdWGDTH6DEDjlgCudBGE0Ur2ROv5dUwILojQ2ulGqxYXrDW9CTDG5V9rEgFgQZWIFuDk41g8B/X%2BvDmnQMcFUKyM0l4LQHGAB3ASmwbiNIgI4ICuBCCEreBx3cTG6PQa4HMXg8HFOIVIMh%2BIqHMPA7Q9hjguHSCAeA/tojJGyO/e2xwTYu3oeEfh1oP7h5iCpDsJIIAA"></iframe>

<p><small><a href="/images/2022-07-11-plan-b-for-uuids-double-aes-128/aes128-latency.c">source: aes128-latency.c</a></small></p>

<p>This simple solution works if our external interface may expose arbitrary 16-byte ids.
AES-128 defines permutation, so we could also run it in reverse to generate sequence/nonce pairs for preexisting rows that avoid changing their external id too much (e.g., pad integer ids with zero bytes).</p>

<p>However, it’s sometimes important to generate <a href="https://datatracker.ietf.org/doc/html/rfc4122">valid UUIDs</a>,
or to at least save one bit in the encoding as an escape hatch for a versioning scheme.
We can do that, with <a href="https://en.wikipedia.org/wiki/Format-preserving_encryption">format-preserving encryption</a>.</p>

<h2 id="controlling-one-bit-in-the-external-encrypted-id">Controlling one bit in the external encrypted id</h2>

<p>We view our primary keys as pairs of 64-bit integers, where the first integer is a sequentially allocated identifier.
Realistically, the top bit of that sequential id will always be zero (i.e., the first integer’s value will be less than \(2^{63}\)).
Let’s ask the same of our external ids.</p>

<p>The code in this post assumes a little-endian encoding, for simplicity (and because the world runs on little endian), but the same logic works for big endian.</p>

<p><a href="https://www.cs.ucdavis.edu/~rogaway/papers/subset.pdf#page=7">Black and Rogaway’s cycle-walking method</a> can efficiently fix one input/output bit: we just keep encrypting the data until bit 63 is zero.</p>

<p>When decrypting, we know the initial (fully decrypted) value had a zero in bit 63, and we also know that we only re-encrypted when the output did <em>not</em> have a zero in bit 63.
This means we can keep iterating the decryption function (at least once) until we find a value with a zero in bit 63.</p>

<iframe width="800px" height="500px" src="https://godbolt.org/e?readOnly=true&amp;hideEditorToolbars=true#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAZg2kHAGTwGTAA5dwAjTGIQAA4uUgAHVAVCWwZnNw9vXySUmwFA4LCWSOi4i0wrfIYhAiZiAgz3Tx8KqrTa%2BoJC0Iio2PiFOoamrNahrp7i0oGASgtUV2Jkdg4AUgAmLywaYIBqYwBxELljIQB5OQAlR2w1jQBBTa8g5DcsPbWvRyYFJQaAHQIL53R7PV7vTCfb60PAsQgKIEg%2B5PLYQ1wfL6OOHwwTEIJIryg1EvBhvDFQrFBAgEACeCUwiOBRJR4LJkOhjiG6HCqBchOJbPJmO%2B3J2AtZaPZFM53OpErBUuFlNFBHwqAVJPRIq5BHxRk1Qo5WIUtIUAHpTQpRLRaIalcbvq4GHhuYbHqYWFwNjE8HtGMhiPTqsYANaYWlrACsACEuFxowARL4xlGe72%2BvZYQPBtJhiPRuMJqPJrypsGPIbEVzWPYJfEseq0/ORsEAdnLTw0AE5XNSAGySYwEPZKACOrgDmBTrJ7fcEg%2BHewYAhWM/bpc7KKrNZHmFUBCizFoxjw6FZHdnvepMSX4Vph4Uha4/aT66ebc3s4eExsyFHeq7v6B5HmIp7oP6ZIYJgEA7rW9Zwk2LZ1qGswXluPZwXuIHEMe4F7MQmAEO%2B9zdumPp%2BoeLAJCRFbduaABUezOAwABuUQjkQeywjS9AALQOHghh7Pej4AnsDHmlebAsMgCS0hAmz9lRCSkJ8Gz9gkoZqSkABemCoFQEAqbMaFlleKnQomBwsCwxiqCQxgpBmxmYNRakBkGCQhuGkaxhoSZmRhJL1kwwCNnshyOI4ezOsQLi0HsXABXR/DEHssF4PpS5%2Bl81kJmWey5d8SUBYVeCbKmGwxmhdGkZZeU2XZTCMgGTl4C5KkeWSXk%2BQWsYVSWQXfvVbkJFZTXGC1Sjsr8BDtZ1Y3dTm3l5r5z4BUNtHdjJckKUphEEGpSldaOWUGUZh2mdth1LAwBFESRn7fr%2BeD/lhdYNkhvlZpgaBYLBgG1vuh64WBZ7%2BmetUfhh3YfQhjZBshh3beRmYqbRXY7W5e2KRpp1KZgZ46edhmudR13mXRDVeNZxi2fZjnOT65Oqb9K19X5MabYmw0ViFxBhRFUUxXFCWlVeaUZXpmA5RNBUxkVnIS%2BVlWVdDpGjdRE3081jLZgtLOndmvVrf1MaDbzmNa%2BNjW61N%2Bt/QYQyGzErNqSbuYCC2G2BdtjHMQI7ENHsVDxSw3GEAQ/GCcJomMuJknSTj8l4/2h3HfjS1nfpZNXXzWO3bhD3EVTH7Jvzr3/ryLhFQoxisWIZ7GFpEaA9W8FfUjP1aRrl50UX91aQC46TmSKqOI1ACSIQACqLgAsg8AAaT0V12P5A9hoN4RDAbQcYADuhAIMY%2BnxcY4SEO3QEI99EYoX3sPOmknz91jpHw13zY96G22f1vYCO9wYQSJueMumtPx7BptZNY79NY9gBFpCaWlSBXjgV%2BfmpFfj/AIBAV0Dcm7oBbr5CAvcC5XnQKgN%2BsMbYAjARNfeAMVJINQv/KBh8EB0ChBAMAYACGN1hMQ1uCkWHkOtj2Qe0Cxr0LPGvF6gC77dwftmA%2Bx8CCn3PqgS%2B18Pog1AiePeUN0JXhfgIGh6DMKKO/shLS/8rEd23gY/CYD2HWRgRYuqiCGGNTAWguqz0IF0RwRxfB9dBHNxEazVhlNgqkSoZ4j%2BPYxGhgmqo5hMiwEUJ7BgvYnDuEZT4QIohJC24pNiSNSRRE7rSOoqw%2BRlc6h/lHDLJcwAiKCwYFQlgEBWKoAhgxcIrgqAk2yiOIZVB6AMDUtSUOBhgAKCfleKRVobS0AgEIAAmkIYw7S9SGG6WpCZRzhlTLUpMsKiyGkbz6RDLA%2BJ2LGE9qtb2pDbnoCWXRZ5nNfYlkYT1L2DAfb%2BTfEE7sUtMpjKVo1BW0KSrJRTEVNW1UNY5J7N8s2XNkp7D4kiv5dsGbTThMgV2EBPKAuBRbP2YKMWvPNjzf5HNMW/MwYXapuFrnbiaW9PY7yUQIH3OgdwCQIBoAYEMPYyAED1AkvWTANBVBqTFRK95El0BMDqKM2WI5pkiT5IlYIh9YTBE%2BVjZVI5nQpGAMECCUqZWDIfIyNJGqmASO7HgKgGU5UKr2HwxqJx/D%2BFRd2UiCFBBGU2BsTYUYnwbA2Gpb1eBVDZKxhC1pI5irWTKorYqMUGCIoqtVdWljuxhoIBGuN0aNAbFUJGo5jqnwDWpXEnsHqMqGuNZgYNob9TlrxlGqMjh81xuyd2QenLHjUhRI2IIEBTWQNoZhdNexgCoFLi2kNPZV0jkanszp3S07kpeUC3yWqyZHs5vMPYjELkLLypJDQo7SJtugGu31YBGoy3PQC49LZrpeM3d2JgvIGhzrcZU7s9y8CPNpSetuKbSICtUEK6i/aL2v18iAdS8arx7Dw/hvDhMf2czPUZdDdLaRXsYh2oI05aaSUAl27aSGUMisjbBvYmHsP%2BKxgRgjSlYMtlIxAQTvkqMMRo8Ee95pGMIYcbfGxv8JpwMVvEPYkg36soAY4oBzi95QSwBBXxBmYK922nonCu9QEmeIeozRURqHGf%2BrLOzZ8HM6LwWZsFX9ELKNpOzaCRnaYBYBkwzAHzzPWN8z/FRf1AtHxPm5%2BKaS4tYASxopL2ir54LC7ZxLWi5Olt7RWjYWksPRsjcoK4U9XCDjrepKMlXqu1ckJG6MQ76vD1HlOBNoYAQrnHoVljwq0M2fK3GzO/ZcvCdy%2BJyTdHEwMerExsFw3UORty3kk%2BewtHjZw3RPj%2BGiPObyxlrRM2bPpfs/FObmAjW0ek7Jt1ZaSvpPC%2BNxrcaqs1bqxNhrTWfutcrYO4d8aQvhZHpgCcPXwfoH66uFbsMXv9rexBOzO2HMfYBy1%2BrFWvvNd%2BwOjrE3cOHdR1dzLkPofjw9ql8LFOtHw8Gzddl91s0XnXhweYtBOBRl4J4DgWhSCoE4DFBQixliUi2DwUgBBNBc/mKGEAUZfA844JIfn8vhecF4AoEAvg5eC656QOAsAYCIBQKgai3CyAUFFVbhINuUDAG9PEGgtBQZ64gOELXV9mBBk4DLtAtlGAEHOAwWgtItdYEbEYcQRvSD4EItYaDjItf7j%2Bq4Q8WvqSVC17CcIgsgzOCwFrvUcJA/G9vQoAAangO75wGQC5l/wQQIgxDsCkDIQQigVDqAT7oeIBgjAgFMMYcwBe9eQHmKgY9euOB8XOBsHFjZGS68qH9ao9gumjE8PEAItHpj9HiLkVIAhd96FP9UKYfRoiDA3yngQnQRguGaHoSwm%2BOjDG6If2/7/v8X6DDf434lDH7zDi5LArB6B6iYCrA8Dc686a4J4i4cCqAxD9h8SDgrrID/jegAhcAZSOBqS4CEAkDqReBqbODW70DpTPBcCzC8CG5aCmSkBK4q76CcAa6kAsDK6%2BAC5C4oG6766y7y7zCm4W4Z7IBZ4kDkCUD1ALLKCGCVBCAICoCHzN68DB6O4GDVCKHBC0AqFqH8GaEO5O7IAu4bDxBaE27nBZ6GHqHp6qB/QPDEALI65%2BBOHIC1D4AC68Ct7CA2id7SD%2BG95qBa6D76CGAmBmD6B4DhBT5zrC5z6cCL7L58Sr4KC64S5QFAE%2BF6HKGqEOHcCMGERwG8CHyCwJCV4IEcB86kDGHa4cDYCeHSHpRoEYFYHAA4FJQbD4GEHEH4BEC0HS4MEiFG4sECpMD3KUA1FcE8HsENGCEWDCFMEK6sG8EcEcBeBIECHuGrEsFq4bA7G8BLH7HzDBwpB2CSBAA"></iframe>

<p><small><a href="/images/2022-07-11-plan-b-for-uuids-double-aes-128/aes128-cycle-walk.c">source: aes128-cycle-walk.c</a></small></p>

<p>This approach terminates after two rounds of encryption (<code>encode</code>) or decryption (<code>decode</code>), in expectation.</p>

<p>That’s not bad, but some might prefer a deterministic algorithm.
More importantly, the expected runtime scales exponentially with the number of bits we want to control, and no one wants to turn their database server into a glorified shitcoin miner.
This exponential scaling is far from ideal for <a href="https://datatracker.ietf.org/doc/html/rfc4122#section-4.4">UUIDv4</a>, where only 122 of the 128 bits act as payload:
we can expect to loop 64 times in order to fix the remaining 6 bits.</p>

<h2 id="controlling-more-bits-with-a-feistel-network">Controlling more bits with a Feistel network</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Feistel network</a> derives a permutation over tuples of values from <a href="https://en.wikipedia.org/wiki/Pseudorandom_function_family">hash functions</a> over the individual values.
There are <a href="https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-38g.pdf">NIST recommendations for general format-preserving encryption (FFX)</a> with Feistel networks, but they call for 8+ AES invocations to encrypt one value.</p>

<p>FFX solves a much harder problem than ours: 
we only have 64 bits (not even) of actual information, the rest is just random bits.
Full format-preserving encryption must assume everything in the input is meaningful information that must not be leaked, and supports arbitrary domains (e.g., decimal credit card numbers).</p>

<p>Our situation is closer to a 64-bit payload (the sequential id) and a 64-bit random <a href="https://en.wikipedia.org/wiki/Cryptographic_nonce">nonce</a>.
It’s tempting to simply <code>xor</code> the payload with the low bits of (truncated) AES-128, or any <a href="https://en.wikipedia.org/wiki/Pseudorandom_function_family">PRF</a> like <a href="https://en.wikipedia.org/wiki/SipHash">SipHash</a><sup id="fnref:compliance" role="doc-noteref"><a href="#fn:compliance" class="footnote" rel="footnote">7</a></sup> or <a href="https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE3">BLAKE3</a> applied to the nonce:</p>

<pre><code>BrokenPermutation(id, nonce):
    id ^= PRF_k(nonce)[0:len(id)]  # e.g., truncated AES_k
    return (id, nonce)
</code></pre>

<p>The <code>nonce</code> is still available, so we can apply the same <code>PRF_k</code> to the <code>nonce</code>, and undo the <code>xor</code> (<code>xor</code> is a self-inverse) to recover the original <code>id</code>.
Unfortunately, random 64-bit values <em>could</em> repeat on realistic database sizes (a couple billion rows).
When an attacker observes two external ids with the same nonce, they can <code>xor</code> the encrypted payloads and find the <code>xor</code> of the two plaintext sequential ids.
This might seem like a minor information leak, but clever people have been known to amplify similar leaks and fully break encryption systems.</p>

<p>Intuitively, we’d want to also mix the 64 random bits with before returning an external id.
That sounds a lot like a Feistel network, for which <a href="https://inst.eecs.berkeley.edu/~cs276/fa20/notes/Luby_Rackoff_paper.pdf">Luby and Rackoff</a> have shown that <a href="https://en.wikipedia.org/wiki/Feistel_cipher#Theoretical_work">3 rounds are pretty good</a>:</p>

<pre><code>PseudoRandomPermutation(A, B):
    B ^= PRF_k1(A)[0:len(b)]  # e.g., truncated AES_k1
    A ^= PRF_k2(B)[0:len(a)]
    B ^= PRF_k3(A)[0:len(b)]
    
    return (A, B)
</code></pre>

<p>This function is reversible (a constructive proof that it’s a permutation):
apply the <code>^= PRF_k</code> steps in reverse order (at each step, the value fed to the PRF passes unscathed), like peeling an onion.</p>

<p>If we let A be the sequentially allocated id, and B the 64 random bits, we can observe that <code>xor</code>ing the uniformly generated B with a pseudorandom function’s output is the same as generating bits uniformly.
In our case, we can skip the first round of the Feistel network;
we <em>deterministically</em> need exactly two <a href="https://en.wikipedia.org/wiki/Pseudorandom_function_family">PRF</a> evaluations, instead of the two <em>expected</em> AES (<a href="https://en.wikipedia.org/wiki/Pseudorandom_permutation">PRP</a>) evaluations for the previous cycle-walking algorithm.</p>

<pre><code>ReducedPseudoRandomPermutation(id, nonce):
    id ^= AES_k1(nonce)[0:len(id)]
    nonce ^= AES_k2(id)[0:len(nonce)]
    return (id, nonce)
</code></pre>

<p>This is a minimal tweak to fix <code>BrokenPermutation</code>: we hide the value of <code>nonce</code> before returning it, in order to make it harder to use collisions.
That Feistel network construction works for arbitrary splits between <code>id</code> and <code>nonce</code>, but closer (balanced) bitwidths are safer.
For example, we can work within the <a href="https://www.ietf.org/archive/id/draft-peabody-dispatch-new-uuid-format-04.html#v8">layout proposed for UUIDv8</a> and assign \(48 + 12 = 60\) bits for the sequential id (row id or timestamp), and 62 bits for the uniformly generated value.<sup id="fnref:uuidv7" role="doc-noteref"><a href="#fn:uuidv7" class="footnote" rel="footnote">8</a></sup></p>

<iframe width="800px" height="500px" src="https://godbolt.org/e?readOnly=true&amp;hideEditorToolbars=true#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAJi%2BkHAGTwGTAA5dwAjTGIQADYAVlIAB1QFQlsGZzcPb19k1JsBQOCwlkjo%2BItMKwKGIQImYgJM908fSur0uoaCItCIqNiEhXrG5uy24e7ekrLBgEoLVFdiZHYOAFIvAGYsGmCAamMAcRC5YyEAeTkAJUdsdY0AQU2toOQ3LH31rccmBSVGgA6BDfe5PF5vD6YL4/Wh4FiEBTA0EPZ7bSGuT7fRzwhGCYhBZFbMFo14Md6Y6HYoIEAgAT0SmCRIOJqIh5KhMMcw3Q4VQLiJJPZFKxPx5u0FbPRHMpXJ5NMl4OlIqpYoI%2BFQitJGNF3IIBKMWuFnOxCjpCgA9GaFKJaLQjcqTT9XAw8DyjU9TCwuF4ABx4faJYhUYwAa0wdI06ziACEuFxowARb4x1Fen3%2BwPBsMRhOx%2BNJlNspU7TB7aFCbAARTk2BCd2MAFlHgANfZQLhyfz%2BLnY/YxDRzfYAWn2XDmUtL5f2IQu9ewTdb7Ygne7vZ%2B/a8Q9H4%2BLj2GxFc1iz8IadJzdLZAHZU%2BCNABOVw0mKSYwEfZKACOrkYqyLd8fZ9X3fBgBD/LZb2eK9kwgvcDyPd9MFUAgomYWhjDwdBr0gh5AMEX0332cI6RQhRozjGJC1g8FoP/Z4ngtAAqfYABVD3JJgUPQfZHmwIRhwzfY/iE/ZlGuAAxfYqGIVAWH2VwX32Ih5JfQF9kYi1UUmGxkHkoDCKDKgICfQQX0I8JEVIfY0AYYZDmMb0/QDcNL1jbS8F0gs4kTCcaJwh90ycpTMBYRIYUTQ4WBYYwlAIYxMESPAX1UCANCsiyCAUCdqPo%2Bj7xQ0Lwsi6LVBIGK8AzCACsSKyXPIqNvOy/zSSDJhgBYJh9iORxHHkhgZLtMcowA/hiHbVIAC9MEIgNvgihMIP2WaNy4KNFrwTZUy8GNfLy3DqqKhzoqYJlf3KyrqtqiNyI2xq6OLfKQrCubiuME6lA5P44tSC6nqu1y4wanz7ofYhMAIZYGGC0L6qonDaLvR4mNRdT9mwcliAZTKvgHRJQ3WAd9gAdwQOhoUYBRliCYAlIQaEaGIOyIDhWl6GHBw8EMIcFMkKS8CqbighR5iCDp/Zf0xxIuP2MHKdod83SE98WBSd9KxrOsG2bFsrMMbjRehJQbPQZjOOF/YVbs2d50XFtAWF82WLFoJElcd8CY0PGPYt1w7IUTi3SoOlacN1hoWIQxgGhGyDyYGkkWFzSnng48kJQ4g0Iw7jRo6uLfwwTAIBT98g1PTGL0DUNdvWG8HpddIvlrgDcOL8XkNQsQs/FzCQfvFv9QQk8OvLlzK97muIoOl6a/8vuH0BPGirx0gHonkG8r%2BAECCqp6F9DQEvx/clVUcF71drG3taah7N6ibfqr3wFQOP9cIutrXW2vxH9t3vGD8wb8v4qRxHuFsCKhkd6hUfs/VYVlDIXkHF8LwMR9jn01gubWvcH5/xgcA0B4DgyQMSI/Q%2BQC4HZhcuOJBKD34YNbOvXCt9GhEJIQAo%2BqxX6oOrBfD%2BLYv4b3%2BHfFhOCwInxerQ22/CHpgwhhnaGxDMA9xymvRGyMniowAJIMAAG53wUDjDQOdOLxXJAXD29t1EaS0gPY8pdh7nlHkYuKWA0BYCLjYxC7cM6d0wt3dA1cm55XrgIRus9%2B6HlTl4zOvjFFYWUQ%2BcJg87Fngrl7eJfdoLyKKjPVe89YlFViSvZutF4l5WwfvXBXwQEvQgeU/%2BgDj7kJDJQocmwaFzl4Vg3%2B%2B9SEv2jPgrMRk6m4KaQg1pyCuEa0vvQ0puEZGQ3kXve6CNcruV0v8PAU1CJR31HrWSEBtGoF8YxcIrgqBWUmtNd8pyqD0AYFZGkUkDDACythaR4MFnWltLQCAQgACaQhjA7IjgwdAsl0pnIhbcxgVlbltVecolZ1iA66UOT3J4dNVDoHcIkCAMd3zIAQA0dSQYyx4FUFZfF%2Bw0XcUYugTiTALmbKufse5RF%2BS0FZZgImcJggBNnlSl0qRgDBG4oS4lJySJMiKvS%2BoDCHx4CoO2UlNBVD7DAGAcRXZ/DVwSQ%2BUuggjKbC8JsOIZE2hZjJaoKRI0SDjWZTNIqa0YxLS5KylMS1NqbV1XPe8BqCBGp8KajQXhVDGvSlKsisZbrA1mQqpVEBgg8qCJgH1uF/WBpNXERwDBjU2vvPMjOyzkyIxpKiDqQQID8tXoE3CLdLnbNQAQeVvrgBNqKsCvZLAIBtPgS5NKH5mWoCMn2iMg4rJMThS8uaGlBzj3je2Nt74NUvUucOiAo7IxzDTXq%2B8TA%2BTMJtXtB8S6O3gxBWC7tvaKG5iZVNddm7xwTsYlOsiYDZ35twoqxd7aV1gMHfekdN66Tjh3b6/dJBt6fqRQBTF2LQo9p8JujQIAkG%2BGochu9mAH3AfHfsJiSbeWYBnRaAeqbe5wZxYhrwj7UPhow8BrgWGcPNNzAsfDjFCMppI2R/NrdkkjwjJXbJN4xxWV5mvdJrc04d3QjE0xWBuIvScSY1xhc8Z8Y8UPFJo8XEFyU/%2BlTem3H50U0e9NBoA3UbxnRrwcRjViQ0TzejpqHPXCcy%2BY10Yc30b/r02Blcn6iPMw%2BSjCHjWmcwOgWz6G2mRfQMxoy8X2MEe5URnjh5yNxr9ZZzNxmou2fsz4RzzmLWueK%2B50rWafMWvy%2Bgep7DMBWTq0F4%2B%2BbC1Q2ddeEtjwOALFoJwOIvBPAcC0KQVAnBeoKCWCsKk2weCkAIJoPrCxQwgDiGlAbHBJDDeW%2BNzgvAFAgDSkt0bfXSBwFgDARAKBZKJXoGQCgeK7tk2iMgYAPomM0HllEI7EBwh7YsswTGnAFtoCiowAgFwGC0DpHtrAHUjDiDO6QfAYNrB4F0UdlHSFMDIDdmsMbNIqh7bhOECOmNnBYD2/qeEoPzuvoAGr8yJhcRkI2Fv8EECIMQ7ApAyEEIoFQ6gUe6CYwYIwIBTDGHMGTo7kAFioClukbHw4LheBHB1Jkh2qh45qPYUFYxPBMYCCmmYAwmN5DSAII3egrc1GmP0aITHLB686CMJoLgWh6FdxjgQXRGiO9KBbiwHvbcu490H2YT7pvLFWHofUmA1g8H64N3bKOJscFUL6GIw5FLAGQJ5LwgIuDtkcFZXAhA7UvCY/sZwoVXtIK2OOXgp2tDbtIGtjb%2BhOA7dICwdbaURtjcz4d47i3lsLEuzd3H%2BOiCPcoA0F5yhDBVCEAgVARMOe8HB/dgOAgV/BFoOvzfw%2Bd8vYeygD7XgmO79excN2J%2Bt97dn48YgLyDt%2BFUHjuo%2BARu8C52EFtD52kEAKFzUD2zF30EjilzMH0DwHCHlyrXG2VwEFV3V013ekOxm3jwjz/0PzXw32f24FbzBmT14CJgjkSHp1Tw4CG1IDP32w4GwG/znztWz1z3z0LzHGL1LwgHLzRnwHnybxbwnzOw7zpiYCwGiGQK2z7wH270YNHwsHHzbxW070Hx7w4C2HTxH0/zUI7y2y8F0N4GUIMIWF0UZnSBAEkCAA"></iframe>

<p><small><a href="/images/2022-07-11-plan-b-for-uuids-double-aes-128/aes128-feistel.c">source: aes128-feistel.c</a></small></p>

<p>Again, we can evaluate the time it takes to encode (or symmetrically, decode) an internal identifier into an opaque UUID by <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AB9U8lJL6yAngGVG6AMKpaAVxYMQAZgCcpBwAyeAyYAHLuAEaYxCAAbNIADqgKhLYMzm4e3n5JKTYCQSHhLFEx8RaYVvkMQgRMxAQZ7p6%2BFVVptfUEhWGR0XHSCnUNTVmtQ109xaUDAJQWqK7EyOwcAKQATF7ByG5YANRrXo54LCzBBMTBAHQIR9hrGgCCm9sMu64HR45D6FhUt3ujxeWx2e0wh2OvwugK8D2erzBnwh32hqFh8JBbw%2BXyhBHQtDwEQxwMR73BkJ%2Bl2CwBJCOephYXA2AA48PsEsQqMYANaYACeGjWAFYAEJcLgigAiR1FwMZzLZHK5vIFkrFEulstJCK2/2CEKE2AAinJsKFHNhjABZJ4ADX2UC4cgCAUp332sQ0s32AFp9lxZqS9ZgaCF9qEAPIWq22h1Ol1u74e2IbH3%2BwM6p5DYiuazK071fmq/mkgDscvpPlcF3ixgI%2ByUAEdXIwVtqqzXBHWGwwBO2vJWXmWZYOszm8w3MKoCNFmLRjHh0OWh49qxcWfX9hF%2BbOFCLxbEtWOESOOy9nhMbMh9l2CD3lVQIHeHxFCApSPs0AwhvsFaz2T5UsxSvPAb01YUpSDU9Vw0Hx/yVWcWASSEpT/M5jCUAhjEwBI8HiVQIA0T83wIBQgxPC8Lx8JCUKONDjAw1QSEwvBFQgWjPyAg8hUgijYKxTkmGAFgmH2ABxRxHFvBhiBcWgAyFKt%2BGIR0UgAL0wLd2XogNZX2HTjkU/S8E2OUNlFaDqLXWjUPQlhjCYTAlHeVj2M4/ZuLFUy%2BPPHUaMwZC7MYhynJc3YmCGNzWQ4wKEi4gUDy4XioL8uDiEwAglgYfZaJ449VzPelszqa9G0uSd9mnWdiHnRd0H2FTROwtsMEwCAJ3zTlC2IYsgI5HkrLWCt/NcBg0kOEaqzXTqpxnOcxHqqqlzSnwZoqrqrlE3qSwG1bhrQ2zdOG2C1rg64Eh5OzLtIfyDrS6jaIunlrmbVt3hRYUHi8NDOSfJ7LuuPsPs/P6S29Q4NlifYjVNc1LRte1VoBl7gZWQ4vt0v7YuQ57XswFs20wUGVSAwNIehqMY0Ru1VoyrLatyuLrkwFbKPu4rTCYAhqQiVxZ1MKA%2B2CQkQlmH1ZoLba%2BsS55WqwYxaFQVAEg6jaG266Xdpuxs8E0rcMoSTBuaGqbqLGiaTruuDJc1ottZ5fabfVqr5tqxal2W5d2bgg7yu5lEfsm0612e66eVu6azx96iVLUvWtIbQy0KFQcDMpfZDeNggTLMsyht952A5Zz3dKa7mcPeNq1eLy7%2BKzNd6ey/3Z2evyiqo0CbxFg1b1rSR62BYBMvrMCeQUYwomAYJn37rcACo0GQuhokn%2BormiU3TrvLwNi3JXPwQUyY7XSKWH2AA3FxuZXiBNg2ZAEhrZdhUcBgRUcHONg2fz9j//%2BAH/3vsQdAZFkD31/oAqB%2BwQCQx/j9Jg98IBK3mHA%2Biy5v4QCPqgiBFliBIMXqgZe9BiBr2IBvYgswQAgFwcKYgERVD30/LQ5YjDv6R2otAqB982AsBIKWdhcDkDgO/vXKsTdGZQBfAPAgswj7uiMrvH0w1pJK3bjKYqXcDIMFFhCaRg9njD2wteceld0AQCvkuLeo0Li733qgQ%2Bx8BKnwUOfK%2BBgbD0Dvt/EBYCEgf3fq/L%2BP8qxcMATw1AF9IbCk2MKY2qhmEbBiWKNRkEAkf2CZAsJf8IlRNibEzA6AEnRIPEfLUr9AmfwgaE7JOTv6P2ftUzhtSYEY1FKktCECfr4MwSg5hYoymQTQd0pB2DqEsKYGwjYiSf5JJAVMmZsT6ELLQXM5ACysnZJ4YFfhTChEiLTHTTKzcpFz1kfI5Mii0yTVUagdR45SpgW0Q2ZAyFjCuHiBAb8v5LENXnhfYp3yGy/P2P8/k1iqxAr7t2GRoLVB2QBatKF%2BiGzz35Ai0sJ84J4CoI6ee8L6K6TRQXM6PgJE5VTqdcleKCVGWJZCAAYn6LgrTJTsw7sGLw%2BpwyhAAErYGUIpDQwqlJYm5RCUIABJAAKtgXlQrRXAguPsUSM8IXUS0SiqqBgEhKBfqKPlAqCo2MEJuBsXlRQaUwKgJ8YMgIQzMrrTSNqIB2rVEGNJWKfBxw6gnbSdlKXpw9Fal1QF%2BIGTzhZEla4LU%2BU6UHJxDc4K8MfvybxsQ3WCkSbEICn4Q22tJgKb0YjqIpoSGmzYGbC38i4Nmi1%2BbXXVu9NKPNCcXWZuLQ9NcPqrX%2Bt0oGwy0lDXKFzhZfOwdrY%2BFtlte2/VLp2ROgZT8Olo7OMLuuGFW4p7BDskY0eyBTHboYOmwGb0iYlrXGuedul5ZaSVirV1EcIwyrlRe9dZ9vGzI2K03BooWBILrm%2B0lWqHC7pHiYieDgIBvusnBSoTBdWFIPHGuyoH/RHv2hyqsTYFAkAIBAeDiH0CtudU%2BQjeqfQAHonXWrIzqvV%2BU%2BIcLXFAht5GkNihSqg15CR3nxBg1eq4ggny0OuF4XFsSxMSaSVJ9J7DNn/wgFwa4GhQXaoQwxsUw79jUeSilHTz7ZW8vmApv%2BSmVNqfY/q7T1Gf5DOo1KozJmamAPM6p%2Be6miMHm0xmP0EZ%2BWCt08K6UBnHOvqA9Syl5YNFPA4PMWgnBhS8E8BwLQpBUCcGkrhpY6NXg8FIAQTQcX5g8hAMKYiCWOCSGS0V9LnBeAKBAMRQrqW4ukDgLAJAS88IkPIJQbrK8YjIGAMyWtNBaA1UaxACItW3zMF6pwfLS82CCEjDo/ktWsCiSMOIVrpB8AZWsHgC%2BzlavTkwMgfmqw0sXEqLVwkERiBFmcFgWr1IWCLba1QAwwAFAADU8CYAAO6RiNil/L/BBAiDEOwKQMhBCKBUOoPbuha0GCMCAUwxhzAPca5AeYKtqiNY4L6SM37fSiWcg1yoF3qj2AYE4FwzQ9CBANNMfotbcipAEKMTwnPkjc4YFMPoMRa2WFpx0YYjQmdZDFzTo7AhOgNGFyUDnFgpe870BMZXbORcSHmNl5YsOCsZVWDweLiWat7YyxwVQLJYi%2BniPsYAwiAwbGuCyiAjhPy4EICQSGXha37GcMQ6IAfAy8Ba1ocWpBSvlf0JwarpAPt6dICltLNuGtNYK0VmPlWNhW4z/VnPrWY8neICkOwkggA%3D%3D%3D">encoding in a loop, with a data dependency between each iteration and the next</a>
<small>(<a href="/images/2022-07-11-plan-b-for-uuids-double-aes-128/aes128-feistel-latency.c">source: aes128-feistel-latency.c</a>)</small>.</p>

<p>The format-preserving Feistel network essentially does double the work of a plain AES-128 encryption, with a serial dependency between the two AES-128 evaluations.
We expect roughly twice the latency, and <a href="https://uica.uops.info/?code=.L8%3A%0D%0A%20%20%20%20%20%20%20%20movq%20%20%20%20xmm0%2C%20rdx%0D%0A%20%20%20%20%20%20%20%20add%20%20%20%20%20rcx%2C%201%0D%0A%20%20%20%20%20%20%20%20pxor%20%20%20%20xmm0%2C%20xmm15%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm14%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm13%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm12%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm11%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm10%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-72%5D%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-56%5D%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-40%5D%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-24%5D%0D%0A%20%20%20%20%20%20%20%20aesenclast%20%20%20%20%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-120%5D%0D%0A%20%20%20%20%20%20%20%20movq%20%20%20%20rax%2C%20xmm0%0D%0A%20%20%20%20%20%20%20%20and%20%20%20%20%20rax%2C%20r9%0D%0A%20%20%20%20%20%20%20%20xor%20%20%20%20%20rdi%2C%20rax%0D%0A%20%20%20%20%20%20%20%20movq%20%20%20%20xmm0%2C%20rdi%0D%0A%20%20%20%20%20%20%20%20pxor%20%20%20%20xmm0%2C%20xmm9%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-104%5D%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20XMMWORD%20PTR%20%5Brsp-88%5D%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm8%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm7%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm6%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm5%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm4%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm3%0D%0A%20%20%20%20%20%20%20%20aesenc%20%20xmm0%2C%20xmm2%0D%0A%20%20%20%20%20%20%20%20aesenclast%20%20%20%20%20%20xmm0%2C%20xmm1%0D%0A%20%20%20%20%20%20%20%20movq%20%20%20%20rax%2C%20xmm0%0D%0A%20%20%20%20%20%20%20%20and%20%20%20%20%20rax%2C%20rsi%0D%0A%20%20%20%20%20%20%20%20xor%20%20%20%20%20rdx%2C%20rax%0D%0A%20%20%20%20%20%20%20%20cmp%20%20%20%20%20r8%2C%20rcx%0D%0A%20%20%20%20%20%20%20%20jne%20%20%20%20%20.L8&amp;syntax=asIntel&amp;uArchs=ICL&amp;tools=uiCA&amp;alignment=0&amp;uiCAHtmlOptions=traceTable&amp;uiCAHtmlOptions=graph">uiCA agrees</a>:
78 cycles/format-preserving encoding on Ice Lake (compared to 36 cycles for AES-128 of 16 bytes).</p>

<p>On my unloaded 2 GHz EPYC 7713, I observe 98 cycles/format-preserving encoding (compared to 50 cycles for AES-128 of 16 bytes), and 26.5 ns/format-presering encoding when boosting a single active core (13.5 ns for AES-128).</p>

<p>Still much faster than a syscall, and, although twice as slow as AES-128 of one 16 byte block, not that slow: somewhere between a L3 hit and a load from RAM.</p>

<h2 id="sortable-internal-ids-pseudo-random-external-ids-not-not-fast">Sortable internal ids, pseudo-random external ids: not not fast</h2>

<p>With hardware-accelerated <a href="(https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)">AES-128</a> (<a href="https://en.wikipedia.org/wiki/SipHash">SipHash</a> or <a href="https://en.wikipedia.org/wiki/BLAKE_(hash_function)#BLAKE3">BLAKE3</a> specialised for 8-byte inputs would probably be slower, but not unreasonably so), converting between structured 128-bit ids and opaque UUIDs takes less than 100 cycles on contemporary x86-64 servers… faster than a load from main memory!</p>

<p>This post only addressed the question of runtime performance.
I think the real challenges with encrypting external ids aren’t strictly technical in nature, and have more to do with making it hard for programmers to accidentally leak internal ids.
I don’t know how that would go because I’ve never had to use this trick in a production system, but it seems like it can’t be harder than doing the same in a schemas that have explicit internal primary keys and external ids on each table.
I’m also hopeful that one could do something smart with views and user-defined types.</p>

<p>Either way, I believe the runtime overhead of encrypting and decrypting 128-bit identifiers is a non-issue for the vast majority of database workloads.
Arguments against encrypting structured identifiers should probably focus on system complexity, key management<sup id="fnref:rotation" role="doc-noteref"><a href="#fn:rotation" class="footnote" rel="footnote">9</a></sup> (e.g., between production and testing environments), and graceful failure in the face of <a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s01-hochschild.pdf#page=3">faulty hardware</a> or code accidentally leaking internal identifiers.</p>

<p><small>Thank you Andrew, Barkley, Chris, Jacob, Justin, Marius, and Ruchir, for helping me clarify this post, and for reminding me about things like range-sharded distributed databases.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:state-of-sin" role="doc-endnote">
      <p>I’m told I must remind everyone that sharing internal identifiers with external systems is a classic design trap, because one day you’ll want to decouple your internal representation from the public interface, and that’s really hard to do when there’s no explicit translation step anywhere. <a href="#fnref:state-of-sin" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:really-performance-sensitive" role="doc-endnote">
      <p>There’s also a third class of <em>really</em> performance-sensitive systems, where the high-performance data plane benefited from managing a transient (reallocatable) id space separately from the control plane’s domain-driven keys… much like one would use mapping tables to decouple internal and external keys. <a href="#fnref:really-performance-sensitive" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:also-arm" role="doc-endnote">
      <p>ARMv8’s cryptographic extension offers <a href="https://developer.arm.com/documentation/ddi0596/2021-12/SIMD-FP-Instructions/AESE--AES-single-round-encryption-">similar AESD/AESE instructions</a>. <a href="#fnref:also-arm" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:poll" role="doc-endnote">
      <p>On the other hand, <a href="https://twitter.com/pkhuong/status/1530678742447579136">when I asked twitter to think about it, most response were wildly optimistic</a>, maybe because people were thinking of throughput and not latency. <a href="#fnref:poll" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-strictly" role="doc-endnote">
      <p>The first 64-bit field can be arbitrarily structured, and, e.g., begin with a sharding key. The output also isn’t <em>incorrect</em> if the second integer is always 0 or a table-specific value.  However, losing that entropy makes it easier for an attacker to correlate ids across tables. <a href="#fnref:not-strictly" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:latency-throughput" role="doc-endnote">
      <p>It’s important to measure latency and not throughput because we can expect to decode one id at a time, and immediately block with a data dependency on the decoded result. Encoding may sometimes be closer to a throughput problem, but low latency usually implies decent throughput, while the converse is often false. For example, a <a href="https://www.bbc.com/news/uk-england-london-51433720">747 carrying 400 passengers across the Atlantic in just under 5 hours</a> is more efficient in person-km/h (throughput) than a <a href="https://www.britishairways.com/en-us/information/about-ba/history-and-heritage/celebrating-concorde">Concorde, with a maximum capacity of 100 passengers</a>, but the Concorde was definitely faster: three and a half hours from JFK to LHR is shorter than five hours, and that’s the metric individual passengers usually care about. <a href="#fnref:latency-throughput" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:compliance" role="doc-endnote">
      <p>Most likely an easier route than AES in a corporate setting that’s likely to mandate frequent key rotation. <a href="#fnref:compliance" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:uuidv7" role="doc-endnote">
      <p>Or copy <a href="https://www.ietf.org/archive/id/draft-peabody-dispatch-new-uuid-format-04.html#v7">UUIDv7</a>, with its 48-bit timestamp and 74 bit random value. <a href="#fnref:uuidv7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:rotation" role="doc-endnote">
      <p>Rotating symmetric keys isn’t hard a technical problem, when generating UUIDs with a Feistel network: we can use 1-2 bits to identify keys, and eventually reuse key ids.  Rotation however must imply that we will eventually fail to decode (reject) old ids, which may be a bug or a feature, depending on who you ask. A saving grace may be that it should be possible for a service to update old external ids to the most recent symmetric key without accessing any information except the symmetric keys. <a href="#fnref:rotation" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hacking tips for Linux perf porcelain]]></title>
    <link href="https://www.pvk.ca/Blog/2022/06/01/hacking-tips-for-linux-perf-porcelain/"/>
    <updated>2022-06-01T21:09:08-04:00</updated>
    <id>https://www.pvk.ca/Blog/2022/06/01/hacking-tips-for-linux-perf-porcelain</id>
    <content type="html"><![CDATA[<p>Sometimes you just want to abuse <a href="https://perf.wiki.kernel.org/index.php/Main_Page">Linux <code>perf</code></a>
to make it do a thing it’s not designed for, and a proper C program
would represent an excessive amount of work.</p>

<p>Here are two tricks I find helpful when jotting down hacky analysis scripts.</p>

<h2 id="programmatically-interacting-with-addr2line--i">Programmatically interacting with <code>addr2line -i</code></h2>

<p>Perf can resolve symbols itself, but <a href="https://sourceware.org/binutils/docs/binutils/addr2line.html">addr2line</a>
is a lot more flexible (especially when you inflict
<a href="https://github.com/libhugetlbfs/libhugetlbfs/blob/6b126a4d7da9490fa40fe7e1b962edcb939feddc/HOWTO#L25-L30">subtle things on your executable’s mappings</a>).</p>

<p>It’s already nice that <code>addr2line -Cfe /path/to/binary</code> lets you write
hex addresses to stdin and spits out the corresponding function name
on one line, and its source location on the next (or <code>??</code> / <code>??:0</code> if
debug info is missing).  However, for heavily inlined (<em>cough</em> C++
<em>cough</em>) programs, you really want the whole callstack that’s encoded
in the debug info, not just the most deeply inlined function (“oh
great, it’s in <code>std::vector&lt;Foo&gt;::size()</code>”).</p>

<p><a href="https://sourceware.org/binutils/docs/binutils/addr2line.html#:~:text=%2Di-,%2D%2Dinlines,-If%20the%20address">The <code>--inline</code> flag</a>
addresses that… by printing source locations for inline callers on
their own line(s).  Now that the output for each address can span
a variable number of lines, how is one to know when to stop reading?</p>

<p>A simple trick is to always write <em>two</em> addresses to <code>addr2line</code>’s
standard input: the address we want to symbolicate, and
that never has debug info (e.g., 0).</p>

<p>EDIT: <a href="https://twitter.com/trav_downs/status/1532206949038624768">Travis Downs reports that <code>llvm-addr2line-14</code> finds debug info for 0x0</a>
(presumably a bug. I don’t see that on llvm-addr2line-12)
and suggests looking for <code>0x0.*</code> in addition to <code>??</code>/<code>??:0</code>.  It’s
easy enough to stop when either happens, and clang’s version of
<code>addr2line</code> can be a lot faster than binutil’s on files with a lot of
debug information.<sup id="fnref:more-robust" role="doc-noteref"><a href="#fn:more-robust" class="footnote" rel="footnote">1</a></sup></p>

<p>We now know that the first set of resolution information lines (one
line when printing only the file and line number, two lines when
<a href="https://sourceware.org/binutils/docs/binutils/addr2line.html#:~:text=%2Df-,%2D%2Dfunctions,-Display%20function%20names">printing function names as well with <code>-f</code></a>)
belongs to the address we want to symbolicate.  We also know to
expect output for missing information (<code>??:0</code> or <code>??</code> / <code>??:0</code>)
from the dummy address.  We can thus keep reading until we find
a set of lines that corresponds to missing information, and 
disregard that final source info.</p>

<p>For example, passing <code>$IP\n0\n</code> on stdin could yield:</p>

<pre><code>??
??:0
??
??:0
</code></pre>

<p>or, without <code>-f</code> function names,</p>

<pre><code>??:0
??:0
</code></pre>

<p>In both cases we first consume the first set of lines (the output
for<code>$IP</code> must include at least one record), then consume the next set
of lines and observe it represent missing information, so we stop
reading.</p>

<p>When debug information is present, we might instead find</p>

<pre><code>foo()
src/foo.cc:10
??
??:0
</code></pre>

<p>The same algorithm clearly works.</p>

<p>Finally, with inlining, we might instead observe</p>

<pre><code>inline_function()
src/bar.h:5
foo()
src/foo.cc:12
??
??:0
</code></pre>

<p>We’ll unconditionally assign the first pair of lines to <code>$IP</code>,
read a second pair of lines, see that it’s not <code>??</code> / <code>??:0</code>
and push that to the bottom of the inline source location
stack, and finally stop after reading the third pair of lines.</p>

<h2 id="triggering-pmu-events-from-non-pmu-perf-events">Triggering PMU events from non-PMU perf events</h2>

<p>Performance monitoring events in perf tend to be much more powerful
than non-PMU events: each perf “driver” works independently, so only
PMU events can snapshot <a href="https://man7.org/linux/man-pages/man1/perf-intel-pt.1.html">the Processor Trace buffer</a>,
for example.</p>

<p>However, we sometimes really want to trigger on a non-PMU event.  For
example, we might want to watch for writes to a specific address with
a hardware breakpoint, and snapshot the PT buffer to figure out what
happened in the microseconds preceding that write.  Unfortunately,
that doesn’t work out of the box: only PMU events can snapshot the
buffer.  I remember running into a similar limitation when I wanted to
capture performance counters after non-PMU events.</p>

<p>There is however a way to trigger PMU events from most non-PMU events:
watch for <a href="https://perfmon-events.intel.com/index.html?pltfrm=snb.html&amp;evnt=BR_INST_RETIRED.FAR_BRANCH">far branches</a>!
I believe I also found these events much more reliable to detect
preemption than the scheduler’s software event, many years ago.</p>

<p>Far branches are rare (they certainly don’t happen in regular x86-64
userspace program), but interrupt usually trigger a far CALL to
execute the handler in ring 0 (attributed to ring 0), and a far RET to
switch back to the user program (attributed to ring 3).</p>

<p>We can thus configure</p>

<pre><code>perf record \
    -e intel_pt//u \
    -e BR_INST_RETIRED.FAR_BRANCH/aux-sample-size=...,period=1/u \
    -e mem:0x...:wu ...
</code></pre>

<p>to:</p>

<ol>
  <li>trigger a debug interrupt when userspace writes to the watched
memory address</li>
  <li>which will increment the <code>far_branch</code> performance monitoring
counter</li>
  <li>which triggers Linux’s performance monitoring interrupt handler</li>
  <li>which will finally write both the far branch event and its
associated PT buffer to the perf event ring buffer.</li>
</ol>

<p>Not only does this work, but it also minimises the trigger latency.
That’s a big win compared to, e.g., <a href="https://man7.org/linux/man-pages/man1/perf-record.1.html#:~:text=%2D%2Dswitch%2Doutput%2Devent">perf record’s built-in <code>--switch-output-event</code></a>:
a trigger latency on the order of hundreds of microseconds forces a
large PT buffer in order to capture the period we’re actually
interested in, and copying that large buffer slows down everything.</p>

<h2 id="is-this-documented">Is this documented?</h2>

<p>Who knows? (Who cares?) These tricks fulfill a common need in quick
hacks, and I’ve been using (and rediscovering) them for years.</p>

<p>I find tightly scoped tools that don’t try to generalise have an ideal
insight:effort ratio.  Go write your own!</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:more-robust" role="doc-endnote">
      <p>I ended up generating passing a string suffixed with a UUIDv4 as a sentinel: <code>llvm-addr2line</code> just spits back any line that doesn’t look addresses.  Alexey Alexandrov on the profiler developers’ slack noted that <code>llvm-symbolizer</code> cleanly terminates each sequence of frames with an empty line. <a href="#fnref:more-robust" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bounded dynamicism with cross-modifying code]]></title>
    <link href="https://www.pvk.ca/Blog/2021/12/19/bounded-dynamicism-with-cross-modifying-code/"/>
    <updated>2021-12-19T19:43:01-05:00</updated>
    <id>https://www.pvk.ca/Blog/2021/12/19/bounded-dynamicism-with-cross-modifying-code</id>
    <content type="html"><![CDATA[<p><small>Originally posted on the <a href="https://engineering.backtrace.io/2021-12-19-bounded-dynamicism-with-cross-modifying-code/">Backtrace I/O tech blog</a>.</small></p>

<p>All long-lived programs are either implemented in <a href="https://wiki.c2.com/?DynamicLanguage">dynamic languages</a>,<sup id="fnref:images" role="doc-noteref"><a href="#fn:images" class="footnote" rel="footnote">1</a></sup>
or eventually <a href="https://en.wikipedia.org/wiki/Greenspun%27s_tenth_rule">Greenspun</a>
themselves into subverting static programming
languages to create a dynamic system (e.g., Unix process trees). The
latter approach isn’t a bad idea, but it’s easy to introduce more
flexibility than intended (e.g.,
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228">data-driven JNDI lookups</a>)
when we add late binding features piecemeal, without a holistic view
of how all the interacting components engender a weird program
modification language.</p>

<p>At Backtrace, we mostly implement late (re)binding by isolating subtle
logic in dedicated executables with short process lifetimes: we can
replace binaries on disk atomically, and their next invocation will
automatically pick up the change. In a pinch, we sometimes edit
template or Lua source files and hot reload them in <a href="http://openresty.org/en/">nginx</a>.
We prefer this to first-class programmatic support for runtime
modification because Unix has a well understood permission model
around files, and it’s harder to bamboozzle code into overwriting
files when that code doesn’t perform any disk I/O.</p>

<p>However, these patterns aren’t always sufficient. For example, we
sometimes wish to toggle code that’s deep in performance-sensitive
query processing loops, or tightly coupled with such logic. That’s
when we rely on <a href="https://github.com/backtrace-labs/dynamic_flag">our <code>dynamic_flag</code> library</a>.</p>

<p>This library lets us tweak flags at runtime, but flags can only take
boolean values (enabled or disabled), so the dynamicism it introduces
is hopefully bounded enough to avoid unexpected emergent
complexity.  The functionality looks like classic feature flags,
but thanks to the flags’ minimal runtime overhead coupled with the
ability to flip them at runtime, there are additional use cases, such
as disabling mutual exclusion logic during single-threaded startup or
toggling log statements. The library has also proved invaluable for crisis
management, since we can leave flags (enabled by default) in
well-trodden pieces of code without agonising over their impact on
application performance. These flags can serve as ad hoc circuit
breakers around complete features or specific pieces of code when new
inputs tickle old latent bugs.</p>

<p>The secret behind this minimal overhead? Cross-modifying machine code!</p>

<p>Intel tells us we’re not supposed to do that, at least not without
pausing threads… yet the core of <a href="https://github.com/backtrace-labs/dynamic_flag">the <code>dynamic_flag</code> C library</a>
has been toggling branches on thousands of machines for years, without
any issue.  It’s available under the
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/main/LICENSE">Apache license</a>
for other adventurous folks.</p>

<h2 id="overhead-matters">Overhead matters</h2>

<p>Runtime efficiency is an essential feature in <code>dynamic_flag</code>—
enough to justify mutating machine code while it’s executing on other
cores
—not only because it unlocks additional use cases, but, more
importantly, because it frees programmers from worrying about the
performance impact of branching on a flag in the most obvious
location, even if that’s in the middle of a hot inner loop.</p>

<p>With the aim of encouraging programmers to spontaneously protect code
with flag checks, without prodding during design or code review, we
designed <code>dynamic_flag</code> to minimise the amount of friction and mental
overhead of adding a new feature flag. That’s why we care so much
about all forms of overhead, not just execution time. For example,
there’s no need to break one’s flow and register flags separately from
their use points. Adding a feature flag should not feel like a chore.</p>

<p>However, we’re also aware that feature flags tend to stick around
forever. We try to counteract this inertia with static registration:
all the <code>DF_*</code> expansions in an executable appear in
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L458-L466">its <code>dynamic_flag_list</code> section</a>, and
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L247-L277">the <code>dynamic_flag_list_state</code> function</a>
enumerates them at runtime. Periodic audits will reveal flags that
have become obsolete, and flags are easy to find:
each flag’s full name includes its location in the source code.</p>

<p>We find value in <code>dynamic_flag</code> because its runtime overhead is
negligible for all but the most demanding
code,<sup id="fnref:macro-writers-bill-of-rights" role="doc-noteref"><a href="#fn:macro-writers-bill-of-rights" class="footnote" rel="footnote">2</a></sup> while the interface lets us easily
make chunks of code toggleable at runtime without having to worry
about things like “where am I supposed to register this new option?”
The same system is efficient <em>and</em> ergonomic enough for all teams in
all contexts, avoids contention in our source tree, and guarantees
discoverability for whoever happens to be on call.</p>

<h2 id="how-to-use-dynamic_flag">How to use <code>dynamic_flag</code></h2>

<p>All dynamic flags have a “kind” (namespace) string, and a name. We
often group all flags related to an experimental module or feature in
the same “kind,” and use the name to describe the specific functionality
in the feature guarded by the flag. A dynamic flag can be disabled by
default (like a feature flag), or enabled by default, and evaluating a
dynamic flag’s value implicitly defines and registers it with the
<code>dynamic_flag</code> library.</p>

<p>A dynamic flag introduced with <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L98-L110">the <code>DF_FEATURE</code> macro</a>,
as in the code snippet below, is disabled (evaluates to false) by
default, and instructs the compiler to optimise for that default value.</p>

<iframe width="800px" height="400px" src="https://godbolt.org/e#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:16,endLineNumber:12,positionColumn:16,positionLineNumber:12,selectionStartColumn:16,selectionStartLineNumber:12,startColumn:16,startLineNumber:12),source:'%23include+%3Cstdio.h%3E%0A%23include+%3Chttps://raw.githubusercontent.com/backtrace-labs/dynamic_flag/main/include/dynamic_flag.h%3E%0A%0Aint+new_feature(void)%3B%0A%0Aint+foo(void)%0A%7B%0A%0A++++/*+...+preexisting+code...+*/%0A++++printf(%22preexisting+prologue%5Cn%22)%3B%0A%0A++++if+(DF_FEATURE(my_module,+flag_name))+%7B%0A++++++++new_feature()%3B%0A++++%7D%0A%0A++++/*+Old+feature+code+*/%0A++++printf(%22preexisting+epilogue%5Cn%22)%3B%0A++++return+1%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cg112,filters:(b:'0',binary:'0',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'1',libraryCode:'0',trim:'1'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'-O2+-c',selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1,tree:'1'),l:'5',n:'0',o:'x86-64+gcc+11.2+(C,+Editor+%231,+Compiler+%231)',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4"></iframe>

<p>We can instead enable code by default and optimise for cases where the
flag is enabled with <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L112-L124">the <code>DF_DEFAULT</code> macro</a>.</p>

<iframe width="800px" height="400px" src="https://godbolt.org/e#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:13,endLineNumber:12,positionColumn:13,positionLineNumber:12,selectionStartColumn:13,selectionStartLineNumber:12,startColumn:13,startLineNumber:12),source:'%23include+%3Cstdio.h%3E%0A%23include+%3Chttps://raw.githubusercontent.com/backtrace-labs/dynamic_flag/main/include/dynamic_flag.h%3E%0A%0Aint+now_old_feature(void)%3B%0A%0Aint+foo(void)%0A%7B%0A%0A++++/*+...+preexisting+code...+*/%0A++++printf(%22preexisting+prologue%5Cn%22)%3B%0A%0A++++if+(DF_DEFAULT(my_module,+flag_name))+%7B%0A++++++++now_old_feature()%3B%0A++++%7D%0A%0A++++/*+Old+feature+code+*/%0A++++printf(%22preexisting+epilogue%5Cn%22)%3B%0A++++return+1%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cg112,filters:(b:'0',binary:'0',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'1',libraryCode:'0',trim:'1'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'-O2+-c',selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1,tree:'1'),l:'5',n:'0',o:'x86-64+gcc+11.2+(C,+Editor+%231,+Compiler+%231)',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4"></iframe>

<p>Each <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L98-L155"><code>DF_*</code> condition</a>
in the source is actually its own flag;
a flag’s full name looks like <code>kind:name@source_file:line_number</code> (e.g.,
<code>my_module:flag_name@&lt;stdin&gt;:15</code>), and each condition has its own
state record. It’s thus safe, if potentially confusing, to define flags
of different types (feature or default) with the same kind and
name. These macros may appear in <a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:17,positionColumn:1,positionLineNumber:17,selectionStartColumn:1,selectionStartLineNumber:17,startColumn:1,startLineNumber:17),source:'%23include+%3Chttps://raw.githubusercontent.com/backtrace-labs/dynamic_flag/main/include/dynamic_flag.h%3E%0A%0Aint+old_feature(void)%3B%0A%0Aextern+inline+int+foo(void)%0A%7B%0A%0A++++/*+...+preexisting+code...+*/%0A%0A++++if+(DF_DEFAULT(my_module,+flag_name))+%7B%0A++++++++return+old_feature()%3B%0A++++%7D%0A%0A++++/*+Old+feature+code+*/%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cclang1300,filters:(b:'0',binary:'0',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'1',libraryCode:'0',trim:'1'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'-O2+-c',selection:(endColumn:9,endLineNumber:5,positionColumn:9,positionLineNumber:5,selectionStartColumn:9,selectionStartLineNumber:5,startColumn:9,startLineNumber:5),source:1,tree:'1'),l:'5',n:'0',o:'x86-64+clang+13.0.0+(C,+Editor+%231,+Compiler+%231)',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4">inline</a>
or
<a href="https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:18,endLineNumber:21,positionColumn:18,positionLineNumber:21,selectionStartColumn:18,selectionStartLineNumber:21,startColumn:18,startLineNumber:21),source:'%23include+%3Chttps://raw.githubusercontent.com/backtrace-labs/dynamic_flag/main/include/dynamic_flag.h%3E%0A%0Aint+old_feature(void)%3B%0A%0Astatic+inline+int+foo(void)%0A%7B%0A%0A++++/*+...+preexisting+code...+*/%0A%0A++++if+(DF_DEFAULT(my_module,+flag_name))+%7B%0A++++++++return+old_feature()%3B%0A++++%7D%0A%0A++++/*+Old+feature+code+*/%0A++++return+0%3B%0A%7D%0A%0Aint+bar(void)%0A%7B%0A%0A++++return+foo()%3B%0A%7D'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cg112,filters:(b:'0',binary:'0',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'1',libraryCode:'0',trim:'1'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'-O2+-c',selection:(endColumn:9,endLineNumber:5,positionColumn:9,positionLineNumber:5,selectionStartColumn:9,selectionStartLineNumber:5,startColumn:9,startLineNumber:5),source:1,tree:'1'),l:'5',n:'0',o:'x86-64+gcc+11.2+(C,+Editor+%231,+Compiler+%231)',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4">static inline</a>
functions: each instantiation will get its own metadata block, and an
arbitrary number of blocks can share the same full name.</p>

<p>Before manipulating these dynamic flags,
applications must <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L291">call <code>dynamic_flag_init_lib</code> to initialise the library’s state</a>.
Once the library is initialised, interactive or configuration-driven
usage typically toggles flags by calling
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L220-L230"><code>dynamic_flag_activate</code> and <code>dynamic_flag_deactivate</code></a> with POSIX extended regexes that match
the flags’ full names.</p>

<h2 id="using-dynamic_flag-programmatically">Using <code>dynamic_flag</code> programmatically</h2>

<p>The <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L98-L124"><code>DF_FEATURE</code> and <code>DF_DEFAULT</code> macros</a>
directly map to classic feature flags, but the
<code>dynamic_flag</code> library still has more to offer. Applications can
programmatically enable and disable blocks of code to implement a
restricted form of <a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming">aspect oriented programming</a>:
<a href="https://en.wikipedia.org/wiki/Advice_(programming)">“advice”</a> cannot
be inserted post hoc, and must instead be defined inline in the
source, but may be toggled at runtime by unrelated code.</p>

<p>For example, an application could let individual HTTP requests opt
into detailed tracing with a query string parameter <code>?tracing=1</code>, and
set <code>request-&gt;tracing_mode = true</code> in its internal request object when
it accepts such a request. Environments where fewer than one request
in a million enables tracing could easily spend more aggregate time
evaluating <code>if (request-&gt;tracing_mode == true)</code> than they do in the
tracing logic itself.
One could try to reduce the overhead by coalescing the trace code in
fewer conditional blocks, but that puts more distance between the
tracing code and the traced logic it’s supposed to record, which
tends to cause the two to desynchronise and adds to development
friction.</p>

<p>It’s tempting to instead optimise frequent checks for the common case
(no tracing) with a dynamic flag that is enabled whenever at least one
in-flight request has opted into tracing. That’s why
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L141-L155">the <code>DF_OPT</code> (for opt-in logic) macro</a> exists.</p>

<iframe width="800px" height="400px" src="https://godbolt.org/e#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:10,positionColumn:1,positionLineNumber:10,selectionStartColumn:1,selectionStartLineNumber:10,startColumn:1,startLineNumber:10),source:'%23include+%3Cstdbool.h%3E%0A%23include+%3Cstdio.h%3E%0A%23include+%3Chttps://raw.githubusercontent.com/backtrace-labs/dynamic_flag/main/include/dynamic_flag.h%3E%0A%0Astruct+request+%7B%0A++++bool+tracing_mode%3B%0A%7D%3B%0A%0Avoid+trace_request(struct+request+*req,+const+char+*func)%3B%0A%0Avoid+foo(struct+request+*req)%0A%7B%0A++++if+(DF_OPT(request_tracing,+check)+%26%26+req-%3Etracing_mode)%0A++++++++trace_request(req,+__FUNCTION__)%3B%0A++++%0A++++return%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:cg112,filters:(b:'0',binary:'0',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'1',libraryCode:'0',trim:'1'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'-O2+-c',selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1,tree:'1'),l:'5',n:'0',o:'x86-64+gcc+11.2+(C,+Editor+%231,+Compiler+%231)',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4"></iframe>

<p>The
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L141-L155"><code>DF_OPT</code> macro</a>
instructs the compiler to assume the flag is disabled, but leaves the
flag enabled (i.e., the conditional always evaluates
<code>request-&gt;tracing_mode</code>) until the library is initialised with
<code>dynamic_flag_init_lib</code>.<sup id="fnref:unreachable" role="doc-noteref"><a href="#fn:unreachable" class="footnote" rel="footnote">3</a></sup> After initialisation,
the flag acts like a <code>DF_FEATURE</code> (i.e., the overhead is a <code>test eax</code>
instruction that falls through without any conditional branching)
until it is explicitly enabled again.</p>

<p>With this flag-before-check pattern, it’s always safe to enable
<code>request_tracing</code> flags: in the worst case, we’ll just look at the
request object, see that <code>request-&gt;tracing_mode == false</code>, and skip
the tracing logic. Of course, that’s not ideal for performance. When
we definitely know that no request has asked for tracing, we want to
disable <code>request_tracing</code> flags and not even look at the request
object’s <code>tracing_mode</code> field.</p>

<p>Whenever the application receives a request that opts into tracing, it
can enable all flags with kind <code>request_tracing</code> by
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L189-L205">executing <code>dynamic_flag_activate_kind(request_tracing, NULL)</code></a>.
When that same request leaves the system (e.g., when the application
has fully sent a response back), the application
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L207-L218">undoes the activation with <code>dynamic_flag_deactivate_kind(request_tracing, NULL)</code></a>.</p>

<p>Activation and deactivation calls actually increment and decrement
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/src/dynamic_flag.c#L99-L110">counters associated with each instance of a <code>DF_...</code> macro</a>, so this scheme works correctly when
multiple requests with overlapping lifetimes opt into tracing:
tracing blocks will check whether <code>request-&gt;tracing_mode == true</code>
whenever at least one in-flight request has <code>tracing_mode == true</code>, and
skip these conditionals as soon as no such request exists.</p>

<h2 id="practical-considerations-for-programmatic-manipulation">Practical considerations for programmatic manipulation</h2>

<p>Confirming that a flag is set to its expected value (disabled for
<code>DF_FEATURE</code> and <code>DF_OPT</code>, enabled for <code>DF_DEFAULT</code>) is fast…
because we shifted all the complexity to the flag flipping
code. Changing the value for a set of flags is extremely slow
(milliseconds of runtime and several <a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">IPIs</a> for
<a href="https://man7.org/linux/man-pages/man2/mprotect.2.html">multiple <code>mprotect(2)</code> calls</a>), so it only makes sense to use dynamic flags when they are
rarely activated or deactivated (e.g., less often than once a minute
or even less often than once an hour).</p>

<p>We have found programmatic flag manipulation to be useful not just for
opt-in request tracing or to enable log statements, but also to
minimise the impact of complex logic on program phases that do not
require them. For example, mutual exclusion and
<a href="http://www.cs.toronto.edu/~tomhart/papers/tomhart_thesis.pdf">safe memory reclamation deferral (PDF)</a>
may be redundant while a program is in a single-threaded
startup mode; we can guard such code behind <code>DF_OPT(steady_state, ...)</code>
to accelerate startup,
and enable <code>steady_state</code> flags just before spawning worker threads.</p>

<p>It can also make sense to guard slow paths with <code>DF_OPT</code> when
a program only enters phases that needs this slow path logic every few minutes. That was the case for a
<a href="https://www.youtube.com/watch?v=hZDr4pfz0Nc">software transactional memory system with batched updates</a>.
Most of the time, no update is in flight, so readers never have to
check for concurrent writes.  These checks can be guarded with
<code>DF_OPT(stm, ...)</code> conditions., as long as the program enables <code>stm</code>
flags around batches of updates.  Enabling and disabling all these
flags can take a while (milliseconds), but, as long as updates are
infrequent enough, the improved common case (getting rid of a memory
load and a conditional jump for a read barrier) means the tradeoff
is favourable.</p>

<p>Even when flags are controlled programmatically, it can be useful to
work around bugs by manually forcing some flags to remain enabled or
disabled.  In the tracing example above, we could find a crash in one
of the tracing blocks, and wish to prevent <code>request-&gt;tracing_mode</code> from
exercising that block of code.</p>

<p>It’s easy to force a flag into an active state: flag activations
are counted, so it suffices to <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L220-L224">activate it</a>
manually, once. However, we want it to be safe <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L226-L230">issue ad hoc <code>dynamic_flag_deactivate</code> calls</a>
without wedging the system in a weird state, so activation counts don’t go negative.
Unfortunately, this means we can’t use deactivations
to prevent, e.g., a crashy request tracing block from being
activated.</p>

<p>Flags can instead be “unhooked” dynamically.  While unhooked,
increments to a flag’s activation count are silently disregarded.
The <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L232-L239"><code>dynamic_flag_unhook</code> function</a>
unhooks <code>DF_*</code> conditions when their full name matches the extended POSIX regular expression it received as an argument.
When a flag has been
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L232-L239">“unhook”ed</a>
more often than it has been
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L241-L245">“rehook”ed</a>,
attempts to activate it will
silently no-op. Once a flag has been unhooked, we can
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L226-L230">issue <code>dynamic_flag_deactivate</code> calls</a>
until <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/src/dynamic_flag.c#L99-L110">its activation count reaches 0</a>.
At that point, the flag is disabled, and will remain disabled
until rehooked.</p>

<h2 id="the-core-implementation-trick">The core implementation trick</h2>

<p>The <a href="https://gcc.gnu.org/legacy-ml/gcc-patches/2009-07/msg01556.html">introduction of <code>asm goto</code> in GCC 4.5</a>
made it possible to <a href="https://lwn.net/Articles/350714/">implement control operators in inline assembly</a>.
When the condition actually varies at runtime, it usually
makes more sense to <a href="https://gcc.gnu.org/onlinedocs/gcc/Extended-Asm.html#:~:text=6.47.2.4%20Flag%20Output%20Operands">set an output variable with a condition code</a>,
but <code>dynamic_flag</code> conditions are actually static in machine code:
each <code>DF_*</code> macro expands to <em>one</em> 5-byte instruction,
<a href="https://c9x.me/x86/html/file_module_x86_id_315.html">a <code>test eax, imm32</code> instruction</a>
that falls through to the common case when that’s the flag’s value
(i.e., enabled for <code>DF_DEFAULT</code>, disabled for <code>DF_FEATURE</code> and
<code>DF_OPT</code>), and a <a href="https://c9x.me/x86/html/file_module_x86_id_147.html">32-bit relative <code>jmp rel32</code> to the unexpected path</a>
(disabled for <code>DF_DEFAULT</code>, enabled for <code>DF_FEATURE</code> and <code>DF_OPT</code>)
otherwise. Activating and deactivating dynamic flags toggles the
corresponding target instructions between <code>test imm32</code> (0xA9) and <code>jmp rel32</code> (0xE9).</p>

<p>The <code>DF_...</code> macros expand into a
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L443-L478">lot more inline assembly than just that one instruction</a>;
the rest of the expansion is a lot of noise to
register everything with structs and pointers in dedicated
sections. Automatic static registration is mostly orthogonal to the
performance goals, but is key to the (lazy-)programmer-friendly
interface.</p>

<p>We use <code>test eax, imm32</code> instead of a nop because it’s exactly five
bytes, just like <code>jmp rel32</code>, and because its 4-byte immediate is in
the same place as the 4-byte offset of <code>jmp rel32</code>. We can thus encode
the jump offset at assembly-time, and flip between falling through to
the common path (<code>test</code>) and jumping to the unexpected path (<code>jmp</code>) by
overwriting the opcode byte (0xA9 for <code>test</code>, 0xE9 for <code>jmp</code>).</p>

<p>Updating a single byte for each dynamic flag avoids questions
around the correct order for writes.  This single-byte
cross-modification (we overwrite instruction bytes while other threads
may be executing the mutated machine code) also doesn’t affect the
size of the instruction (both <code>test eax</code> and <code>jmp rel</code> span 5 bytes),
which should hopefully suffice to avoid sharp edges around instruction
decoding in hardware, despite our disregard for
<a href="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3a-part-1-manual.pdf#page=260">Intel’s recommendations regarding cross-modifying code in Section 8.1.3 of the SDM</a>.<sup id="fnref:cpu-millenia" role="doc-noteref"><a href="#fn:cpu-millenia" class="footnote" rel="footnote">4</a></sup></p>

<p>The library does try to protect against code execution exploits by
<a href="https://man7.org/linux/man-pages/man2/mprotect.2.html">relaxing and reinstating page protection with <code>mprotect(2)</code></a>) around
all cross modification writes.  Since <code>mprotect</code>-ing from
Read-Write-eXecute permissions to Read-eXecute acts as a
<a href="https://lwn.net/Articles/728795/#:~:text=These%20users%20have%20found%20a%20trick%20to%20get%20the%20desired%20behavior%20without%20calling%20membarrier()%3A%20they%20make%20a%20call%20to%20either%20mprotect()%20or%20munmap()%20instead"><code>membarrier</code> (issues IPIs) on Linux/x86-64</a>,
we can also know that the updated code is globally visible by the time
a <a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/include/dynamic_flag.h#L189-L230">call to <code>dynamic_flag_activate</code>, etc.,</a> returns.</p>

<p>It’s not practical to bounce page protection for <em>each</em> <code>DF_</code> expansion,
especially with inlining (some users have hundreds of inlined calls to
flagged functions, e.g., to temporarily paper over
use-after-frees by nopping out a few calls to <code>free(2)</code>). Most of the
complexity in <code>dynamic_flag.c</code> is simply in
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/src/dynamic_flag.c#L474-L529">gathering metadata records for all <code>DF_</code> sites that should be activated or deactivated</a>, and in
<a href="https://github.com/backtrace-labs/dynamic_flag/blob/00381c2cab5c8628e6a7d18730a98f7d7e6712f2/src/dynamic_flag.c#L390-L449">amortising <code>mprotect</code> calls for stretches of <code>DF_</code> sites on contiguous pages</a>.</p>

<h2 id="sometimes-code-is-just-done">Sometimes, code is just done</h2>

<p>The <a href="https://github.com/backtrace-labs/dynamic_flag"><code>dynamic_flag</code> library</a>
is an updated interface for the core implementation of the
<a href="https://github.com/appnexus/acf/blob/master/common/an_hook.h">6-year old <code>an_hook</code></a>,
and reflects years of experience with that functionality. We’re happy
to share it, but aren’t looking for feature requests or contributions.</p>

<p>There might be some small clean-ups as we add support for ARM or RISC
V, or let the library interoperate with a Rust implementation.
However, we don’t expect changes to the interface, i.e., the <code>DF_</code> macros
and the activation/deactivation functions, nor to its core structure,
especially given the contemporary tastes for hardening (for example,
the cross-modification approach is completely incompatible with
OpenBSD’s and OS X’s strict <code>W^X</code> policies). The library works for our
target platforms, and we don’t wish to take on extra complexity that
is of no benefit to us.</p>

<p>Of course, <a href="https://github.com/backtrace-labs/dynamic_flag/blob/main/LICENSE">it’s Apache licensed</a>,
so anyone can fork the library and twist it beyond
recognition. However, if you’re interested in powerful patching
capabilities, dynamic languages (e.g., Erlang, Common Lisp, or even
Python and Ruby), or tools like <a href="https://liveplusplus.tech">Live++</a>
and <a href="https://www.indefiant.com/">Recode</a> may be more appropriate.<sup id="fnref:ask-games" role="doc-noteref"><a href="#fn:ask-games" class="footnote" rel="footnote">5</a></sup>
We want <code>dynamic_flag</code> to remain simple and just barely flexible
enough for our usage patterns.</p>

<p><small>Thank you, Jacob, Josh, and Per, for feedback on earlier
versions.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:images" role="doc-endnote">
      <p>It’s no accident that canonical dynamic languages like Smalltalk, Forth, and Lisp are all image-based: how would an image-based system even work if it were impossible to redefine functions or types? <a href="#fnref:images" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:macro-writers-bill-of-rights" role="doc-endnote">
      <p>Like <a href="https://www.youtube.com/watch?v=LIEX3tUliHw">guaranteed optimisations in Lisps</a>, the predictable performance impact isn’t important because all code is performance sensitive, but because performance is a cross-cutting concern, and a predictably negligible overhead makes it easier to implement new abstractions, especially with the few tools available in C.  In practice, the impact of considering a code path reachable in case a flag is flipped from its expected value usually dwarfs that of the single <code>test</code> instruction generated for the dynamic flag itself. <a href="#fnref:macro-writers-bill-of-rights" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:unreachable" role="doc-endnote">
      <p>Or if the <code>dynamic_flag</code> library isn’t aware of that <code>DF_OPT</code>, maybe because the function surrounding that <code>DF_OPT</code> conditional was loaded dynamically. <a href="#fnref:unreachable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cpu-millenia" role="doc-endnote">
      <p>After a few CPU-millenia of production experience, the cross-modification logic hasn’t been associated with any “impossible” bug, or with any noticeable increase in the rate of hardware hangs or failures. <a href="#fnref:cpu-millenia" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ask-games" role="doc-endnote">
      <p>The industry could learn a lot from game development practices, <em>especially</em> for stateful non-interactive backend servers and slow batch computations. <a href="#fnref:ask-games" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slitter: a slab allocator that trusts, but verifies]]></title>
    <link href="https://www.pvk.ca/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator/"/>
    <updated>2021-08-01T17:26:04-04:00</updated>
    <id>https://www.pvk.ca/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator</id>
    <content type="html"><![CDATA[<p><small>Originally posted on the <a href="https://engineering.backtrace.io/2021-08-04-slitter-a-slab-allocator-that-trusts-but-verifies/">Backtrace I/O tech blog</a>.</small></p>

<p><a href="https://github.com/backtrace-labs/slitter">Slitter</a> is Backtrace’s
deliberately middle-of-the-road
<a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">thread-caching</a>
<a href="https://people.eecs.berkeley.edu/~kubitron/courses/cs194-24-S13/hand-outs/bonwick_slab.pdf">slab allocator</a>,
with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/include/slitter.h#L7-L44">explicit allocation class tags</a>
(rather than derived from the object’s size class).
It’s mostly written in Rust, and we use it in our C backend server.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a>’s design is about as
standard as it gets: we hope to dedicate the project’s complexity
budget to always-on “observability” and safety features.  We don’t
wish to detect all or even most memory management errors, but we
should statistically catch a small fraction (enough to help pinpoint
production issues) of such bugs, and <em>always</em> constrain their scope to
the mismanaged allocation class.<sup id="fnref:blast-radius" role="doc-noteref"><a href="#fn:blast-radius" class="footnote" rel="footnote">1</a></sup></p>

<p>We decided to code up Slitter last April, when we noticed
that we would immediately benefit from backing allocation with
temporary file mappings:<sup id="fnref:when-does-it-flush" role="doc-noteref"><a href="#fn:when-does-it-flush" class="footnote" rel="footnote">2</a></sup>
the bulk of our data is mapped from
persistent data files, but we also regenerate some cold metadata
during startup, and accesses to that metadata have amazing locality,
both temporal and spatial (assuming bump allocation).  We don’t want the OS
to swap out all the heap–that way lie <a href="https://blog.acolyer.org/2017/06/15/gray-failure-the-achilles-heel-of-cloud-scale-systems/">grey failures</a>–so
we opt specific allocation classes into it.</p>

<p>By itself, this isn’t a reason to write a slab allocator: we could
easily have configured <a href="http://jemalloc.net/jemalloc.3.html#arena.i.extent_hooks">specialised arenas in jemalloc</a>,
for example.  However, we also had eyes on longer term improvements to
observability and debugging or mitigation of memory management errors in
production, and those could only be unlocked by migrating to an
interface with explicit tags for each allocation class (type).</p>

<p>Classic mallocs like <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
and <a href="https://github.com/google/tcmalloc">tcmalloc</a> are fundamentally
unable to match that level of integration: we can’t tell <code>malloc(3)</code>
what we’re trying to allocate (e.g., a <code>struct request</code> in the HTTP
module), only its size.  It’s still possible to wrap malloc in a richer
interface, and, e.g., track heap consumption by tag.  Unfortunately,
the result is slower than a native solution, and, without help from
the underlying allocator, it’s easy to incorrectly match tags between
<code>malloc</code> and <code>free</code> calls.  In my experience, this frequently leads to
useless allocation statistics, usually around the very faulty code
paths one is attempting to debug.</p>

<p>Even once we have built detailed statistics on top of a regular
malloc, it’s hard to convince the underlying allocator to only recycle
allocations within an object class: not only do mallocs eagerly
recycle allocations of similar sizes regardless of their type, but
they will also release unused runs of address space, or repurpose them
for totally different size classes.  That’s what mallocs are supposed
to do…  it just happens to also make debugging a lot harder when
things inevitably go wrong.<sup id="fnref:ub" role="doc-noteref"><a href="#fn:ub" class="footnote" rel="footnote">3</a></sup></p>

<p>Slab allocators work with semantically richer allocation tags: an
allocation tag describes its objects’ size, but can also specify how
to initialise, recycle, or deinitialise them.  The problem is that
slab allocators tend to focus exclusively on speed.</p>

<p><a href="https://github.com/omniti-labs/portableumem">Forks of libumem</a>
may be the exception, thanks to the Solaris culture of pervasive
hooking.  However, <code>umem</code>’s design reflects the sensibilities of the
00s, when it was written: threads share a few caches, and the
allocator tries to reuse address space.  In contrast, Slitter assumes memory
is plentiful enough for thread-local caches and type-stable
allocations.<sup id="fnref:not-as-configurable" role="doc-noteref"><a href="#fn:not-as-configurable" class="footnote" rel="footnote">4</a></sup></p>

<h2 id="our-experience-so-far">Our experience so far</h2>

<p>We have been running <a href="https://crates.io/crates/slitter">Slitter</a> in
production for over two months, and rely on it to:</p>

<ul>
  <li>detect when an allocation is freed with the wrong allocation class
tag (i.e., detect type confusion on free).</li>
  <li>avoid any in-band metadata: there are guard pages between
allocations and allocator metadata, and no intrusive freelist for
use-after-frees to stomp over.</li>
  <li>guarantee <a href="https://www.usenix.org/legacy/publications/library/proceedings/osdi96/full_papers/greenwald/node2.html">type stable allocations</a>:
once an address has been used to fulfill a request for a certain
allocation class, it will only be used for that class.  Slitter
doesn’t overlay intrusive lists on top of freed allocations, so the
data always reflects what the application last stored there.  This
means that double-frees and use-after-frees only affect the faulty
allocation class.  An application could even rely on
read-after-free being benign to simplify non-blocking algorithms.<sup id="fnref:but-we-dont-depend-on-it-too-much" role="doc-noteref"><a href="#fn:but-we-dont-depend-on-it-too-much" class="footnote" rel="footnote">5</a></sup></li>
  <li>let each allocation class specify how its backing memory should
be mapped in (e.g., plain 4 KB pages or file-backed swappable pages).</li>
</ul>

<p>Thanks to extensive contracts and a mix of hardcoded and random tests,
we encountered only two issues during the initial rollout, both in the
small amount of lock-free C code that is hard to test.<sup id="fnref:legacy-gcc" role="doc-noteref"><a href="#fn:legacy-gcc" class="footnote" rel="footnote">6</a></sup></p>

<p>Type stability exerts a heavy influence all over Slitter’s design, and
has obvious downsides.  For example, a short-lived application that
progresses through a pipeline of stages, where each stage allocates
different types, would definitely waste memory if it were to replace a
regular malloc with a type-stable allocator like Slitter.  We believe
the isolation benefits are more than worth the trouble, at least for
long-lived servers that quickly enter a steady state.</p>

<p>In the future, we hope to also:</p>

<ul>
  <li>detect when an interior pointer is freed.</li>
  <li>detect simple<sup id="fnref:jump" role="doc-noteref"><a href="#fn:jump" class="footnote" rel="footnote">7</a></sup> buffer overflows that cross allocation classes, by inserting guard pages.</li>
  <li>always detect frees of addresses Slitter does not manage.</li>
  <li>detect most back-to-back double-frees.</li>
  <li>detect a random fraction of buffer overflows, with a sampling <a href="https://en.wikipedia.org/wiki/Electric_Fence">eFence</a>.</li>
</ul>

<p>In addition to these safety features, we plan to rely on the allocator
to improve observability into the calling program, and wish to:</p>

<ul>
  <li>track the number of objects allocated and recycled in each
allocation class.</li>
  <li>sample the call stack when the heap grows.</li>
  <li>track allocation and release call stacks for a small fraction of objects.</li>
</ul>

<p>Here’s how it currently works, and why we wrote it in Rust, with dash
of C.</p>

<h2 id="the-high-level-design-of-slitter">The high level design of Slitter</h2>

<p>At a <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/doc/design.md">high level</a>,
Slitter</p>

<ol>
  <li>reserves shared 1 GB <code>Chunk</code>s of memory via the <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/src/mapper.rs"><code>Mapper</code> trait</a></li>
  <li>carves out smaller type-specific <code>Span</code>s from each chunk with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs"><code>Mill</code> objects</a></li>
  <li>bump-allocates objects from <code>Span</code>s with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/press.rs"><code>Press</code> objects</a>, into <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/magazine.rs">allocation <code>Magazines</code></a></li>
  <li>pushes and pops objects into/from <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/c/cache.c">thread-local magazines</a></li>
  <li>caches populated magazines in global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/class.rs#L62-L67">type-specific lock-free stacks</a></li>
  <li>manages empty magazines with a global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/rack.rs">mostly lock-free <code>Rack</code></a></li>
</ol>

<p>Many general purpose memory allocators implement strategies similarly
inspired by <a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">Bonwick’s slab allocator</a>,
and time-tested mallocs may well provide better performance
and lower fragmentation than Slitter.<sup id="fnref:type-stable" role="doc-noteref"><a href="#fn:type-stable" class="footnote" rel="footnote">8</a></sup>
The primary motivation for designing Slitter is that having explicit
allocation classes in the API makes it easier for the allocator to
improve the debuggability and resilience of the calling program.<sup id="fnref:also-good-for-perf" role="doc-noteref"><a href="#fn:also-good-for-perf" class="footnote" rel="footnote">9</a></sup>
For example, most allocators can tell you the size of your program’s
heap, but that data is much more useful when broken down by struct
type or program module.</p>

<p>Most allocators try to minimise accesses to the metadata associated with
allocations.  In fact, that’s often seen as a strength of the slab
interface: the allocator can just rely on the caller to pass the
correct allocation class tag, instead of hitting metadata to figure
out there the freed address should go.</p>

<p>We went in the opposite direction with Slitter.  We still rely on the
allocation class tag for speed, but also actively look for mismatches
before returning from deallocation calls. Nothing depends on
values computed by the mismatch detection logic, and the resulting
branch is trivially predictable (the tag always matches), so we can
hope that wide out-of-order CPUs will hide most of the checking
code, if it’s simple enough.</p>

<p>This concern (access to metadata in few instructions) combined with
our goal of avoiding in-band metadata lead to a
<a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs#L6-L19">simple layout for each chunk’s data and metadata</a>.</p>

<pre><code>.-------.------.-------|---------------.-------.
| guard | meta | guard | data ... data | guard |
'-------'------'-------|---------------'-------'
  2 MB    2 MB   2 MB  |      1 GB        2 MB
                       v
               Aligned to 1 GB
</code></pre>

<p>A chunk’s data is always a 1 GB address range, aligned to 1 GB: the
underlying mapper doesn’t have to immediately back that with memory,
but it certainly can, e.g., in order to use gigantic pages.  The chunk
is preceded and followed by 2 MB guard pages.  The metadata for the
chunk’s data lives in a 2 MB range, just before the preceding guard
page (i.e., 4 MB to 2 MB before the beginning of the aligned 1 GB
range).  Finally, the 2 MB metadata range is itself preceded by a 2MB
guard page.</p>

<p>Each chunk is statically divided in 65536 spans of 16 KB each.  We can
thus map a span to its slot in the metadata block with a shifts,
masks, and some address arithmetic.  <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs">Mill</a>s
don’t have to hand out individual 16 KB spans at a time, they simply
have to work in multiples of 16 KB, and never split a span in two.</p>

<h2 id="why-we-wrote-slitter-in-rust-and-c">Why we wrote Slitter in Rust and C</h2>

<p>We call Slitter from C, but wrote it in Rust, despite the
more painful build<sup id="fnref:uber-crate" role="doc-noteref"><a href="#fn:uber-crate" class="footnote" rel="footnote">10</a></sup> process: that pain isn’t going
anywhere, since we expect our backend to be in a mix of C, C++, and
Rust for a long time.  We also sprinkled in some C when the
alternative would have been to pull in a crate just to make a couple
syscalls, or to enable unstable Rust features: we’re not
“rewrite-it-in-Rust” absolutists, and merely wish to use Rust for its
strengths (control over data layout, support for domain-specific
invariants, large ecosystem for less performance-sensitive logic, ability to
lie to the compiler where necessary, …), while avoiding its
weaknesses (interacting with Linux interfaces defined by C headers, or
fine-tuning code generation).</p>

<p>The majority of allocations only interact with the thread-local
magazines.  That’s why we <a href="https://github.com/backtrace-labs/slitter/blob/main/c/cache.c">wrote that code in C</a>:
stable Rust doesn’t (yet) let us access <a href="https://doc.rust-lang.org/std/intrinsics/fn.likely.html">likely/unlikely annotations</a>,
nor <a href="https://www.akkadia.org/drepper/tls.pdf#page=35">fast “initial-exec”</a> <a href="https://github.com/rust-lang/rust/issues/29594">thread-local storage</a>.
Of course, allocation and deallocation are the main entry points into
a memory allocation library, so this creates a bit of friction with
Rust’s linking process.<sup id="fnref:bad-linker" role="doc-noteref"><a href="#fn:bad-linker" class="footnote" rel="footnote">11</a></sup></p>

<p>We also had to implement our <a href="https://github.com/backtrace-labs/slitter/blob/main/c/stack.c">lock-free multi-popper Treiber stack</a>
in C: x86-64 doesn’t have anything like LL/SC, so we instead pair
the top-of-stack pointer with a generation counter… and 
<a href="https://github.com/rust-lang/rust/issues/32976#issuecomment-641360955">Rust hasn’t stabilised 128-bit atomics</a> yet.</p>

<p>We chose to use atomics in C instead of a simple lock in Rust because
the lock-free stack (and the atomic bump pointer, which Rust handles
fine) are important for our use case: when we rehydrate cold metadata
at startup, we do so from multiple I/O-bound threads, and we have
observed hiccups due to lock contention in malloc.  At some point,
lock acquisitions are rare enough that contention isn’t an issue;
that’s why we’re comfortable with locks when refilling bump allocation
regions.</p>

<h2 id="come-waste-performance-on-safety">Come waste performance on safety!</h2>

<p>A recurring theme in the design of <a href="https://github.com/backtrace-labs/slitter">Slitter</a>
is that we find ways to make the core (de)allocation logic slightly
faster, and immediately spend that efficiency on safety, debuggability
or, eventually, observability.  For a lot of code, performance is a
constraint to satisfy, not a goal to maximise; once we’re close to
good enough, it makes sense to trade performance
away.<sup id="fnref:even-works-for-perf" role="doc-noteref"><a href="#fn:even-works-for-perf" class="footnote" rel="footnote">12</a></sup> I also believe that there are
<a href="https://research.google/pubs/pub50370/">lower hanging fruits in memory placement</a>
than shaving a few nanoseconds from the allocation path.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a> also focuses on
instrumentation and debugging features that are always active, even in
production, instead of leaving that to development tools, or to logic
that must be explicitly enabled.  In a SaaS world, development and
debugging is never done.  Opt-in tools are definitely useful, but
always-on features are much more likely to help developers catch
the rarely occurring bugs on which they tend to spend an inordinate
amount of investigation effort (and if a debugging feature can be
safely enabled in production at a large scale, why not leave it
enabled forever?).</p>

<p>If that sounds like an interesting philosophy for a slab allocator,
<a href="https://github.com/backtrace-labs/slitter">come hack on Slitter</a>!
Admittedly, the value of Slitter isn’t as clear for pure Rust hackers
as it is for those of us who blend C and Rust, but per-class allocation
statistics and placement decisions should be useful, even in safe
Rust, especially for larger programs with long runtimes.</p>

<p>Our <a href="https://github.com/backtrace-labs/slitter">MIT-licensed code is on github</a>,
there are <a href="https://github.com/backtrace-labs/slitter/issues">plenty of small improvements to work on</a>,
and, while we still have to re-review the documentation, it has decent
test coverage, and we try to write straightforward code.</p>

<p><small>This post was much improved by feedback from my beta readers, Barkley, David,
Eloise, Mark, Per, Phil, Ruchir, and Samy.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:blast-radius" role="doc-endnote">
      <p>In my experience, their unlimited blast radius is what makes memory management bugs so frustrating to track down.  The design goals of generic memory allocators (e.g., recycling memory quickly) and some implementation strategies (e.g., <a href="http://phrack.org/issues/57/9.html#article">in-band metadata</a>) make it easy for bugs in one module to show up as broken invariants in a completely unrelated one that happened to share allocation addresses with the former.  <a href="http://phrack.org/issues/57/8.html#article">Adversarial thinkers</a> will even exploit the absence of isolation to <a href="https://www.openwall.com/articles/JPEG-COM-Marker-Vulnerability">amplify small programming errors into arbitrary code execution</a>.  Of course, one should simply not write bugs, but when they do happen, it’s nice to know that the broken code most likely hit itself and its neighbours in the callgraph, and not unrelated code that also uses the same memory allocator (something Windows got right with private heaps). <a href="#fnref:blast-radius" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:when-does-it-flush" role="doc-endnote">
      <p>Linux does not have anything like the <a href="https://www.freebsd.org/cgi/man.cgi?query=mmap&amp;sektion=2&amp;format=html#:~:text=a%20core%20file.-,MAP_NOSYNC,-Causes%20data%20dirtied">BSD’s <code>MAP_NOSYNC</code> mmap flag</a>.  This has historically created problems for <a href="https://lkml.org/lkml/2013/9/7/135">heavy mmap users like LMDB</a>.  Empirically, Linux’s flushing behaviour is much more reasonable these days, especially when dirty pages are a small fraction of physical RAM, as it is for us: in a well configured installation of our backend server, most of the RAM goes to clean file mappings, so only the <a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt"><code>dirty_expire_centisec</code> timer</a> triggers write-outs, and we haven’t been growing the file-backed heap fast enough for the time-based flusher to thrash too much. <a href="#fnref:when-does-it-flush" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ub" role="doc-endnote">
      <p>There are obvious parallels with undefined behaviour in C and C++… <a href="#fnref:ub" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-as-configurable" role="doc-endnote">
      <p>umem also takes a performance hit in order to let object classes define callbacks for object initialisation, recycling, and destruction.  It makes sense to let the allocator do some pre-allocation work: if you’re going to incur a cache miss for the first write to an allocation, it’s preferable to do so before you immediately want that newly allocated object (yes, profiles will show more cycles in the allocators, but you’re just shifting work around, hopefully farther from the critical path). Slitter only supports the bare minimum: objects are either always zero-initialised, or initially zero-filled and later left untouched.  That covers the most common cases, without incurring too many branch mispredictions. <a href="#fnref:not-as-configurable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-we-dont-depend-on-it-too-much" role="doc-endnote">
      <p>One could be tempted to really rely on it not just for isolation and resilience, but during normal operations.  That sounds like a bad idea (we certainly haven’t taken that leap), at least until <a href="https://github.com/backtrace-labs/slitter/issues/11">Slitter works with Valgrind/ASan/LSan</a>: it’s easier to debug easily reproducible issues when one can just slot in calls to regular malloc/calloc/free with a dedicated heap debugger. <a href="#fnref:but-we-dont-depend-on-it-too-much" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:legacy-gcc" role="doc-endnote">
      <p>It would be easy to blame the complexity of lock-free code, but the initial version, with C11 atomics, was correct.  Unfortunately, gcc backs C11 atomic <code>uint128_t</code>s with locks, so we had to switch to the legacy interface, and that’s when the errors crept in. <a href="#fnref:legacy-gcc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:jump" role="doc-endnote">
      <p>There isn’t much the allocator can do if an application writes to a wild address megabytes away from the base object.  Thankfully, buffer overflows tend to proceed linearly from the actual end of the undersized object. <a href="#fnref:jump" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:type-stable" role="doc-endnote">
      <p>In fact, Slitter actively worsens external fragmentation to guarantee type-stable allocations. We think it’s reasonable to sacrifice heap footprint in order to control the blast radius of use-after-frees and double-frees. <a href="#fnref:type-stable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:also-good-for-perf" role="doc-endnote">
      <p>That’s why we’re interested in allocation class tags, but they can also help application and malloc performance.  Some malloc developers are looking into tags for placement (should the allocation be backed by memory local to the NUMA node, with huge pages, …?) or lifetime (is the allocation immortal, short-lived, or tied to a request?) hints. <a href="#fnref:also-good-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:uber-crate" role="doc-endnote">
      <p>We re-export our dependencies from an uber-crate, and let our outer <a href="https://mesonbuild.com/">meson</a> build invoke <code>cargo</code> to generate a static library for that facade uber-crate. <a href="#fnref:uber-crate" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bad-linker" role="doc-endnote">
      <p><a href="https://github.com/rust-lang/rfcs/issues/2771">Rust automatically hides foreign symbols when linking <code>cdylib</code>s</a>.  We worked around that with static linking, but statically linked rust libraries are mutually incompatible, hence the uber-crate. <a href="#fnref:bad-linker" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:even-works-for-perf" role="doc-endnote">
      <p>And not just for safety or productivity features!  I find it often makes sense to give up on small performance wins (e.g., aggressive autovectorisation or link-time optimisation) when they would make future performance investigations harder.  The latter are higher risk, and only potential benefits, but their upside (order of magnitude improvements) dwarfs guaranteed small wins that freeze the code in time. <a href="#fnref:even-works-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Entomological solutions]]></title>
    <link href="https://www.pvk.ca/Blog/2021/06/07/entomological-solutions/"/>
    <updated>2021-06-07T00:36:03-04:00</updated>
    <id>https://www.pvk.ca/Blog/2021/06/07/entomological-solutions</id>
    <content type="html"><![CDATA[<p>Non-blocking algorithms have a reputation for complexity.  However, if
you ask people who work on systems where strong progress guarantees
are mandatory (e.g., hard real-time systems), they’ll often disagree.</p>

<p>I believe the difference is rooted in the way systems which <em>must</em>
bound their pauses will sacrifice nice-to-haves to more cleanly
satisfy that hard requirement.</p>

<p>Researchers and library writers instead tend to aim for maximal
guarantees or functionality while assuming (sacrificing) as little as
possible.  Someone who’s sprinkling lock-free algorithms in a large
codebase will similarly want to rely on maximally general algorithms,
in order to avoid increasing the scope of their work: if
tail latency and lock-freedom were high enough priorities to justify
wide-ranging changes, the program would probably have been designed
that way from the start.</p>

<p>It makes sense to explore general solutions, and academia is
definitely a good place to do so.  It was fruitful for mathematicians
to ask questions about complex numbers, then move on to fields, rings,
groups, etc., like sadistic kids probing what a bug can still do as
they tear its legs off one by one.  Quicksort and mergesort are
probably the strongest exemplars of that sort of research in computer
science: we don’t even ask what data is made of before assuming a
comparison sort is probably a good idea.</p>

<p>It is however more typical to trade something in return for
generality.  When there’s no impact on performance or resource
usage, code complexity usually takes a hit.</p>

<p>When solving a core problem like lock-freedom in a parallel
realtime system, we instead ask how much more we can assume, what else
we can give up, in order to obtain simpler, more robust solutions.
We don’t want generality, we’re not crippling bugs; we want
specificity, we’re dosing eggs with <a href="https://en.wikipedia.org/wiki/Antennapedia">Hox to get more legs</a>.</p>

<p>The first time someone used to academic non-blocking algorithms hears
about the resulting maximally specialised solutions, they’ll sometimes
complain about “cheating.” Of course, it’s never cheating when a
requirement actually is met; the surprise merely shows that the rules
typically used to evaluate academic solutions are but approximations
of reality, and can be broken…  and practitioners faced with
specific problems are ideally placed to determine what rules
they can flout.</p>

<h2 id="hoarding-is-caring">Hoarding is caring?</h2>

<p>My favourite example of such cheating is type-stable memory.  The
literature on Safe memory reclamation (SMR) conflates<sup id="fnref:or-does-it" role="doc-noteref"><a href="#fn:or-does-it" class="footnote" rel="footnote">1</a></sup> two
problems that are addressed by SMR algorithms: reclamation races, and
the ABA problem.</p>

<p>A reclamation race is what happens when a thread dereferences a
pointer to the heap, but the pointee has already been deallocated;
even when the racy accesses are loads, they can result in a
segmentation fault (and a crash).</p>

<p>The ABA problem is what happens when a descriptor (e.g., a pointer) is
reused to refer to something else, but some code is unaware of the
swap. For example, a thread could load a global pointer to a logically
read-only object, read data off that pointer, sleep for a while, and
observe that the global pointer has the same value. That does not mean
nothing has changed: while the thread was sleeping, the pointee could
have been freed, and then recycled to satisfy a fresh allocation.</p>

<p>Classic SMR algorithms like <a href="https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-579.pdf">epoch reclamation</a>
and <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.395.378&amp;rep=rep1&amp;type=pdf">hazard pointers</a>
solve both problems at once; in fact, addressing
reclamation races is their core contribution (it’s certainly the
challenging part), and ABA protection is simply a nice corollary.</p>

<p>However, programs <em>can choose</em> not to crash on benign use-after-free:
reading from freed objects only triggers crashes when memory is mapped
and unmapped dynamically, and that’s usually not an option for hard
realtime systems.  On smaller embedded targets, there’s a fixed
physical memory budget; either the program fits, or the program is
broken.  Larger shared-memory parallel programs often can’t afford the IPIs
and other hiccups associated with releasing memory to the operating
system.  Either way, half the problem solved by SMR doesn’t even exist
for them.</p>

<p>The other half, ABA, is still an issue…  but that subproblem is
easier to solve.  For example, we can tag data with sequence counters.<sup id="fnref:reuse-dont-recycle" role="doc-noteref"><a href="#fn:reuse-dont-recycle" class="footnote" rel="footnote">2</a></sup></p>

<h2 id="lock-free-stacks-and-aba">Lock-free stacks and ABA</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Treiber_stack">lock-free multiple producers / single consumer linked stack</a> might be
the simplest lock-free data structure.<sup id="fnref:but-maybe-not-scalable" role="doc-noteref"><a href="#fn:but-maybe-not-scalable" class="footnote" rel="footnote">3</a></sup></p>

<p>Pushing a new record to such a stack is easy:<sup id="fnref:not-in-c" role="doc-noteref"><a href="#fn:not-in-c" class="footnote" rel="footnote">4</a></sup> load the current top
of stack pointer, publish that in our <a href="https://en.wikipedia.org/wiki/CAR_and_CDR">new record’s “next” (CDR) field</a>,
and attempt to replace the top of stack with a pointer to our new
record with a compare-and-swap (CAS).</p>

<p>How do we consume from such a stack?</p>

<p>The simplest way is to use a fetch-and-set (atomic exchange) to
simultaneously clear the stack (set the top-of-stack pointer to the
“empty stack” sentinel, e.g., <code>NULL</code>) and read the previous
top-of-stack.  Any number of consumers can concurrently execute such a
batch pop, although only one will grab everything.</p>

<p>Alternatively, if there’s only one consumer at a time, it can pop with
a compare-and-swap.  The consumer must load the current top-of-stack
pointer.  If the stack is empty, there’s nothing to pop.  Otherwise,
it can read the top record’s “next” field, and attempt to CAS out the
top-of-stack pointer from the one it just read to the “next” record.</p>

<p>The tricky step here is the one where the consumer reads the “next”
field in the current top-of-stack record: that step would be subject
to reclamation races, except that there’s only one consumer, so
we know no one else concurrently popped that record and freed it.</p>

<p>What can we do to support multiple consumers? Can we
simply make sure that stack records are always safe to read, e.g., by
freeing them to an object pool? Unfortunately, while
use-after-free is benign for producers, it is not for
consumers.</p>

<p>The key problem is that a consumer can observe that the top of stack
points to record A, and that record A’s “next” field points to B, and
then get stuck or sleep for a while.  During that time, another thread
pops A and B, frees both, pushes C, and then pushes A’, a new record
that happens to have the same address as A.  Finally, the initial
consumer will compare the top-of-stack pointers with A (which also
matches A’), and swap that for B, resurrecting a record that has
already been consumed and freed.</p>

<p>Full-blown SMR would fix all that.  However, if we can instead assume
read-after-free do not crash (e.g., we use a type-stable allocator or
an explicit object pool for records), we simply have to reliably
detect when a record has returned to the top of the stack.<sup id="fnref:ll-sc" role="doc-noteref"><a href="#fn:ll-sc" class="footnote" rel="footnote">5</a></sup></p>

<p>We can do that by tagging the top-of-stack pointer with a sequence
counter, and update both with a double-wide compare-and-swap: instead
of CASing the top-of-stack pointer, we want to CAS that pointer and
its monotonically increasing counter.  Every successful CAS of the
pointer will also increment the counter by one, so the sequence counter
will differ when a record is popped and pushed back on the stack.</p>

<p>There is still a risk of ABA: the counter can wrap around.  That’s not
a practical concern with 64-bit counters, and there are reasonable
arguments that narrower counters are safe because no consumer will
stay “stuck” for minutes or hours.<sup id="fnref:bitpacking" role="doc-noteref"><a href="#fn:bitpacking" class="footnote" rel="footnote">6</a></sup></p>

<h2 id="single-compare-multiple-swaps">Single-compare multiple-swaps</h2>

<p>Sometimes, the application can naturally guarantee that CASed fields
are ABA-free.</p>

<p>For example, a hierarchical bump allocator may carve out global
type-specific arenas from a shared chunk of address space, and satisfy
allocations for each object type from the type’s current arena.
Within an arena, allocations are reserved with atomic increments.
Similarly, we carve out each arena from the shared chunk of address
space with a (larger) atomic increment.
Neither bump pointer ever decreases: once a region of address space
has been converted to an arena, it stays that way, and once an object
has been allocated from an arena, it also remains allocated (although
it might enter a freelist).  Arenas are also never recycled: once
exhausted, they stay exhausted.</p>

<p>When an allocation type has exhausted its current arena (the arena’s
bump pointer is exhausted), we want to atomically grab a new arena
from the shared chunk of address space, and replace the type’s arena
pointer with the newly created arena.</p>

<p>A lock-free algorithm for such a transaction looks like it would have
to build on top of multi-word compare-and-swap (MCAS), a hard operation that can
be <a href="https://www.cl.cam.ac.uk/research/srg/netos/papers/2002-casn.pdf">implemented in a wait-free manner, but with complex algorithms</a>.</p>

<p>However, we know that the compare-and-swapped state evolves
monotonically: once an arena has been carved out from the shared
chunk, it will never be returned as a fresh arena again.  In other
words, there is no ABA, and a compare-and-swap on an arena pointer
will never succeed spuriously.</p>

<p>Monotonicity also means that we can acquire a consistent snapshot of both
the type’s arena pointer and the chunk’s allocation state by reading
everything twice.  Values are never repeated, so any write
that executes concurrently with our snapshot loop will be detected: the
first and second reads of the updated data will differ.</p>

<p>We also know that the only way a type’s arena pointer can be replaced
is by allocating a new one from the shared chunk.  If we took a
consistent snapshot of the type’s arena pointer and of the shared
chunk’s allocation state, and the allocation state hasn’t changed
since, the arena pointer must also be unchanged (there’s a
hierarchy).</p>

<p>We can combine all these properties to atomically replace a type’s
arena pointer with a new one obtained from the shared chunk, using a
much simpler core operation, a single-compare multiple-swap (SCMS). We
want to execute a series of CASes (one to allocate an arena
in the chunk, a few to initialise the arena, and one to publish
the new arena), but we can also assume that once the first
updated location matches the CAS’s expected value, all other ones will
as well.  In short, only the first CAS may fail.</p>

<p>That’s the key simplifier compared to full-blown multi-word
compare-and-swap algorithms: they have to incrementally acquire update
locations, any of which might turn the operation into a failure.</p>

<p>We can instead encode all the CASes in a transaction descriptor, CAS
that descriptor in the first update location, and know that the
multi-swaps will all succeed iff that CAS is successful.</p>

<p>If the first CAS is successful, we also know that it’s safe to execute
the remaining CASes, and finally replace the descriptor with its
final value with one last CAS.  We don’t even have to publish the
descriptor to all updated locations, because concurrent allocations
will notice the current arena has been exhausted, and try to get a new
one from the shared chunk… at which point they will notice the
transaction descriptor.</p>

<p>All the CASes after the first one are safe to execute arbitrarily
often thanks to monotonicity.  We already know that any descriptor that has
been published with the initial CAS will not fail, which means the only
potential issue is spuriously successful CASes… but our mutable
fields never repeat a value, so that can’t happen.</p>

<p>The application’s guarantee of ABA-safety ends up really simplifying
this single-compare multiple-swap algorithm (SCMS), compared to a
multi-word compare-and-swap (MCAS).  In a typical MCAS implementation, helpers
must abort when they detect that they’re helping a MCAS operation that
has already failed or already been completed.  Our single-compare
assumption (once the first CAS succeeds, the operation succeeds) takes
care of the first case: helpers never see failed operations.  Lack
of ABA means helpers don’t have to worry about their CASes succeeding
after the SCMS operation has already been completed: they will always fail.</p>

<p>Finally, we don’t even need any form of SMR on the transaction
descriptor: <a href="https://arxiv.org/pdf/1708.01797.pdf#page=4">a sequence counter in the descriptor and a copy of that counter in a tag next to pointers to that descriptor</a>
suffice to disambiguate incarnations of the same physical descriptor.</p>

<p>Specialising to the allocator’s monotonic state let us use
single-compare multiple-swap, a simplification of full multi-word
compare-and-swap, and further specialising that primitive for
monotonic state let us get away with nearly half as many CASes (k + 1
for k locations) as the state of the art for MCAS (2k + 1 for k locations).</p>

<h2 id="a-plea-for-integration">A plea for integration</h2>

<p>There is a common thread between never unmapping allocated addresses,
sequence tags, type-stable memory, and the allocator’s single-compare
multiple-swap: monotonicity.</p>

<p>The lock-free stack shows how easy it is to conjure up artificial
monotonicity.  However, when we integrate algorithms more tightly
with the program and assume the program’s state is naturally
monotonic, we’ll often unlock simpler and more efficient solutions.  I
also find there’s something of a virtuous cycle: it’s easier for a
module to guarantee monotonicity to its components when it itself only
has to handles monotonic state, like a sort of end-to-end monotonicity
principle.</p>

<p>Unfortunately, it’s not clear how much latent monotonicity there is
in real programs.  I suppose that makes it hard to publish algorithms
that assumes its presence.  I think it nevertheless makes sense to
explore such stronger assumptions, in order to help practitioners estimate
what we could gain in exchange for small sacrifices.</p>

<p>Asymmetric synchronisation is widely used these days, but I imagine it
was once unclear how much practical interest there might be in that
niche; better understood benefits lead to increased adoption.  I hope
the same can happen for algorithms that assume monotonic state.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:or-does-it" role="doc-endnote">
      <p><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.5131&amp;rep=rep1&amp;type=pdf">Maged Michael’s original Safe Memory Reclamation paper</a> doesn’t: allowing arbitrary memory management is the paper’s main claim.  I think there’s a bit of a first mover’s advantage, and researchers are incentivised to play within the sandbox defined by Michael.  For example, <a href="https://arxiv.org/abs/1708.01797">Arbel-Raviv and Brown in “Reuse, don’t Recycle”</a> practically hide the implementation of their proposal on page 17 (Section 5), perhaps because a straightforward sequence counter scheme is too simple for publication nowadays. <a href="#fnref:or-does-it" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:reuse-dont-recycle" role="doc-endnote">
      <p>See <a href="https://arxiv.org/abs/1708.01797">Reuse, don’t Recycle</a> for a flexible take. <a href="#fnref:reuse-dont-recycle" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-maybe-not-scalable" role="doc-endnote">
      <p>A stack fundamentally focuses contention towards the top-of-stack pointer, so lock-free definitely doesn’t imply scalable. It’s still a good building block. <a href="#fnref:but-maybe-not-scalable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-in-c" role="doc-endnote">
      <p>In assembly language, anyway. Language memory models make this surprisingly hard. For example, any ABA in the push sequence is benign (we still have the correct bit pattern in the “next” field), but C and C++’s pointer provenance rules say that accessing a live object through a pointer to a freed object that happens to alias the new object is undefined behaviour. <a href="#fnref:not-in-c" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ll-sc" role="doc-endnote">
      <p>Load Linked / Store Conditional solves <em>this specific problem</em>, but that doesn’t mean LL/SC as found on real computers is necessarily a better primitive than compare-and-swap. <a href="#fnref:ll-sc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bitpacking" role="doc-endnote">
      <p>Which is nice, because it means we can pack data and sequence counters in 64-bit words, and use the more widely available single-word compare-and-swap. <a href="#fnref:bitpacking" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
