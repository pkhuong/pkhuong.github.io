
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Paul Khuong: some Lisp</title>
  <meta name="author" content="Paul Khuong">
  <meta name="description" content="Paul Khuong's personal blog. Some Lisp, some optimisation, mathematical or computer.">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://www.pvk.ca/posts/31/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Paul Khuong: some Lisp" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Poller+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Germania+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fontdiner+Swanky&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Lato&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Cardo&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Sorts+Mill+Goudy&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=EB+Garamond&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Della+Respira&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=UnifrakturMaguntia&subset=all&display=fallback" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Arimo|EB+Garamond|PT+Sans+Caption&subset=latin,cyrillic&display=fallback' rel='stylesheet' type='text/css'>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
    Macros: {
     sp: "^",
     sb: "_"
    }
  }});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<meta name="twitter:dnt" content="on">

</head>

<body >
  <header role="banner"><hgroup>
  <h1><a href="/">Paul Khuong: some Lisp</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/Blog/archives">Archives</a></li>
  <li><a href="/atom.xml" title="subscribe via RSS">RSS</a></li>
</ul>

<br>

      
        <form action="https://google.com/search" method="get">
          <fieldset role="search">
            <input type="hidden" name="q" value="site:https://www.pvk.ca" />
      
      
            <input class="search" type="text" name="q" results="0" placeholder="Search" aria-label="Search"/>
          </fieldset>
        </form>
  
</nav>
  <div id="main">
    <div id="content">
      
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title" style="font-family: "><a href="/Blog/2013/12/19/so-you-want-to-write-an-lp-solver/">So you want to write an LP solver (a.k.a. My Little LP: Cholesky Is Magic)</a></h1>
    
    
      <p class="meta">
        





Dec
  
19th, 
2013




        
         | <a href="/Blog/2013/12/19/so-you-want-to-write-an-lp-solver/#disqus_thread"
              data-disqus-identifier="https://www.pvk.ca/Blog/2013/12/19/so-you-want-to-write-an-lp-solver/"
	      >Comments</a>
        
        
      </p>
    
  </header>


  <div class="entry-content" style="font-family: ; font-size: "><p><em>In which a <a href="http://www.evanmiller.org/mathematical-hacker.html">Lisp hacker exhibits his ignorance of applied mathematics</a> ;)</em></p>

<p>Linear programming (LP) is awesome.  I based my PhD work on solving
LPs with millions of variables and constraints… instead of integer
programs a couple orders of magnitude smaller.  However, writing LP
solvers is an art that should probably be left to experts, or to those
willing to dedicate a couple years to becoming one.</p>

<p>That being said, if one must write their own, an interior point method
(IPM) is probably the best approach.  This post walks through the
development of a primal affine scaling method; it doesn’t guarantee
state-of-the-art practical or theoretical efficiency (it’s not even
polynomial time), but is simple and easy to understand.</p>

<p>I think interior point methods suffer from an undeserved bad
reputation: they only <em>seem</em> complicated compared to the nice
combinatorial Simplex method.  That would explain why I regularly
(once or twice a year) see hackers implement a (classic!) simplex
method, but rarely IPMs.</p>

<p>The problem is that simplex only works because world-class
implementations are marvels of engineering: it’s almost like their
performance is in spite of the algorithm, only thanks to
coder-centuries of effort.  Worse, textbooks usually present the
classic simplex method (the dark force behind simplex tableaux) so
that’s what unsuspecting hackers implement.  Pretty much no one uses
that method: it’s slow and doesn’t really exploit sparsity (the fact
that each constraint tends to only involve a few variables).  Avoiding
the trap of the classic simplex is only the first step.  Factorising
the basis — and updating the factors — is essential for efficient
revised simplex methods, and some clever tricks really help in
practice (packages like
<a href="http://web.stanford.edu/group/SOL/software/lusol/">LUSOL</a> mostly
take care of that); pivot selection (pricing) is fiddly but has a huge
impact on the number of iterations — never mind degeneracy, which
complicates everything; and all that must be balanced with minimising
work per iteration (which explains the edge of dual simplex: there’s
no primal analogue to <s>devex</s> normalised dual steepest edge
pricing).  There’s also presolving and crash basis construction, which
can simplify a problem so much that it’s solved without any simplex
pivot.  Despite all this sophistication, only 
<a href="http://www.cs.yale.edu/homes/spielman/Research/SimplexStoc.pdf">recent theoretical advances</a>
protect against instances going exponential.</p>

<p><small>Sidenote: Vasek Chvatal’s
<a href="http://books.google.ca/books/about/Linear_Programming.html?id=DN20_tW_BV0C&amp;redir_esc=y">Linear Programming</a>
takes a view that’s closer to the revised simplex algorithm and seems
more enlightening to me.  Also, it’s been a while since I read about pricing
in the simplex, but the dual simplex definitely has a computational edge for
obscure reasons; <a href="http://d-nb.info/978580478/34">this dissertation</a> might have 
interesting details.</small></p>

<p>There’s also the learning angle.  My perspective is biased (I’ve been
swimming in this for 5+ years), but I’d say that implementing IPMs
teaches a lot more optimisation theory than implementing simplex
methods.  The latter are mostly engineering hacks, and relying on
tableaux or bases to understand duality is a liability for large scale
optimisation.</p>

<p>I hope to show that IPMs are the better choice, with respect to both the
performance of naïve implementations and the insights gained by coding
them, by deriving a decent implementation from an intuitive starting
point.</p>

<h1 id="affine-scaling-in-30-minutes">Affine scaling in 30 minutes</h1>

<p>I’ll solve linear problems of the form
<script type="math/tex">\min\sb{x} cx</script>
subject to
<script type="math/tex">Ax = b</script>
<script type="math/tex">l \leq x \leq u.</script></p>

<p>I will assume that we already have a feasible point that is strictly
inside the box \([l, u]\) and that <em>A</em> has full row rank.  Full rank
doesn’t always pan out in practice, but that’s a topic for another
day.</p>

<p>If it weren’t for the box constraint on <em>x</em>, we would be minimising a
smooth function subject to affine equalities.  We could then easily
find a feasible descent direction by projecting \(-c\) (the opposite
of the gradient) in the nullspace of <em>A</em>, i.e., by solving the linear
equality-constrained least squares
<script type="math/tex">\min\sb{d} \|(-c)-d\|\sb{2}</script>
subject to
<script type="math/tex">Ad = 0,</script>
e.g. with LAPACK’s <a href="http://www.netlib.org/lapack/lug/node28.html">xGGLSE</a>.</p>

<p>The problem with this direction is that it doesn’t take into account
the box constraints.  Once some component of <em>x</em> is at its bound, we
can’t move past that wall, so we should try to avoid getting too close
to any bound.</p>

<p>Interior point methods add a logarithmic penalty on the distance
between <em>x</em> and its bounds; (the gradient of) the objective function
then reflects how close each component of <em>x</em> is to its lower or upper
bound.  As long as <em>x</em> isn’t directly on the border of the box, we’ll
be able to make a non-trivial step forward without getting too close
to that border.</p>

<p>There’s a similar interpretation for simpler primal affine scaling
methods we’ll implement here,
but I prefer the original explanation.  These methods rescale the
space in which we solve for <em>d</em> so that <em>x</em> is always far from its
bounds; rescaling the direction back in the original space means that
we’ll tend to move more quickly along coordinates that are far from
their bounds, and less for those close to either bound.  As long as
we’re strictly inside the box, the transformation is meaningful and
invertible.</p>

<p><small>It’s a primal method because we only manipulate primal
variables; dual methods instead work with the dual variables
associated with the linear constraints. Primal-dual methods
work on both sets of variables at once.</small></p>

<p>This sketch might be useful.  <em>x</em> is close to the left-hand side
bound, and, after projection in the nullspace, <em>d</em>
quickly hits the border.  With scaling, <em>d</em> is more vertical, and
this carries to the unscaled direction.</p>

<p><img class="center" src="/images/2013-12-19-so-you-want-to-write-an-lp-solver/affine-scaling-1.jpg" /></p>

<p>The descent direction may naturally guide us away from a close-by
bound; scaling is then useless.  However, each jump away from that
bound makes the next longer, and such situations resolve themselves
after a couple iterations.</p>

<p>Formally, let
<script type="math/tex">s = \min\\{x-l, u-x\\} > 0</script>
be the slack vector (computed element wise) for the distance from <em>x</em>
to its bounds.  We’ll just scale <em>d</em> and <em>c</em> elementwise by <em>S</em>
(<em>S</em> is the diagonal matrix with the elements of <em>s</em> on its diagonal).
The scaled least square system is
<script type="math/tex">\min\sb{d\sb{s}} \|(-Sc)-d\sb{s}\|</script>
subject to
<script type="math/tex">ASd\sb{s} = 0.</script></p>

<p>Now that we have a descent direction \(d=Sd\sb{s}\), we only
have to decide how far along to go in that direction.  We determine
the limit, <em>r</em>, with a ratio test.  For each coordinate, look at
\(d\sb{i}\): if it’s zero, this coordinate doesn’t affect the limit;
if it’s negative, the limit must be at most \((l-x)\sb{i}/d\sb{i}\);
if it’s positive, the limit is at most \((u-x)\sb{i}/d\sb{i}\).  We
then take the minimum of these ratios.</p>

<p>If the minimum doesn’t exist (i.e., \(r=\infty\)), the problem is
unbounded.</p>

<p>Otherwise, we could go up to \(x+rd\) to improve the solution
while maintaining feasibility.  However, we want to stay strictly
inside the feasible space: once we hit a bound, we’re stuck there.
That’s why we instead take a slightly smaller step, e.g., \(0.9r\).</p>

<p>This drawing shows what’s going on: given <em>d</em>, <em>r</em> takes us exactly to
the edge, so we take a slightly shorter step.  The new solution is
still strictly feasible, but has a better objective value than the previous
one.  In this case, we’re lucky and the new iterate \(x\sp{\prime}\)
is more isotropic than the previous one; usually, we slide closer
to the edge, only less so than without scaling.
<img class="center" src="/images/2013-12-19-so-you-want-to-write-an-lp-solver/affine-scaling-3.jpg" /></p>

<p>We then solve the constrained least squares system again, with a new
value for <em>S</em>.</p>

<h2 id="finding-a-feasible-solution">Finding a feasible solution</h2>

<p>I assumed that <em>x</em> is a strictly feasible (i.e., interior) solution:
it satisfies the equality constraint \(Ax=b\) and is strictly inside
its box \([l, u]\).  That’s not always easy to achieve; in theory,
finding a feasible solution is as hard as optimising a linear program.</p>

<p>I’ll now assume that <em>x</em> is strictly inside its box and repair it
toward feasibility, again with a scaled least squares solution.  This
time, we’re looking for the least norm (the system is underdetermined)
solution of <script type="math/tex">Ad = (b - Ax).</script></p>

<p>We rescale the norm to penalise movements in coordinates that are
already close to their bound by instead solving
<script type="math/tex">ASd\sb{s} = (b - Ax),</script>
for example with
<a href="http://www.netlib.org/lapack/lug/node27.html">xGELSD</a>.</p>

<p>If we move in direction \(Sd\sb{s}\) with step \(r=1\), we’ll
satisfy the equality exactly.  Again, we must also take the box into
account, and perform a ratio test to determine the step size; if
\(0.9r &gt; 1\), we instead set \(r=1/0.9\) and the result is a
strictly feasible solution.</p>

<h2 id="an-initial-implementation">An initial implementation</h2>

<p>I uploaded some hacktastic CL to
<a href="https://github.com/pkhuong/cholesky-is-magic/commits/master">github</a>.
The initial affine scaling method corresponds to
<a href="https://github.com/pkhuong/cholesky-is-magic/tree/507231221a98929e26ab80410d7892f19c6d5bdf">this commit</a>.
The outer loop looks at the residual \(Ax-b\) to determine whether
to run a repair (feasibility) or optimisation iteration.</p>

<p>The code depends on a few libraries for the MPS parser and on matlisp,
and isn’t even ASDF-loadable; I just saved and committed whatever I
defined in the REPL.</p>

<p>This commit depends on a patch to matlisp’s <code>src/lapack.lisp</code> (and to
<code>packages.lisp</code> to export <code>lapack:dgglse</code>):</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
</pre></td><td class="code"><pre><code class=""><span class="line">(def-fortran-routine dgglse :void
</span><span class="line">"
</span><span class="line">   -- LAPACK driver routine (version 3.0) --
</span><span class="line">      Univ. of Tennessee, Univ. of California Berkeley, NAG Ltd.,
</span><span class="line">      Courant Institute, Argonne National Lab, and Rice University
</span><span class="line">      March 31, 1993
</span><span class="line">      
</span><span class="line">PURPOSE
</span><span class="line">      DGGLSE solves the linear equality constrained least squares
</span><span class="line">      (LSE) problem:
</span><span class="line">
</span><span class="line">              minimize || A*x - c ||_2   subject to B*x = d
</span><span class="line">"
</span><span class="line">  (m :integer :input)
</span><span class="line">  (n :integer :input)
</span><span class="line">  (p :integer :input)
</span><span class="line">  (a (* :double-float) :input-output)
</span><span class="line">  (lda :integer :input)
</span><span class="line">  (b (* :double-float) :input-output)
</span><span class="line">  (ldb :integer :input)
</span><span class="line">  (c (* :double-float) :input-output)
</span><span class="line">  (d (* :double-float) :input-output)
</span><span class="line">  (x (* :double-float) :output)
</span><span class="line">  (work (* :double-float) :workspace-output)
</span><span class="line">  (lwork :integer :input)
</span><span class="line">  (info :integer :output))</span></code></pre></td></tr></table></div></figure></notextile></div>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class=""><span class="line">CL-USER&gt; (matlisp:load-blas-&amp;-lapack-libraries)
</span><span class="line">[...]
</span><span class="line">CL-USER&gt; (load "read-mps.lisp")
</span><span class="line">T
</span><span class="line">CL-USER&gt; (load "standard-form.lisp")
</span><span class="line">T
</span><span class="line">CL-USER&gt; (load "affine-scaling.lisp")
</span><span class="line">T</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>MPS is a very old (punchcard-oriented) format to describe linear and
mixed integer programs.  The parser mostly works (the format isn’t
very well specified), and <code>standard-form.lisp</code> converts everything to
our standard form with equality constraints and a box around decision
variables.</p>

<p>I will test the code on a couple LPs from
<a href="http://www.netlib.org/lp/data/readme">netlib</a>, a classic instance set
(I think the tarball in
<a href="http://www.numerical.rl.ac.uk/cute/netlib.html">CUTEr</a> is OK).</p>

<p>AFIRO is a tiny LP (32 variables by 28 constraints, 88 nonzeros).
Good news: the program works on this trivial instance and finds the
optimum (the relative difference with the reference value is 7e-6).
The first four “repair” iterations find a feasible solutions, and 11
more iterations eventually get to the optimum.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
</pre></td><td class="code"><pre><code class=""><span class="line">CL-USER&gt; (defparameter *affine* (make-affine-state 
</span><span class="line">                                 (to-standard-form
</span><span class="line">                                  (read-mps-file "/Users/pkhuong/mps-reader/netlib/afiro"))))
</span><span class="line">*AFFINE*
</span><span class="line">CL-USER&gt; (time (affine-scaling *affine*))
</span><span class="line">   0: Repair:     832.88     ...   567.67       827.27    
</span><span class="line">   1: Repair:     827.27     ...   618.03       794.68    
</span><span class="line">   2: Repair:     794.68     ...   691.33       586.30    
</span><span class="line">   3: Repair:     586.30     ...   552.10      1.59378d-13
</span><span class="line">   4: Optimise:  -20.296     ...   148.53      -90.318    
</span><span class="line">   5: Optimise:  -90.318     ...   1414.9      -246.98    
</span><span class="line">   6: Optimise:  -246.98     ...   11350.      -446.35    
</span><span class="line">   7: Optimise:  -446.35     ...   56.016      -452.73    
</span><span class="line">   8: Optimise:  -452.73     ...   13.263      -461.82    
</span><span class="line">   9: Optimise:  -461.82     ...  0.85664      -463.97    
</span><span class="line">  10: Optimise:  -463.97     ...   4.20034d-2  -464.50    
</span><span class="line">  11: Optimise:  -464.50     ...   5.00704d-3  -464.72    
</span><span class="line">  12: Optimise:  -464.72     ...   1.31850d-4  -464.74    
</span><span class="line">  13: Optimise:  -464.74     ...   1.40724d-5  -464.75    
</span><span class="line">  14: Optimise:  -464.75     ...   9.34845d-7  -464.75    
</span><span class="line">Evaluation took:
</span><span class="line">  0.021 seconds of real time
</span><span class="line">  0.021653 seconds of total run time (0.014447 user, 0.007206 system)
</span><span class="line">  104.76% CPU
</span><span class="line">  34,228,752 processor cycles
</span><span class="line">  1,346,800 bytes consed
</span><span class="line">  
</span><span class="line">-464.7498078036151d0
</span><span class="line">#&lt;MATLISP:REAL-MATRIX  51 x 1
</span><span class="line">    80.000     
</span><span class="line">    25.500     
</span><span class="line">    54.500     
</span><span class="line">    84.800     
</span><span class="line">     :
</span><span class="line">    80.424     &gt;
</span><span class="line">6.662821645122319d-13</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>I also tested on ADLITTLE, another tiny instance (97x57, 465 nz): 48
iterations, 0.37 seconds total.</p>

<p>AGG is more interesting, at 163x489 and 2541 nz; that one took 156
iterations and 80 seconds (130% CPU, thanks to Apple’s parallel BLAS).
Finally, FIT1D is challenging, at 1026x25 and 14430 nz; that one took
100 seconds and the final objective value was off by .5%.</p>

<p>All these instances are solved in a fraction of a second by
state-of-the-art solvers.  The next steps will get us closer to a
decent implementation.</p>

<h1 id="first-simplify-the-linear-algebra">First, simplify the linear algebra</h1>

<p>The initial implementation resorted to DGGLSE for all least squares
solves.  That function is much too general.</p>

<p>I
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/ecb6b58362f5658eaa4184292022f4a6691384dd">changed the repair phase’s least squares to a GELSY</a>
(GELSD isn’t nicely wrapped in matlisp yet).</p>

<p>We can do the same for the optimisation phase’s constrained least squares.
Some doodling around with Lagrange multipliers shows that the solution
of
<script type="math/tex">\min\sb{x} \|x-c\|\sb{2}\sp{2}</script>
subject to
<script type="math/tex">Ax = 0</script>
is
<script type="math/tex">x = A\sp{t}(AA\sp{t})\sp{-1}Ac - c.</script></p>

<p>This is a series of matrix-vector multiplications and a single linear
solve,
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/d17d65e96686bc6334b5c524d54b1359ec1e1513">which I perform with GELSY for now</a>: 
we need the numerical stability because, when we’re close to optimality, the system is really badly conditioned and
regular linear solves just crash.</p>

<p>Both linear algebra microoptimisations are in
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/df036c9e37420df532739a66bc53eee82b209f26">this commit</a>.</p>

<p>ADLITTLE becomes about five times faster, and FIT1D now runs in 2.9
seconds (instead of 100), but AGG eventually fails to make progress:
we still have problems with numerical stability.</p>

<h1 id="now-the-magic-sauce">Now, the magic sauce</h1>

<p>Our numerical issues aren’t surprising: we work with a scaled matrix
\(AS\), where the diagonal of \(S\) reflects how close \(x\) is
to its bounds.  As we get closer to an optimal solution, some of
\(x\) will converge to the bounds, and some to a value in-between:
the limit is a basic solution.  We then take this badly conditioned
matrix and multiply it by its own transpose, which squares the
condition number! It’s a wonder that we can get any useful bit from
our linear solves.</p>

<p>There’s a way out: the normal matrix \(AA\sp{t}\) is not only
symmetric, but also positive (semi)definite (the full rank assumption
isn’t always satisfied in practice).  This means we can go for a
Cholesky \(LL\sp{t}\) (or \(LDL\sp{t}\)) factorisation.</p>

<p>We’re lucky: it turns out that Cholesky factorisation reacts to our
badly conditioned matrix in a way that is safe for Newton steps in
IPMs (for reasons I totally don’t understand).  It also works for us.
I guess the intuition is that the bad conditioning stems from our
scaling term.  When we go back in the unscaled space, rounding errors
have accumulated exactly in the components that are reduced into
nothingness.</p>

<p>I used Cholesky factorisation with LAPACK’s
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/c7214f820c933bea5627f75792cdb32cbf72ffb2">DPOTRF and DPOTRS for the optimisation projection</a>,
and
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/4d80ad9f409f27faf74e6ae964ac19f5bcfff189">for repair iterations as well</a>.</p>

<p>LAPACK’s factorisation may fail on semidefinite matrices;
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/f0668c9b4e5e622fc60df9c770acb3a96e3ba4d9">I call out to GELSY when this happens</a>.</p>

<p>To my eyes, this yields the first
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/baf4168e0718ed3ef6a2bd0fb4e58cf210e7976a">decent commit</a>.</p>

<p>The result terminates even closer to optimum, more quickly.  AGG stops
after 168 iterations, in 26 seconds, and everything else is slightly
quicker than before.</p>

<h1 id="icing-on-the-cake">Icing on the cake</h1>

<p>I think the code is finally usable: the key was to reduce everything
to PD (positive definite) solves and Cholesky factorisation.  So far,
we’ve been doing dense linear algebra with BLAS and LAPACK and
benefitted from our platform’s tuned and parallel code.  For small (up
to a hundred or so constraints and variables) or dense instances, this
is a good simple implementation.</p>

<p>We’ll now get a 10-100x speedup on practical instances.  I already
noted, en passant, that linear programs in the wild tend to be very
sparse.  This is certainly true of our four test instances: their
nonzero density is around 1% or less.  Larger instances tend to be
even sparser.</p>

<p>There’s been a lot of work on sparse PD solvers.  Direct sparse
linear solvers is another complex area that I don’t think should be
approached lightly: expensive supercomputers have been solving sparse
PD linear systems for a couple decades, and there’s some really crazy
code around.  I’ve read reports that, with appropriate blocking and
vectorisation, a sparse matrix can be factored to disk at 80% peak
FLOP/s.  If you’re into that, I’m told there are nice introductions,
like <a href="http://www.cise.ufl.edu/research/sparse/CSparse/">Tim Davis’s book</a>,
which covers a didactical yet useful implementation of sparse Cholesky in
2000 LOC.</p>

<p>I decided to go with CHOLMOD from
<a href="http://www.cise.ufl.edu/research/sparse/SuiteSparse/">SuiteSparse</a>, a
mixed GPL/LGPL library.  CHOLMOD implements state of the art methods:
when all its dependencies are available, it exposes parallelisation
and vectorisation opportunities to the BLAS (thanks to permutations
computed, e.g., by METIS) and exploits Intel’s TBB and Nvidia’s CUDA.
It’s also well designed for embedding in other programs; for example,
it won’t <code>abort(3)</code> on error (I’m looking at you, LLVM), and includes
data checking/printing routines that help detect FFI issues.  Its
interface even helps you find memory leaks!  Overall, it’s a really
refreshing experience compared to other academic code, and I wish all
libraries were this well thought out.</p>

<p>I built only the barest version on my Mac and linked it with a
<a href="https://github.com/pkhuong/cholesky-is-magic/blob/f62571572b66f61e069ed994415960e5e2b309bd/wrapper.c">wrapper</a>
to help
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/f62571572b66f61e069ed994415960e5e2b309bd">my bindings</a>.
I had to build the dynamic library with <code>-Wl,-all_load</code> on OS X to
preserve symbols that only appear in archives; <code>-Wl,--whole-archive</code>
should do it for gnu ld.  (I also bound a lot of functions that I
don’t use: I was paying more attention to the TV at the time.)</p>

<p>CHOLMOD includes functions to simultaneously multiply a sparse matrix with
its transpose and factorise the result.
<a href="https://github.com/pkhuong/cholesky-is-magic/blob/de30e45634670e01f39e55a3735637347e251850/sparse-cholesky.lisp#L376">Function <code>solve-dense</code></a>
shows how I use it to solve a dense PD system.  It’s a three-step
process:</p>

<ol>
  <li>Analyse the nonzero pattern of the constraint matrix to determine
an elimination order (trivial for a dense matrix);</li>
  <li>Compute the normal matrix and factor it according to the analysis
stored in the factor struct;</li>
  <li>Solve for that factorisation.</li>
</ol>

<p>This is really stupid: I’m not even dropping zero entries in the dense
matrix.  Yet,
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/0095354735a35f11ccb8aaa6f41b8e6253ab87bd">it suffices</a>
to speed up AGG from 26 to 14 seconds!</p>

<p>It’s clear that we need to preserve the constraint matrix <em>A</em> in a
sparse format.  CHOLMOD has a function to translate from the trivial
triplet representation (one vector of row indices, another of column
indices, and another of values) to the more widely used compressed
representation.  Instances in our standard form already represent the
constraint matrix as a vector of triplets.  We only have to
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/6bd1e48058af1c2621f94aef8c6c13ba38119c74">copy from CL to a CHOLMOD triplet struct</a>
to exploit sparsity in Cholesky solves and in matrix-vector
multiplications.</p>

<p>We now solve AGG in 0.83 seconds and FIT1D in 0.95.  I think we can
expect runtimes of one second or less for instances up to ~200x200.
Better, we can finally hope to solve respectable LPs, like FIT2D
(10500x26, 138018 nz, 2.3 s) or FIT1P (1677x628, 10894 nz, 14 s).</p>

<h1 id="finishing-touches">Finishing touches</h1>

<p>Our normal matrix \(ASS\sp{t}A\sp{t}\) always has the same pattern (nonzero
locations): we only change the scaling diagonals in the middle.  Sparse
solvers separate the analysis and factorisation steps for exactly
such situations.  When we solve a lot of systems with the same pattern, it
makes sense to spend a lot of time on a one-time analysis that we then
reuse at each iteration: fancy analysis routines generate
factors that take up less space and need fewer FLOPs to build.</p>

<p>I do that
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/eabbc91d9a8cf38edd42b1c8ce1549c57afbcb79">here</a>.
Our less tiny instances are almost twice as fast.  We solve FIT1P in 8
seconds now, and even FIT2P (13525x3001, 60784 nz) in 200 seconds.</p>

<p>I then made
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/3a489beaa77dc3f0d95ae4822dd0544819d61754">some microoptimisation to reuse storage</a>.</p>

<p>Finally, I added steps that push the current solution away from
close-by bounds.  These
<a href="https://github.com/pkhuong/cholesky-is-magic/commit/a2c4f065da3cf8549fb3576067d64b54a6383a63">centering steps</a>
help subsequent iterations make longer jumps toward optimality</p>

<p>The last two changes shave a couple more seconds on large instances
and gets closer to optimality on nastier ones.</p>

<h1 id="not-quite-the-end">Not quite the end</h1>

<p>I hope I managed to communicate the intuition behind primal affine
scaling methods, and that the walkthrough helped map that intuition to
a sparse implementation.  I also realise that the code isn’t pretty: I
wrote it during a short marathon and tried to only make incremental
changes.  Still, the algorithm should be more or less usable for small
instances; more than a naïve simplex method, anyway.</p>

<p>That’s particularly true on multicore machines.  Parallelising simplex
methods has been an area of slow research for a couple decades; the
best work I’ve seen so far takes a huge hit by reverting to the
classic algorithm and hopes that parallelisation can compensate for
that initial 10-1000x slowdown.  In contrast, even our sparse method
is already parallel: CHOLMOD automatically vectorises, parallelises,
and offloads work to the GPU.</p>

<p>I’ll try to code a sparse primal-dual affine scaling method from
scratch soon.  Primal-dual methods usually work better than pure
primal (or dual) methods and I find their theory interesting (if
<a href="http://discontinuity.info/~pkhuong/KKT/KKT09.jpg">a bit complicated</a>).</p>

<p>If you liked this post, you might be interested in Stephen Boyd’s
<a href="http://online.stanford.edu/course/convex-optimization-winter-2014">Convex optimisation course</a>.
He’s offering it online this winter, starting January 21st.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/32">&larr; Older</a>
    
    <a href="/Blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/30">Newer &rarr;</a>
    
  </div>
</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Website copyright &copy; 2020 - <a href="mailto:pvk@pvk.ca">Paul Khuong</a> | <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/TheChymera/Koenigspress">Königspress</a></span>
</p>

</footer>
  

<script id="dsq-count-scr" src="//pvk.disqus.com/count.js" async></script>












<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-20468541-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
