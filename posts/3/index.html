
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Paul Khuong: some Lisp</title>
  <meta name="author" content="Paul Khuong">
  <meta name="description" content="Paul Khuong's personal blog. Some Lisp, some optimisation, mathematical or computer.">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://www.pvk.ca/posts/3/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Paul Khuong: some Lisp" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Poller+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Germania+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fontdiner+Swanky&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Lato&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Cardo&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Sorts+Mill+Goudy&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=EB+Garamond&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Della+Respira&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=UnifrakturMaguntia&subset=all&display=fallback" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Arimo|EB+Garamond|PT+Sans+Caption&subset=latin,cyrillic&display=fallback' rel='stylesheet' type='text/css'>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
    Macros: {
     sp: "^",
     sb: "_"
    }
  }});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<meta name="twitter:dnt" content="on">

</head>

<body >
  <header role="banner"><hgroup>
  <h1><a href="/">Paul Khuong: some Lisp</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/Blog/archives">Archives</a></li>
  <li><a href="/atom.xml" title="subscribe via RSS">RSS</a></li>
</ul>

<br>

      
        <form action="https://google.com/search" method="get">
          <fieldset role="search">
            <input type="hidden" name="q" value="site:https://www.pvk.ca" />
      
      
            <input class="search" type="text" name="q" results="0" placeholder="Search" aria-label="Search"/>
          </fieldset>
        </form>
  
</nav>
  <div id="main">
    <div id="content">
      
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title" style="font-family: "><a href="/Blog/2020/01/20/lazy-linear-knapsack/">Lazy Linear Knapsack</a></h1>
    
    
      <p class="meta">
        





Jan
  
20th, 
2020




        
         | <a href="/Blog/2020/01/20/lazy-linear-knapsack/#disqus_thread"
              data-disqus-identifier="https://www.pvk.ca/Blog/2020/01/20/lazy-linear-knapsack/"
	      >Comments</a>
        
        
      </p>
    
  </header>


  <div class="entry-content" style="font-family: ; font-size: "><p>The <a href="https://en.wikipedia.org/wiki/Continuous_knapsack_problem">continuous knapsack problem</a> may be the simplest non-trivial linear
programming problem:</p>

<p>\[\max_{x \in [0, 1]^n} p’x\]
subject to
\[w’x \leq b.\]</p>

<p>It has a linear objective, one constraint, and each decision variable
is bounded to ensure the optimum exists.  Note the key difference from
the <a href="https://en.wikipedia.org/wiki/Knapsack_problem#0/1_knapsack_problem">binary knapsack problem</a>: decision variables are allowed to take any
value between 0 and 1.  In other words, we can, e.g., stick half of
a profitable but large item in the knapsack. That’s why this knapsack
problem can be solved in linear time.</p>

<h2 id="dual-to-primal-is-reasonable">Dual to primal is reasonable</h2>

<p>Duality also lets us determine the shape of all optimal solutions to
this problem.  For each item \(i\) with weight \(w_i\) and profit
\(p_i\), let its profit ratio be \(r_i = p_i / w_i,\)
and let \(\lambda^\star\) be the optimal dual (Lagrange or linear)
multiplier associated with the capacity constraint \(w’x \leq b.\)
If \(\lambda^\star = 0,\) we simply take all items with a positive
profit ratio (\(r_i &gt; 0\)) and a non-negative weight \(w_i \geq 0.\)
Otherwise, every item with a profit ratio \(r_i &gt; \lambda^\star\)
will be at its weight upper bound (1 if \(w_i \geq 0\), 0
otherwise), and items with \(r_i &lt; \lambda^\star\) will instead be
at their lower bound (0 of \(w_i \leq 0\), and 1 otherwise).</p>

<p>Critical items, items with \(r_i = \lambda^\star,\) will take any value
that results in \(w’x = b.\) Given \(\lambda^\star,\) we can
derive the sum of weights for non-critical items; divide the
remaining capacity for critical items by the total weight of critical
items, and let that be the value for every critical item (with the
appropriate sign for the weight).</p>

<p>For example, if we have capacity \(b = 10,\) and the sum of weights
for non-critical items in the knsapsack is \(8,\) we’re left with another
two units of capacity to distribute however we want among
critical items (they all have the same profit ratio \(r_i =
\lambda^\star,\) so it doesn’t matter where that capacity goes).  Say
critical items with a positive weight have a collective weight of 4;
we could then assign a value of \(2 / 4 = 0.5\) to the corresponding
decision variable (and 0 for critical items with a non-positive
weight).</p>

<p>We could instead have \(b = 10,\) and the sum of weights for
non-critical items in the knapsack \(12\): we must find two units of
capacity among critical items (they all cost \(r_i = \lambda^\star\)
per unit, so it doesn’t matter which).  If critical items with a
negative weight have a collective weight of \(-3,\) we could assign
a value of \(-2 / -3 = 0.6\overline{6}\) to the corresponding decision
variables, and 0 for critical items with a non-negative weight.</p>

<p>The last case highlights something important about the knapsack: in
general, we can’t assume that the weights <em>or profits</em> are positive.
We could have an item with a non-positive weight and non-negative
profit (that’s always worth taking), an item with positive weight and
negative profit (never interesting), or weights and profits of the
same sign.  The last case is the only one that calls for actual
decision making.  Classically, items with negative weight and profit
are rewritten away, by assuming they’re taken in the knapsack, and
replacing them with a decision variable for the complementary decision
of removing that item from the knapsack (i.e., removing the additional
capacity in order to improve the profit).  I’ll try to treat them
directly as much as possible, because that reduction can be a
significant fraction of solve times in practice.</p>

<p>The characterisation of optimal solutions above makes it easy to
directly handle elements with a negative weight: just find the optimal
multiplier, compute the contribution of non-critical elements (with
decision variables at a bound) to the left-hand side of the capacity
constraint, separately sums the negative and positive weights for
critical elements, then do a final pass to distribute the remaining
capacity to critical elements (and 0-weight / 0-value elements if one
wishes).</p>

<h2 id="solving-the-dual-looks-like-selection">Solving the dual looks like selection</h2>

<p>Finding the optimal multiplier \(\lambda^\star\) is similar to a
selection problem: the value is either 0 (the capacity constraint is
redundant), or one of the profit ratios \(r_i,\) and, given a
multiplier value \(\lambda,\) we can determine if it’s too high or
too low in linear time.  If the non-critical elements yield a left-hand
side such that critical elements
can’t add enough capacity (i.e., no solution with the optimal form can
be feasible), \(\lambda\) is too low.  If the maximum weight of
potentially optimal solutions is too low, \(\lambda\) is too high.</p>

<p>We can thus sort the items by profit ratio \(r_i\), compute the
total weight corresponding to each ratio with a prefix sum (with a
pre-pass to sum all negative weights), and perform a linear (or
binary) search to find the critical profit ratio.
Moreover, the status of non-critical items is monotonic as
\(\lambda\) grows: if an item with positive weight is taken at
\(\lambda_0\), it is also taken for every \(\lambda \leq
\lambda_0\), and a negative-weight item that’s taken at
\(\lambda_0\) is also taken for every \(\lambda \geq \lambda_0.\)
This means we can adapt selection algorithms like <a href="https://en.wikipedia.org/wiki/Quickselect">Quickselect</a> to solve
the continuous knapsack problem in linear time.</p>

<p>I’m looking at large instances, so I would like to run these
algorithms in parallel or even distributed on multiple machines, and
ideally use GPUs or SIMD extensions.  Unfortunately, selection doesn’t
parallelise very well: we can run a distributed quickselect where
every processor partitions the data in its local RAM, but that still
requires a logarithmic number of iterations.</p>

<h2 id="selection-looks-like-quantile-estimation-does-the-dual">Selection looks like quantile estimation; does the dual?</h2>

<p><a href="https://cs.stackexchange.com/questions/27685/can-someone-explain-lazyselect">Lazy Select</a> offers a completely different angle for the selection
problem.  Selecting the \(k\)th smallest element from a list of
\(n\) elements is the same as finding the \(k / n\)th quantile<sup id="fnref:abuse-of-language"><a href="#fn:abuse-of-language" class="footnote">1</a></sup> in
that list of \(n\) elements.  We can use <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">concentration bounds</a><sup id="fnref:binomial"><a href="#fn:binomial" class="footnote">2</a></sup> to estimate quantiles from a sample of, e.g.,
\(m = n^{3/4}\) elements: the population quantile value is very
probably between the \(qm - \frac{\log m}{\sqrt{m}}\)th and \(qm +
\frac{\log m}{\sqrt{m}}\)th values of the sample.  Moreover, this
range very probably includes at most \(\mathcal{O}(n^{3/4})\)
elements<sup id="fnref:same-bounds"><a href="#fn:same-bounds" class="footnote">3</a></sup>, so a second pass suffices to buffer all the
elements around the quantile, and find the exact quantile.  Even with
a much smaller sample size \(m = \sqrt{n},\) we would only need four
passes.</p>

<p>Unfortunately, we can’t directly use that correspondence between
selection and quantile estimation for the continuous knapsack.</p>

<p>I tried to apply a similar idea by sampling the knapsack elements
equiprobably, and extrapolating from a solution to the sample.  For
every \(\lambda,\) we can derive a selection function 
\(f_\lambda (i) = I[r_i \geq \lambda]w_i\) 
(invert the condition if the weight is negative),
and scale up \(\sum_i f(i)\) from the sample to the population).
As long as we sample independently of \(f\), we can reuse the
same sample for all \(f_\lambda.\)
The difficulty here is that, while the error for Lazy Select
scales as a function of \(n,\) the equivalent bounds with
variable weights are a function of \(n(|\max_i w_i| + |\min_i w_i|)^2.\)
That doesn’t seem necessarily practical; scaling with \(\sum_i |w_i|\)
would be more reasonable.</p>

<p>Good news: we can hit that, thanks to linearity.</p>

<p>Let’s assume weights are all integers.  Any item with weight \(w_i\)
is equivalent to \(w_i\) subitems with unit weight (or \(-w_i\)
elements with negative unit weight), and the same profit ratio \(r_i\),
i.e., profit \(p_i / |w_i|\).  The range of <em>subitem</em> weights is now a constant.</p>

<p>We could sample uniformly from the subitems with a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli</a> for each
subitem, but that’s clearly linear time in the sum of weights, rather
than the number of elements.  If we wish to sample roughly \(m\) elements
from a total weight \(W = \sum_i |w_i|,\) we can instead determine how
many subitems (units of weight) to skip before sampling with a
<a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a> of
success probability \(m / W.\) This shows us how to lift the
integrality constraint on weights: sample from an <a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential</a>
with the same parameter \(m / W!\)</p>

<p>That helps, but we could still end up spending much more than constant
time on very heavy elements.  The trick is to deterministically
special-case these elements: stash any element with large weight
\(w_i \geq W / m\) to the side, exactly once.  By <a href="https://en.wikipedia.org/wiki/Markov%27s_inequality">Markov’s inequality</a>,<sup id="fnref:pigeonhole"><a href="#fn:pigeonhole" class="footnote">4</a></sup>
we know there aren’t too many heavy elements: at most \(m.\)</p>

<h2 id="lets-test-this-out">Let’s test this out</h2>

<p>The heart of the estimation problem can be formalised as follows:
given a list of elements \(i \in [n]\) with weight \(w_i \geq 0\),
generate a sample of \(m \leq n\) elements ahead of time. After the
sample has been generated, we want to accept an arbitrary predicate
\(p \in \{0,1\}^n\) and estimate \(\sum_{i\in [n]} p(i) w_i.\)</p>

<p>We just had a sketch of an algorithm for this problem.  Let’s see
what it looks like in Python.  The initial sample logic has to
determine the total weight, and sample items with probability
proportional to their weight.  Items heavier than the cutoff are not
considered in the sample and instead saved to an auxiliary list.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>sample.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">cutoff</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Samples from a list of (index, weight), with weight &gt;= 0.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Items with weight &gt;= cutoff are taken with probability one.</span>
</span><span class="line"><span class="sd">    Others are sampled with rate `rate` / unit of weight.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">large</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">next_sample</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
</span><span class="line">        <span class="n">index</span><span class="p">,</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">item</span>
</span><span class="line">        <span class="k">if</span> <span class="n">weight</span> <span class="o">&gt;=</span> <span class="n">cutoff</span><span class="p">:</span>
</span><span class="line">            <span class="n">large</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">next_sample</span> <span class="o">-=</span> <span class="n">weight</span>
</span><span class="line">            <span class="k">while</span> <span class="n">next_sample</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
</span><span class="line">                <span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
</span><span class="line">                <span class="n">next_sample</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">expovariate</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We can assemble the resulting sample (and list of “large” elements) to
compute a lower bound on the weight of items that satisfy any
predicate that’s independent of the sampling decisions.  The value for
large elements is trivial: we have a list of all large elements.
We can subtract the weight of all large elements from the total item
weight, and determine how much we have to extrapolate up.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>extrapolate.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">hoeffding</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Determines how much we can expect a sample of n i.i.d. values</span>
</span><span class="line"><span class="sd">    sampled from a Bernouli to differ, given an error rate of alpha.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Given a sample X of n i.i.d. values from a Bernoulli distribution,</span>
</span><span class="line"><span class="sd">    let delta be \bar{X} - E[\bar{X}], the one-sided difference</span>
</span><span class="line"><span class="sd">    between the sample average value and the expected sample average.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    Hoeffding&#39;s upper bound (see below) is conservative when the</span>
</span><span class="line"><span class="sd">    empirical probability is close to 0 or 1 (trivially, it can yield</span>
</span><span class="line"><span class="sd">    confidence bounds that are outside [0, 1]!), but simple, and in</span>
</span><span class="line"><span class="sd">    general not much worse than tighter confidence interval.</span>
</span><span class="line">
</span><span class="line"><span class="sd">    P(delta &gt;= eps) &lt;= exp(-2 eps^2 n) = alpha</span>
</span><span class="line"><span class="sd">      -&gt; -2 eps^2 n = ln alpha</span>
</span><span class="line"><span class="sd">     &lt;-&gt;        eps = sqrt[-(ln alpha) / 2n ]</span>
</span><span class="line">
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">eval_weight</span><span class="p">(</span><span class="n">total_weight</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Given a population&#39;s total weight, a memoryless sample (by weight)</span>
</span><span class="line"><span class="sd">    from the population&#39;s items, and large items that were</span>
</span><span class="line"><span class="sd">    deterministically picked, evaluates a lower bound for the sum of</span>
</span><span class="line"><span class="sd">    weights for items in the population that satisfy predicate.</span>
</span><span class="line"><span class="sd">    </span>
</span><span class="line"><span class="sd">    The lower bound is taken with error rate &lt;= alpha.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">large_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="c1"># The remainder was up for sampling, unit of weight at a time.</span>
</span><span class="line">    <span class="n">sampled_weight</span> <span class="o">=</span> <span class="n">total_weight</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">large</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">sampled_weight</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">sample</span><span class="p">:</span>
</span><span class="line">        <span class="k">return</span> <span class="n">large_sum</span>
</span><span class="line">    <span class="c1"># Estimate the Binomial success rate with a Beta</span>
</span><span class="line">    <span class="n">successes</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">)</span>
</span><span class="line">    <span class="n">failures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span> <span class="o">-</span> <span class="n">successes</span>
</span><span class="line">    <span class="c1"># We want a lower bound, and the uniform prior can result in a</span>
</span><span class="line">    <span class="c1"># (valid) bound that&#39;s higher than the empirical rate, so take the</span>
</span><span class="line">    <span class="c1"># min of the two.</span>
</span><span class="line">    <span class="n">empirical_rate</span> <span class="o">=</span> <span class="n">successes</span> <span class="o">/</span> <span class="n">sampled_weight</span>
</span><span class="line">    <span class="n">delta</span> <span class="o">=</span> <span class="n">hoeffding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample</span><span class="p">),</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">large_sum</span> <span class="o">+</span> <span class="n">sampled_weight</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">empirical_rate</span> <span class="o">-</span> <span class="n">delta</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>And finally, here’s how we can sample from an arbitrary list of items,
compure a lower bound on the weight of items that satisfy a predicate,
and compare that with the real lower bound.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>lower_bound.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">predicate</span><span class="p">):</span>
</span><span class="line">    <span class="n">total</span> <span class="o">=</span> <span class="n">total_weight</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># We expect a sample size of roughly rate * len(items), and</span>
</span><span class="line">    <span class="c1"># at most rate * len(items) large items.</span>
</span><span class="line">    <span class="n">sample</span><span class="p">,</span> <span class="n">large</span> <span class="o">=</span> <span class="n">sample_by_weight</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">rate</span> <span class="o">*</span> <span class="n">total</span><span class="p">)</span>
</span><span class="line">    <span class="n">lower_bound</span> <span class="o">=</span> <span class="n">eval_weight</span><span class="p">(</span><span class="n">total</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">large</span><span class="p">,</span> <span class="n">predicate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span><span class="line">    <span class="c1"># Check if the lower bound is valid.</span>
</span><span class="line">    <span class="n">actual</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">weight</span> <span class="k">for</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span> <span class="ow">in</span> <span class="n">items</span> <span class="k">if</span> <span class="n">predicate</span><span class="p">(</span><span class="n">index</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">lower_bound</span> <span class="o">&lt;=</span> <span class="n">actual</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">lower_bound</span><span class="p">,</span> <span class="n">actual</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>How do we test that? Far too often, I see tests for randomised
algorithms where the success rate is computed over randomly generated
inputs.  That’s too weak!  For example, this approach could lead us to accept
that the identity function is a randomised sort function, with success
probability \(\frac{1}{n!}.\)</p>

<p>The property we’re looking for is that, for any input, the success
rate (with the expectation over the pseudorandom sampling decisions)
is as high as requested.</p>

<p>For a given input (list of items and predicate), we can use the <a href="http://pvk.ca/Blog/2018/07/06/testing-slo-type-properties-with-the-confidence-sequence-method/">Confidence sequence method (CSM)</a>
to confirm that the lower bound is valid at least
\(1 - \alpha\) of the time.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>csm_test.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="k">def</span> <span class="nf">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">items</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)]</span>
</span><span class="line">    <span class="n">chosen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">i</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_case</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span><span class="p">)</span>
</span><span class="line">    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</span><span class="line">        <span class="k">yield</span> <span class="n">compare_bounds</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">chosen</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&quot;&quot;&quot;Test case is a list of pairs of weight and predicate value</span>
</span><span class="line"><span class="sd">       rate is the sample rate</span>
</span><span class="line"><span class="sd">       alpha is the confidence parameter for the lower bound.</span>
</span><span class="line"><span class="sd">    &quot;&quot;&quot;</span>
</span><span class="line">    <span class="n">wanted</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span>  <span class="c1"># The Hoeffding bound is conservative, so</span>
</span><span class="line">                        <span class="c1"># this should let csm_driver stop quickly.</span>
</span><span class="line">    <span class="n">result</span> <span class="o">=</span> <span class="n">csm</span><span class="o">.</span><span class="n">csm_driver</span><span class="p">(</span><span class="n">compare_bounds_generator</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">rate</span><span class="p">,</span>
</span><span class="line">                                                     <span class="n">alpha</span><span class="p">),</span>
</span><span class="line">                            <span class="n">wanted</span><span class="p">,</span>
</span><span class="line">                            <span class="mf">1e-6</span><span class="p">,</span>  <span class="c1"># Wrong conclusion with p &lt; 1e-6.</span>
</span><span class="line">                            <span class="nb">file</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stderr</span>
</span><span class="line">                            <span class="p">)</span>
</span><span class="line">    <span class="n">stop</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">result</span>
</span><span class="line">    <span class="k">assert</span> <span class="n">actual</span> <span class="o">&gt;=</span> <span class="n">wanted</span><span class="p">,</span> <span class="s2">&quot;Result: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>With a false positive rate of at most one in a million,<sup id="fnref:lotta-errors"><a href="#fn:lotta-errors" class="footnote">5</a></sup> we can
run automated tests against <code>check_bounds</code>.  I’ll use
<a href="https://hypothesis.works/">Hypothesis</a> to generate list of pairs of weight and predicate value:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="kn">from</span> <span class="nn">hypothesis</span> <span class="kn">import</span> <span class="n">given</span><span class="p">,</span> <span class="n">settings</span><span class="p">,</span> <span class="n">Verbosity</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">hypothesis.strategies</span> <span class="kn">as</span> <span class="nn">st</span>
</span><span class="line">
</span><span class="line"><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Bimodal inputs tend to be harder, so we can add a specialised test
generator.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>test_bimodal_bounds.py </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="py"><span class="line"><span></span><span class="nd">@given</span><span class="p">(</span><span class="n">test_case</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">lists</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">tuples</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">one_of</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">st</span><span class="o">.</span><span class="n">just</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span>
</span><span class="line">                                    <span class="n">st</span><span class="o">.</span><span class="n">booleans</span><span class="p">())),</span>
</span><span class="line">       <span class="n">rate</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
</span><span class="line">       <span class="n">alpha</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">min_value</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">))</span>
</span><span class="line"><span class="k">def</span> <span class="nf">test_bimodal_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
</span><span class="line">    <span class="n">check_bounds</span><span class="p">(</span><span class="n">test_case</span><span class="p">,</span> <span class="n">rate</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Again, we use <a href="https://hypothesis.readthedocs.io/en/latest/data.html">Hypothesis</a> to generate inputs, and
the <a href="https://github.com/pkhuong/csm">Confidence sequence method (available in C, Common Lisp, and Python)</a> to check that the lower bound is
valid with probability at least \(1 - \alpha\).  The CSM tests
for this statistical property with power 1 and adjustable error rate
(in our case, one in a million): we only provide a generator
for success values, and the driver adaptively determines when it makes
sense to make a call and stop generating more data, while accounting
for multiple hypothesis testing.</p>

<p>TL;DR: the estimation algorithm for individual sampling passes works,
and the combination of <a href="https://hypothesis.works/">Hypothesis</a> and <a href="https://github.com/pkhuong/csm">Confidence Sequence Method</a>
lets us painlessly test for a statistical property.</p>

<p>We can iteratively use this sampling procedure to derive lower and
(symmetrically) upper bounds for the optimal Lagrange multiplier
\(\lambda^\star,\) and Hoeffding’s inequality lets us control the
probability that the lower and upper bounds are valid.  Typically,
we’d use a tolerance of \(\sqrt{\log(n) / n},\) for an error
rate of \(1 / n^2.\) I prefer to simply use something like \(7 /
\sqrt{n}:\) the error rate is then less than \(10^{-42},\)
orders of manitude smaller than the probability of hardware failure in any given
nanosecond.<sup id="fnref:memory-error"><a href="#fn:memory-error" class="footnote">6</a></sup>
We can still check for failure of our Las Vegas algorithm,
but if something went wrong, it’s much more likely that we detected
a hardware failure than anything else.  It’s like running <a href="https://en.wikipedia.org/wiki/Super_PI">SuperPi</a>
to stress test a computer, except the work is useful. 😉</p>

<h2 id="repeat-as-necessary-to-solve-a-knapsack">Repeat as necessary to solve a knapsack</h2>

<p>How many sampling passes do we need? Our bounds are in terms of the
sum of item weight: if we let our sample size be in
\(\Theta(\sqrt{n}),\) the sum of weights \(\sum_i |w_i|\) for
unfathomed items (that may or may not be chosen depending on the exact
optimal multiplier \(\lambda^\star\) in the current range) will very
probably shrink by a factor of \(\Omega(n^{1/4}).\) The initial sum can, in
the worst case, be exponentially larger than the bitlength of the
input, so even a division by \(n^{1/4}\) isn’t necessarily that
great.</p>

<p>I intend to apply this Lazy Linear Knapsack algorithm on subproblems in
a more interesting solver, and I know that the sum of weights is
bounded by the size of the initial problem, so that’s good enough for
me!  After a constant (\(\approx 4\)) number of passes, the
difference in item weight between the lower and upper bound on
\(\lambda^\star\) should also be at most 1.  One or two additional
passes will get me near optimality (e.g., within \(10^{-4}\)),
and the lower bound on \(\lambda^\star\) should thus yield
a super-optimal solution that’s infeasible by at most \(10^{-4},\)
which is, for my intended usage (again), good enough.</p>

<p>Given an optimal enough \(\lambda^\star,\) we can construct an
explicit solution in one pass, plus a simple fixup for critical items.
This Lazy Knapsack seems pretty reasonable for parallel or GPU
computing: each sampling pass only needs to read the items (i.e., no
partitioning-like shuffling) before writing a fraction of the data to
a sample buffer, and we only need a constant number of passes (around
6 or 7) in the worst case.</p>

<p><hr style="width: 50%" /></p>
<div class="footnotes">
  <ol>
    <li id="fn:abuse-of-language">
      <p>It’s more like a fractional percentile, but you know what I mean: the value such that the distribution function at that point equals \(k / n\). <a href="#fnref:abuse-of-language" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:binomial">
      <p>Binomial bounds offer even stronger confidence intervals when the estimate is close to 0 or 1 (where Hoeffding’s bound would yield a confidence interval that juts outside \([0, 1]\)), but don’t impact worst-case performance. <a href="#fnref:binomial" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:same-bounds">
      <p>Thanks to Hoeffding’s inequality, again. <a href="#fnref:same-bounds" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:pigeonhole">
      <p>That’s a troll. I think any self-respecting computer person would rather see it as a sort of pigeonhole argument. <a href="#fnref:pigeonhole" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:lotta-errors">
      <p>We’re juggling a handful of error rates here. We’re checking whether the success rate for the Lazy Knapsack sampling subroutine is at least as high as \(1 - \alpha,\) as requested in the test parameters, and we’re doing so with another randomised procedure that will give an incorrect conclusion at most once every one million invocation. <a href="#fnref:lotta-errors" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:memory-error">
      <p><a href="http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf">This classic Google study</a> found 8% of DIMMs hit at least one error per year; that’s more than one single-bit error every \(10^9\) DIMM-second, and they’re mostly hard errors.  <a href="https://users.ece.cmu.edu/~omutlu/pub/memory-errors-at-facebook_dsn15.pdf">More recently, Facebook</a> reported that uncorrectable errors affect 0.03% of servers each month; that’s more than one uncorrectable error every \(10^{10}\) server-second.  If we performed one statistical test every nanosecond, the probability of memory failure alone would still dominate statistical errors by \(10^{20}!\) <a href="#fnref:memory-error" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/4">&larr; Older</a>
    
    <a href="/Blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/2">Newer &rarr;</a>
    
  </div>
</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Website copyright &copy; 2020 - <a href="mailto:pvk@pvk.ca">Paul Khuong</a> | <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/TheChymera/Koenigspress">Königspress</a></span>
</p>

</footer>
  

<script id="dsq-count-scr" src="//pvk.disqus.com/count.js" async></script>












<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-20468541-1', 'auto');
  ga('send', 'pageview');
</script>



</body>
</html>
