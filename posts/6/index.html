
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Paul Khuong: some Lisp</title>
  <meta name="author" content="Paul Khuong">
  <meta name="description" content="Paul Khuong's personal blog. Some Lisp, some optimisation, mathematical or computer.">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="https://www.pvk.ca/posts/6/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Paul Khuong: some Lisp" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Poller+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Germania+One&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Fontdiner+Swanky&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Lato&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Cardo&subset=latin-ext&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Sorts+Mill+Goudy&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=EB+Garamond&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=Della+Respira&display=fallback" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=UnifrakturMaguntia&subset=all&display=fallback" rel="stylesheet" type="text/css">
<link href='//fonts.googleapis.com/css?family=Arimo|EB+Garamond|PT+Sans+Caption&subset=latin,cyrillic&display=fallback' rel='stylesheet' type='text/css'>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  TeX: {
    Macros: {
     sp: "^",
     sb: "_"
    }
  }});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<meta name="twitter:dnt" content="on">

</head>

<body >
  <header role="banner"><hgroup>
  <h1><a href="/">Paul Khuong: some Lisp</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/Blog/archives">Archives</a></li>
  <li><a href="/atom.xml" title="subscribe via RSS">RSS</a></li>
</ul>

<br>

      
        <form action="https://google.com/search" method="get">
          <fieldset role="search">
            <input type="hidden" name="q" value="site:https://www.pvk.ca" />
      
      
            <input class="search" type="text" name="q" results="0" placeholder="Search" aria-label="Search"/>
          </fieldset>
        </form>
  
</nav>
  <div id="main">
    <div id="content">
      
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title" style="font-family: "><a href="/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator/">Slitter: a slab allocator that trusts, but verifies</a></h1>
    
    
      <p class="meta">
        





Aug
  
1st, 
2021




        
         | <a href="/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator/#disqus_thread"
              data-disqus-identifier="https://www.pvk.ca/Blog/2021/08/01/slitter-a-less-footgunny-slab-allocator/"
	      >Comments</a>
        
        
      </p>
    
  </header>



  <div class="entry-content" style="font-family: ; font-size: "><p><small>Originally posted on the <a href="https://engineering.backtrace.io/2021-08-04-slitter-a-slab-allocator-that-trusts-but-verifies/">Backtrace I/O tech blog</a>.</small></p>

<p><a href="https://github.com/backtrace-labs/slitter">Slitter</a> is Backtrace’s
deliberately middle-of-the-road
<a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">thread-caching</a>
<a href="https://people.eecs.berkeley.edu/~kubitron/courses/cs194-24-S13/hand-outs/bonwick_slab.pdf">slab allocator</a>,
with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/include/slitter.h#L7-L44">explicit allocation class tags</a>
(rather than derived from the object’s size class).
It’s mostly written in Rust, and we use it in our C backend server.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a>’s design is about as
standard as it gets: we hope to dedicate the project’s complexity
budget to always-on “observability” and safety features.  We don’t
wish to detect all or even most memory management errors, but we
should statistically catch a small fraction (enough to help pinpoint
production issues) of such bugs, and <em>always</em> constrain their scope to
the mismanaged allocation class.<sup id="fnref:blast-radius" role="doc-noteref"><a href="#fn:blast-radius" class="footnote" rel="footnote">1</a></sup></p>

<p>We decided to code up Slitter last April, when we noticed
that we would immediately benefit from backing allocation with
temporary file mappings:<sup id="fnref:when-does-it-flush" role="doc-noteref"><a href="#fn:when-does-it-flush" class="footnote" rel="footnote">2</a></sup>
the bulk of our data is mapped from
persistent data files, but we also regenerate some cold metadata
during startup, and accesses to that metadata have amazing locality,
both temporal and spatial (assuming bump allocation).  We don’t want the OS
to swap out all the heap–that way lie <a href="https://blog.acolyer.org/2017/06/15/gray-failure-the-achilles-heel-of-cloud-scale-systems/">grey failures</a>–so
we opt specific allocation classes into it.</p>

<p>By itself, this isn’t a reason to write a slab allocator: we could
easily have configured <a href="http://jemalloc.net/jemalloc.3.html#arena.i.extent_hooks">specialised arenas in jemalloc</a>,
for example.  However, we also had eyes on longer term improvements to
observability and debugging or mitigation of memory management errors in
production, and those could only be unlocked by migrating to an
interface with explicit tags for each allocation class (type).</p>

<p>Classic mallocs like <a href="https://github.com/jemalloc/jemalloc">jemalloc</a>
and <a href="https://github.com/google/tcmalloc">tcmalloc</a> are fundamentally
unable to match that level of integration: we can’t tell <code>malloc(3)</code>
what we’re trying to allocate (e.g., a <code>struct request</code> in the HTTP
module), only its size.  It’s still possible to wrap malloc in a richer
interface, and, e.g., track heap consumption by tag.  Unfortunately,
the result is slower than a native solution, and, without help from
the underlying allocator, it’s easy to incorrectly match tags between
<code>malloc</code> and <code>free</code> calls.  In my experience, this frequently leads to
useless allocation statistics, usually around the very faulty code
paths one is attempting to debug.</p>

<p>Even once we have built detailed statistics on top of a regular
malloc, it’s hard to convince the underlying allocator to only recycle
allocations within an object class: not only do mallocs eagerly
recycle allocations of similar sizes regardless of their type, but
they will also release unused runs of address space, or repurpose them
for totally different size classes.  That’s what mallocs are supposed
to do…  it just happens to also make debugging a lot harder when
things inevitably go wrong.<sup id="fnref:ub" role="doc-noteref"><a href="#fn:ub" class="footnote" rel="footnote">3</a></sup></p>

<p>Slab allocators work with semantically richer allocation tags: an
allocation tag describes its objects’ size, but can also specify how
to initialise, recycle, or deinitialise them.  The problem is that
slab allocators tend to focus exclusively on speed.</p>

<p><a href="https://github.com/omniti-labs/portableumem">Forks of libumem</a>
may be the exception, thanks to the Solaris culture of pervasive
hooking.  However, <code>umem</code>’s design reflects the sensibilities of the
00s, when it was written: threads share a few caches, and the
allocator tries to reuse address space.  In contrast, Slitter assumes memory
is plentiful enough for thread-local caches and type-stable
allocations.<sup id="fnref:not-as-configurable" role="doc-noteref"><a href="#fn:not-as-configurable" class="footnote" rel="footnote">4</a></sup></p>

<h2 id="our-experience-so-far">Our experience so far</h2>

<p>We have been running <a href="https://crates.io/crates/slitter">Slitter</a> in
production for over two months, and rely on it to:</p>

<ul>
  <li>detect when an allocation is freed with the wrong allocation class
tag (i.e., detect type confusion on free).</li>
  <li>avoid any in-band metadata: there are guard pages between
allocations and allocator metadata, and no intrusive freelist for
use-after-frees to stomp over.</li>
  <li>guarantee <a href="https://www.usenix.org/legacy/publications/library/proceedings/osdi96/full_papers/greenwald/node2.html">type stable allocations</a>:
once an address has been used to fulfill a request for a certain
allocation class, it will only be used for that class.  Slitter
doesn’t overlay intrusive lists on top of freed allocations, so the
data always reflects what the application last stored there.  This
means that double-frees and use-after-frees only affect the faulty
allocation class.  An application could even rely on
read-after-free being benign to simplify non-blocking algorithms.<sup id="fnref:but-we-dont-depend-on-it-too-much" role="doc-noteref"><a href="#fn:but-we-dont-depend-on-it-too-much" class="footnote" rel="footnote">5</a></sup></li>
  <li>let each allocation class specify how its backing memory should
be mapped in (e.g., plain 4 KB pages or file-backed swappable pages).</li>
</ul>

<p>Thanks to extensive contracts and a mix of hardcoded and random tests,
we encountered only two issues during the initial rollout, both in the
small amount of lock-free C code that is hard to test.<sup id="fnref:legacy-gcc" role="doc-noteref"><a href="#fn:legacy-gcc" class="footnote" rel="footnote">6</a></sup></p>

<p>Type stability exerts a heavy influence all over Slitter’s design, and
has obvious downsides.  For example, a short-lived application that
progresses through a pipeline of stages, where each stage allocates
different types, would definitely waste memory if it were to replace a
regular malloc with a type-stable allocator like Slitter.  We believe
the isolation benefits are more than worth the trouble, at least for
long-lived servers that quickly enter a steady state.</p>

<p>In the future, we hope to also:</p>

<ul>
  <li>detect when an interior pointer is freed.</li>
  <li>detect simple<sup id="fnref:jump" role="doc-noteref"><a href="#fn:jump" class="footnote" rel="footnote">7</a></sup> buffer overflows that cross allocation classes, by inserting guard pages.</li>
  <li>always detect frees of addresses Slitter does not manage.</li>
  <li>detect most back-to-back double-frees.</li>
  <li>detect a random fraction of buffer overflows, with a sampling <a href="https://en.wikipedia.org/wiki/Electric_Fence">eFence</a>.</li>
</ul>

<p>In addition to these safety features, we plan to rely on the allocator
to improve observability into the calling program, and wish to:</p>

<ul>
  <li>track the number of objects allocated and recycled in each
allocation class.</li>
  <li>sample the call stack when the heap grows.</li>
  <li>track allocation and release call stacks for a small fraction of objects.</li>
</ul>

<p>Here’s how it currently works, and why we wrote it in Rust, with dash
of C.</p>

<h2 id="the-high-level-design-of-slitter">The high level design of Slitter</h2>

<p>At a <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/doc/design.md">high level</a>,
Slitter</p>

<ol>
  <li>reserves shared 1 GB <code>Chunk</code>s of memory via the <a href="https://github.com/backtrace-labs/slitter/blob/fa8629989cb63ca5a4acdc2d26741bccda79aac0/src/mapper.rs"><code>Mapper</code> trait</a></li>
  <li>carves out smaller type-specific <code>Span</code>s from each chunk with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs"><code>Mill</code> objects</a></li>
  <li>bump-allocates objects from <code>Span</code>s with <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/press.rs"><code>Press</code> objects</a>, into <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/magazine.rs">allocation <code>Magazines</code></a></li>
  <li>pushes and pops objects into/from <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/c/cache.c">thread-local magazines</a></li>
  <li>caches populated magazines in global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/class.rs#L62-L67">type-specific lock-free stacks</a></li>
  <li>manages empty magazines with a global <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/rack.rs">mostly lock-free <code>Rack</code></a></li>
</ol>

<p>Many general purpose memory allocators implement strategies similarly
inspired by <a href="https://www.usenix.org/legacy/publications/library/proceedings/usenix01/full_papers/bonwick/bonwick.pdf">Bonwick’s slab allocator</a>,
and time-tested mallocs may well provide better performance
and lower fragmentation than Slitter.<sup id="fnref:type-stable" role="doc-noteref"><a href="#fn:type-stable" class="footnote" rel="footnote">8</a></sup>
The primary motivation for designing Slitter is that having explicit
allocation classes in the API makes it easier for the allocator to
improve the debuggability and resilience of the calling program.<sup id="fnref:also-good-for-perf" role="doc-noteref"><a href="#fn:also-good-for-perf" class="footnote" rel="footnote">9</a></sup>
For example, most allocators can tell you the size of your program’s
heap, but that data is much more useful when broken down by struct
type or program module.</p>

<p>Most allocators try to minimise accesses to the metadata associated with
allocations.  In fact, that’s often seen as a strength of the slab
interface: the allocator can just rely on the caller to pass the
correct allocation class tag, instead of hitting metadata to figure
out there the freed address should go.</p>

<p>We went in the opposite direction with Slitter.  We still rely on the
allocation class tag for speed, but also actively look for mismatches
before returning from deallocation calls. Nothing depends on
values computed by the mismatch detection logic, and the resulting
branch is trivially predictable (the tag always matches), so we can
hope that wide out-of-order CPUs will hide most of the checking
code, if it’s simple enough.</p>

<p>This concern (access to metadata in few instructions) combined with
our goal of avoiding in-band metadata lead to a
<a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs#L6-L19">simple layout for each chunk’s data and metadata</a>.</p>

<pre><code>.-------.------.-------|---------------.-------.
| guard | meta | guard | data ... data | guard |
'-------'------'-------|---------------'-------'
  2 MB    2 MB   2 MB  |      1 GB        2 MB
                       v
               Aligned to 1 GB
</code></pre>

<p>A chunk’s data is always a 1 GB address range, aligned to 1 GB: the
underlying mapper doesn’t have to immediately back that with memory,
but it certainly can, e.g., in order to use gigantic pages.  The chunk
is preceded and followed by 2 MB guard pages.  The metadata for the
chunk’s data lives in a 2 MB range, just before the preceding guard
page (i.e., 4 MB to 2 MB before the beginning of the aligned 1 GB
range).  Finally, the 2 MB metadata range is itself preceded by a 2MB
guard page.</p>

<p>Each chunk is statically divided in 65536 spans of 16 KB each.  We can
thus map a span to its slot in the metadata block with a shifts,
masks, and some address arithmetic.  <a href="https://github.com/backtrace-labs/slitter/blob/7afb9781fd25b8cee62afa555b9d38f391131044/src/mill.rs">Mill</a>s
don’t have to hand out individual 16 KB spans at a time, they simply
have to work in multiples of 16 KB, and never split a span in two.</p>

<h2 id="why-we-wrote-slitter-in-rust-and-c">Why we wrote Slitter in Rust and C</h2>

<p>We call Slitter from C, but wrote it in Rust, despite the
more painful build<sup id="fnref:uber-crate" role="doc-noteref"><a href="#fn:uber-crate" class="footnote" rel="footnote">10</a></sup> process: that pain isn’t going
anywhere, since we expect our backend to be in a mix of C, C++, and
Rust for a long time.  We also sprinkled in some C when the
alternative would have been to pull in a crate just to make a couple
syscalls, or to enable unstable Rust features: we’re not
“rewrite-it-in-Rust” absolutists, and merely wish to use Rust for its
strengths (control over data layout, support for domain-specific
invariants, large ecosystem for less performance-sensitive logic, ability to
lie to the compiler where necessary, …), while avoiding its
weaknesses (interacting with Linux interfaces defined by C headers, or
fine-tuning code generation).</p>

<p>The majority of allocations only interact with the thread-local
magazines.  That’s why we <a href="https://github.com/backtrace-labs/slitter/blob/main/c/cache.c">wrote that code in C</a>:
stable Rust doesn’t (yet) let us access <a href="https://doc.rust-lang.org/std/intrinsics/fn.likely.html">likely/unlikely annotations</a>,
nor <a href="https://www.akkadia.org/drepper/tls.pdf#page=35">fast “initial-exec”</a> <a href="https://github.com/rust-lang/rust/issues/29594">thread-local storage</a>.
Of course, allocation and deallocation are the main entry points into
a memory allocation library, so this creates a bit of friction with
Rust’s linking process.<sup id="fnref:bad-linker" role="doc-noteref"><a href="#fn:bad-linker" class="footnote" rel="footnote">11</a></sup></p>

<p>We also had to implement our <a href="https://github.com/backtrace-labs/slitter/blob/main/c/stack.c">lock-free multi-popper Treiber stack</a>
in C: x86-64 doesn’t have anything like LL/SC, so we instead pair
the top-of-stack pointer with a generation counter… and 
<a href="https://github.com/rust-lang/rust/issues/32976#issuecomment-641360955">Rust hasn’t stabilised 128-bit atomics</a> yet.</p>

<p>We chose to use atomics in C instead of a simple lock in Rust because
the lock-free stack (and the atomic bump pointer, which Rust handles
fine) are important for our use case: when we rehydrate cold metadata
at startup, we do so from multiple I/O-bound threads, and we have
observed hiccups due to lock contention in malloc.  At some point,
lock acquisitions are rare enough that contention isn’t an issue;
that’s why we’re comfortable with locks when refilling bump allocation
regions.</p>

<h2 id="come-waste-performance-on-safety">Come waste performance on safety!</h2>

<p>A recurring theme in the design of <a href="https://github.com/backtrace-labs/slitter">Slitter</a>
is that we find ways to make the core (de)allocation logic slightly
faster, and immediately spend that efficiency on safety, debuggability
or, eventually, observability.  For a lot of code, performance is a
constraint to satisfy, not a goal to maximise; once we’re close to
good enough, it makes sense to trade performance
away.<sup id="fnref:even-works-for-perf" role="doc-noteref"><a href="#fn:even-works-for-perf" class="footnote" rel="footnote">12</a></sup> I also believe that there are
<a href="https://research.google/pubs/pub50370/">lower hanging fruits in memory placement</a>
than shaving a few nanoseconds from the allocation path.</p>

<p><a href="https://crates.io/crates/slitter">Slitter</a> also focuses on
instrumentation and debugging features that are always active, even in
production, instead of leaving that to development tools, or to logic
that must be explicitly enabled.  In a SaaS world, development and
debugging is never done.  Opt-in tools are definitely useful, but
always-on features are much more likely to help developers catch
the rarely occurring bugs on which they tend to spend an inordinate
amount of investigation effort (and if a debugging feature can be
safely enabled in production at a large scale, why not leave it
enabled forever?).</p>

<p>If that sounds like an interesting philosophy for a slab allocator,
<a href="https://github.com/backtrace-labs/slitter">come hack on Slitter</a>!
Admittedly, the value of Slitter isn’t as clear for pure Rust hackers
as it is for those of us who blend C and Rust, but per-class allocation
statistics and placement decisions should be useful, even in safe
Rust, especially for larger programs with long runtimes.</p>

<p>Our <a href="https://github.com/backtrace-labs/slitter">MIT-licensed code is on github</a>,
there are <a href="https://github.com/backtrace-labs/slitter/issues">plenty of small improvements to work on</a>,
and, while we still have to re-review the documentation, it has decent
test coverage, and we try to write straightforward code.</p>

<p><small>This post was much improved by feedback from my beta readers, Barkley, David,
Eloise, Mark, Per, Phil, Ruchir, and Samy.</small></p>

<p><hr style="width: 50%" /></p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:blast-radius" role="doc-endnote">
      <p>In my experience, their unlimited blast radius is what makes memory management bugs so frustrating to track down.  The design goals of generic memory allocators (e.g., recycling memory quickly) and some implementation strategies (e.g., <a href="http://phrack.org/issues/57/9.html#article">in-band metadata</a>) make it easy for bugs in one module to show up as broken invariants in a completely unrelated one that happened to share allocation addresses with the former.  <a href="http://phrack.org/issues/57/8.html#article">Adversarial thinkers</a> will even exploit the absence of isolation to <a href="https://www.openwall.com/articles/JPEG-COM-Marker-Vulnerability">amplify small programming errors into arbitrary code execution</a>.  Of course, one should simply not write bugs, but when they do happen, it’s nice to know that the broken code most likely hit itself and its neighbours in the callgraph, and not unrelated code that also uses the same memory allocator (something Windows got right with private heaps). <a href="#fnref:blast-radius" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:when-does-it-flush" role="doc-endnote">
      <p>Linux does not have anything like the <a href="https://www.freebsd.org/cgi/man.cgi?query=mmap&amp;sektion=2&amp;format=html#:~:text=a%20core%20file.-,MAP_NOSYNC,-Causes%20data%20dirtied">BSD’s <code>MAP_NOSYNC</code> mmap flag</a>.  This has historically created problems for <a href="https://lkml.org/lkml/2013/9/7/135">heavy mmap users like LMDB</a>.  Empirically, Linux’s flushing behaviour is much more reasonable these days, especially when dirty pages are a small fraction of physical RAM, as it is for us: in a well configured installation of our backend server, most of the RAM goes to clean file mappings, so only the <a href="https://www.kernel.org/doc/Documentation/sysctl/vm.txt"><code>dirty_expire_centisec</code> timer</a> triggers write-outs, and we haven’t been growing the file-backed heap fast enough for the time-based flusher to thrash too much. <a href="#fnref:when-does-it-flush" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:ub" role="doc-endnote">
      <p>There are obvious parallels with undefined behaviour in C and C++… <a href="#fnref:ub" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:not-as-configurable" role="doc-endnote">
      <p>umem also takes a performance hit in order to let object classes define callbacks for object initialisation, recycling, and destruction.  It makes sense to let the allocator do some pre-allocation work: if you’re going to incur a cache miss for the first write to an allocation, it’s preferable to do so before you immediately want that newly allocated object (yes, profiles will show more cycles in the allocators, but you’re just shifting work around, hopefully farther from the critical path). Slitter only supports the bare minimum: objects are either always zero-initialised, or initially zero-filled and later left untouched.  That covers the most common cases, without incurring too many branch mispredictions. <a href="#fnref:not-as-configurable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:but-we-dont-depend-on-it-too-much" role="doc-endnote">
      <p>One could be tempted to really rely on it not just for isolation and resilience, but during normal operations.  That sounds like a bad idea (we certainly haven’t taken that leap), at least until <a href="https://github.com/backtrace-labs/slitter/issues/11">Slitter works with Valgrind/ASan/LSan</a>: it’s easier to debug easily reproducible issues when one can just slot in calls to regular malloc/calloc/free with a dedicated heap debugger. <a href="#fnref:but-we-dont-depend-on-it-too-much" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:legacy-gcc" role="doc-endnote">
      <p>It would be easy to blame the complexity of lock-free code, but the initial version, with C11 atomics, was correct.  Unfortunately, gcc backs C11 atomic <code>uint128_t</code>s with locks, so we had to switch to the legacy interface, and that’s when the errors crept in. <a href="#fnref:legacy-gcc" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:jump" role="doc-endnote">
      <p>There isn’t much the allocator can do if an application writes to a wild address megabytes away from the base object.  Thankfully, buffer overflows tend to proceed linearly from the actual end of the undersized object. <a href="#fnref:jump" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:type-stable" role="doc-endnote">
      <p>In fact, Slitter actively worsens external fragmentation to guarantee type-stable allocations. We think it’s reasonable to sacrifice heap footprint in order to control the blast radius of use-after-frees and double-frees. <a href="#fnref:type-stable" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:also-good-for-perf" role="doc-endnote">
      <p>That’s why we’re interested in allocation class tags, but they can also help application and malloc performance.  Some malloc developers are looking into tags for placement (should the allocation be backed by memory local to the NUMA node, with huge pages, …?) or lifetime (is the allocation immortal, short-lived, or tied to a request?) hints. <a href="#fnref:also-good-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:uber-crate" role="doc-endnote">
      <p>We re-export our dependencies from an uber-crate, and let our outer <a href="https://mesonbuild.com/">meson</a> build invoke <code>cargo</code> to generate a static library for that facade uber-crate. <a href="#fnref:uber-crate" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:bad-linker" role="doc-endnote">
      <p><a href="https://github.com/rust-lang/rfcs/issues/2771">Rust automatically hides foreign symbols when linking <code>cdylib</code>s</a>.  We worked around that with static linking, but statically linked rust libraries are mutually incompatible, hence the uber-crate. <a href="#fnref:bad-linker" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:even-works-for-perf" role="doc-endnote">
      <p>And not just for safety or productivity features!  I find it often makes sense to give up on small performance wins (e.g., aggressive autovectorisation or link-time optimisation) when they would make future performance investigations harder.  The latter are higher risk, and only potential benefits, but their upside (order of magnitude improvements) dwarfs guaranteed small wins that freeze the code in time. <a href="#fnref:even-works-for-perf" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/posts/7">&larr; Older</a>
    
    <a href="/Blog/archives">Blog Archives</a>
    
    <a class="next" href="/posts/5">Newer &rarr;</a>
    
  </div>
</div>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Website copyright &copy; 2025 - <a href="mailto:pvk@pvk.ca">Paul Khuong</a> | <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/TheChymera/Koenigspress">Königspress</a></span>
</p>

</footer>
  

<script id="dsq-count-scr" src="//pvk.disqus.com/count.js" async></script>














</body>
</html>
